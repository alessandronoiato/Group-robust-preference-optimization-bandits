2024-10-07 17:17:22,590 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log-hyperparam-search/2024_10_07_17_04_32/rdpo,0.1,0.1,1,2022
2024-10-07 17:17:22,592 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2022
2024-10-07 17:17:22,592 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-10-07 17:17:22,681 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3214, l2 distance: 25.6155, acc: 0.89.
2024-10-07 17:17:22,682 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-10-07 17:17:22,683 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [ 4.91492317 13.51244723  6.71533233  9.49961375]
2024-10-07 17:17:22,896 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.8530, 6.9304, 3.3326
2024-10-07 17:17:23,120 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1314] - INFO: unique_group_ids: {0, 1, 2}
2024-10-07 17:17:24,371 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  0, train_loss:  13.4339, val_loss:  12.2721, grad_norm: 0.5704, live_grad: 0.0000, reward_err: 0.0031, 0.0319, 0.0015, KL_dist: 0.6559, 0.3522, 0.6882, param: [4.11786407 9.56583272 4.94852636 8.93320827], weights: [0.29933897 0.3029308  0.39773023], train_wt_loss:  40.3017, val_wt_loss: 36.8163, train_grp_loss: [11.681894   15.95789746 11.12859769], val_grp_loss: [10.57186309 15.00632368 11.25447452], train_hist_grp_loss: [0.17243466 0.18436246 0.45663202], cur_train_grp_loss: [0.17243466 0.18436246 0.45663202], max_reward_err:  0.0319, max_reward_err_index: 1, max_kl_dist:  0.6882, max_kl_dist_index: 2, max_train_grp_loss:  15.9579, max_train_grp_loss_index: 1, max_val_grp_loss:  15.0063, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.4566, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:25,410 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  1, train_loss:  13.4339, val_loss:  12.2720, grad_norm: 0.0048, live_grad: 0.0000, reward_err: 0.0031, 0.0319, 0.0015, KL_dist: 0.6563, 0.3521, 0.6884, param: [4.11542234 9.54485874 4.96135419 8.95314734], weights: [0.27975241 0.29158412 0.42866347], train_wt_loss:  40.3017, val_wt_loss: 36.8160, train_grp_loss: [11.6898449  15.94973627 11.13001506], val_grp_loss: [10.57670447 14.99733481 11.25830082], train_hist_grp_loss: [0.26664348 0.30806709 0.6934107 ], cur_train_grp_loss: [0.09420882 0.12370463 0.23677867], max_reward_err:  0.0319, max_reward_err_index: 1, max_kl_dist:  0.6884, max_kl_dist_index: 2, max_train_grp_loss:  15.9497, max_train_grp_loss_index: 1, max_val_grp_loss:  14.9973, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2368, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:26,528 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  2, train_loss:  13.4340, val_loss:  12.2729, grad_norm: 0.0047, live_grad: 0.0000, reward_err: 0.0034, 0.0319, 0.0014, KL_dist: 0.6572, 0.3525, 0.6893, param: [4.11043287 9.52908274 4.97206497 8.97880234], weights: [0.26039001 0.27949169 0.4601183 ], train_wt_loss:  40.3021, val_wt_loss: 36.8188, train_grp_loss: [11.69224887 15.94806374 11.12911187], val_grp_loss: [10.57670104 14.998038   11.26034444], train_hist_grp_loss: [0.36091643 0.43170846 0.93021953], cur_train_grp_loss: [0.09427294 0.12364137 0.23680883], max_reward_err:  0.0319, max_reward_err_index: 1, max_kl_dist:  0.6893, max_kl_dist_index: 2, max_train_grp_loss:  15.9481, max_train_grp_loss_index: 1, max_val_grp_loss:  14.9980, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2368, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:27,638 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  3, train_loss:  13.4343, val_loss:  12.2749, grad_norm: 0.0053, live_grad: 0.0000, reward_err: 0.0034, 0.0319, 0.0014, KL_dist: 0.6586, 0.3532, 0.6909, param: [4.10287127 9.51842886 4.98053153 9.01018   ], weights: [0.24137279 0.26679236 0.49183484], train_wt_loss:  40.3029, val_wt_loss: 36.8246, train_grp_loss: [11.68905573 15.95293462 11.12588221], val_grp_loss: [10.57183739 15.00852193 11.26059737], train_hist_grp_loss: [0.45520876 0.55533686 1.16700914], cur_train_grp_loss: [0.09429233 0.1236284  0.23678961], max_reward_err:  0.0319, max_reward_err_index: 1, max_kl_dist:  0.6909, max_kl_dist_index: 2, max_train_grp_loss:  15.9529, max_train_grp_loss_index: 1, max_val_grp_loss:  15.0085, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2368, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:28,779 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  4, train_loss:  13.4347, val_loss:  12.2779, grad_norm: 0.0065, live_grad: 0.0000, reward_err: 0.0034, 0.0319, 0.0014, KL_dist: 0.6603, 0.3544, 0.6931, param: [4.09272726 9.51274477 4.98662746 9.04717618], weights: [0.22282071 0.25363472 0.52354457], train_wt_loss:  40.3042, val_wt_loss: 36.8336, train_grp_loss: [11.68030673 15.96435262 11.12037444], val_grp_loss: [10.5621835  15.02878492 11.25909917], train_hist_grp_loss: [0.54947533 0.67900302 1.40373004], cur_train_grp_loss: [0.09426658 0.12366616 0.2367209 ], max_reward_err:  0.0319, max_reward_err_index: 1, max_kl_dist:  0.6931, max_kl_dist_index: 2, max_train_grp_loss:  15.9644, max_train_grp_loss_index: 1, max_val_grp_loss:  15.0288, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2367, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:29,841 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  5, train_loss:  13.4354, val_loss:  12.2820, grad_norm: 0.0084, live_grad: 0.0000, reward_err: 0.0032, 0.0323, 0.0015, KL_dist: 0.6625, 0.3560, 0.6959, param: [4.08000727 9.5118028  4.99023338 9.08957023], weights: [0.2048482  0.24017205 0.55497975], train_wt_loss:  40.3061, val_wt_loss: 36.8459, train_grp_loss: [11.66613817 15.98226431 11.11269134], val_grp_loss: [10.54789693 15.05872479 11.25593697], train_hist_grp_loss: [0.64367136 0.80275769 1.64033375], cur_train_grp_loss: [0.09419602 0.12375467 0.23660371], max_reward_err:  0.0323, max_reward_err_index: 1, max_kl_dist:  0.6959, max_kl_dist_index: 2, max_train_grp_loss:  15.9823, max_train_grp_loss_index: 1, max_val_grp_loss:  15.0587, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2366, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:30,921 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  6, train_loss:  13.4363, val_loss:  12.2873, grad_norm: 0.0110, live_grad: 0.0000, reward_err: 0.0029, 0.0338, 0.0017, KL_dist: 0.6650, 0.3579, 0.6993, param: [4.06473706 9.51530549 4.99124387 9.13702428], weights: [0.18756003 0.22655711 0.58588285], train_wt_loss:  40.3089, val_wt_loss: 36.8618, train_grp_loss: [11.64678058 16.00655463 11.10298733], val_grp_loss: [10.52922063 15.09813191 11.25124283], train_hist_grp_loss: [0.73775312 0.92665121 1.87677399], cur_train_grp_loss: [0.09408176 0.12389352 0.23644024], max_reward_err:  0.0338, max_reward_err_index: 1, max_kl_dist:  0.6993, max_kl_dist_index: 2, max_train_grp_loss:  16.0066, max_train_grp_loss_index: 1, max_val_grp_loss:  15.0981, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2364, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:31,936 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  7, train_loss:  13.4376, val_loss:  12.2938, grad_norm: 0.0143, live_grad: 0.0000, reward_err: 0.0029, 0.0346, 0.0017, KL_dist: 0.6679, 0.3602, 0.7032, param: [4.04696404 9.52289539 4.9895748  9.18908819], weights: [0.17104772 0.21293711 0.61601517], train_wt_loss:  40.3127, val_wt_loss: 36.8813, train_grp_loss: [11.6225534  16.03704478 11.09146282], val_grp_loss: [10.50647657 15.1466862  11.24518851], train_hist_grp_loss: [0.83167877 1.05073303 2.11300777], cur_train_grp_loss: [0.09392565 0.12408182 0.23623377], max_reward_err:  0.0346, max_reward_err_index: 1, max_kl_dist:  0.7032, max_kl_dist_index: 2, max_train_grp_loss:  16.0370, max_train_grp_loss_index: 1, max_val_grp_loss:  15.1467, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2362, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:32,935 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  8, train_loss:  13.4393, val_loss:  12.3015, grad_norm: 0.0186, live_grad: 0.0000, reward_err: 0.0028, 0.0346, 0.0017, KL_dist: 0.6710, 0.3628, 0.7076, param: [4.02675939 9.5341686  4.98517047 9.2452099 ], weights: [0.15538677 0.19944918 0.64516404], train_wt_loss:  40.3180, val_wt_loss: 36.9046, train_grp_loss: [11.59385537 16.07349218 11.07835615], val_grp_loss: [10.48005524 15.20395841 11.23797795], train_hist_grp_loss: [0.92540904 1.17505121 2.34899634], cur_train_grp_loss: [0.09373027 0.12431818 0.23598857], max_reward_err:  0.0346, max_reward_err_index: 1, max_kl_dist:  0.7076, max_kl_dist_index: 2, max_train_grp_loss:  16.0735, max_train_grp_loss_index: 1, max_val_grp_loss:  15.2040, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2360, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:33,955 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  9, train_loss:  13.4417, val_loss:  12.3106, grad_norm: 0.0239, live_grad: 0.0000, reward_err: 0.0028, 0.0350, 0.0017, KL_dist: 0.6744, 0.3658, 0.7124, param: [4.00421964 9.54869097 4.97800987 9.30475086], weights: [0.14063494 0.18621678 0.67314828], train_wt_loss:  40.3250, val_wt_loss: 36.9319, train_grp_loss: [11.56115132 16.11559287 11.0639337 ], val_grp_loss: [10.45040204 15.26941568 11.22983818], train_hist_grp_loss: [1.01890787 1.29965192 2.58470604], cur_train_grp_loss: [0.09349883 0.12460071 0.23570971], max_reward_err:  0.0350, max_reward_err_index: 1, max_kl_dist:  0.7124, max_kl_dist_index: 2, max_train_grp_loss:  16.1156, max_train_grp_loss_index: 1, max_val_grp_loss:  15.2694, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2357, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:34,941 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  10, train_loss:  13.4447, val_loss:  12.3211, grad_norm: 0.0302, live_grad: 0.0000, reward_err: 0.0028, 0.0354, 0.0017, KL_dist: 0.6779, 0.3690, 0.7176, param: [3.97946766 9.56601573 4.96811174 9.36700564], weights: [0.12683149 0.17334701 0.6998215 ], train_wt_loss:  40.3340, val_wt_loss: 36.9634, train_grp_loss: [11.52495644 16.16298588 11.04847894], val_grp_loss: [10.41800147 15.3424311  11.22100916], train_hist_grp_loss: [1.11214296 1.424579   2.82010889], cur_train_grp_loss: [0.09323509 0.12492708 0.23540284], max_reward_err:  0.0354, max_reward_err_index: 1, max_kl_dist:  0.7176, max_kl_dist_index: 2, max_train_grp_loss:  16.1630, max_train_grp_loss_index: 1, max_val_grp_loss:  15.3424, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2354, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:35,941 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  11, train_loss:  13.4484, val_loss:  12.3330, grad_norm: 0.0376, live_grad: 0.0000, reward_err: 0.0027, 0.0354, 0.0018, KL_dist: 0.6816, 0.3724, 0.7230, param: [3.95265272 9.58570088 4.9555377  9.43122423], weights: [0.11399747 0.16092905 0.72507348], train_wt_loss:  40.3453, val_wt_loss: 36.9989, train_grp_loss: [11.48581907 16.21525925 11.03228133], val_grp_loss: [10.38336039 15.42229649 11.21173361], train_hist_grp_loss: [1.20508616 1.54987346 3.05518291], cur_train_grp_loss: [0.0929432  0.12529446 0.23507402], max_reward_err:  0.0354, max_reward_err_index: 1, max_kl_dist:  0.7230, max_kl_dist_index: 2, max_train_grp_loss:  16.2153, max_train_grp_loss_index: 1, max_val_grp_loss:  15.4223, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2351, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:36,933 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  12, train_loss:  13.4530, val_loss:  12.3462, grad_norm: 0.0462, live_grad: 0.0000, reward_err: 0.0023, 0.0363, 0.0022, KL_dist: 0.6854, 0.3761, 0.7286, param: [3.92394965 9.607325   4.94039334 9.49663572], weights: [0.10213677 0.14903367 0.74882956], train_wt_loss:  40.3591, val_wt_loss: 37.0385, train_grp_loss: [11.44430332 16.27195742 11.01562584], val_grp_loss: [10.34699144 15.50823787 11.20224735], train_hist_grp_loss: [1.29771373 1.67557315 3.2899123 ], cur_train_grp_loss: [0.09262757 0.12569968 0.23472939], max_reward_err:  0.0363, max_reward_err_index: 1, max_kl_dist:  0.7286, max_kl_dist_index: 2, max_train_grp_loss:  16.2720, max_train_grp_loss_index: 1, max_val_grp_loss:  15.5082, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2347, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:37,941 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  13, train_loss:  13.4586, val_loss:  12.3607, grad_norm: 0.0558, live_grad: 0.0000, reward_err: 0.0020, 0.0377, 0.0025, KL_dist: 0.6892, 0.3800, 0.7343, param: [3.8935569  9.63050022 4.92282695 9.56247183], weights: [0.0912379  0.13771362 0.77104848], train_wt_loss:  40.3757, val_wt_loss: 37.0822, train_grp_loss: [11.40097248 16.33258943 10.99878369], val_grp_loss: [10.30939771 15.59943242 11.19277088], train_hist_grp_loss: [1.3900065  1.80171235 3.52428732], cur_train_grp_loss: [0.09229277 0.1261392  0.23437502], max_reward_err:  0.0377, max_reward_err_index: 1, max_kl_dist:  0.7343, max_kl_dist_index: 2, max_train_grp_loss:  16.3326, max_train_grp_loss_index: 1, max_val_grp_loss:  15.5994, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2344, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:38,925 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  14, train_loss:  13.4650, val_loss:  12.3766, grad_norm: 0.0664, live_grad: 0.0000, reward_err: 0.0020, 0.0384, 0.0025, KL_dist: 0.6930, 0.3840, 0.7402, param: [3.86169344 9.65488156 4.90302617 9.62798905], weights: [0.08127614 0.12700472 0.79171913], train_wt_loss:  40.3951, val_wt_loss: 37.1297, train_grp_loss: [11.35637406 16.39663767 10.98200466], val_grp_loss: [10.2710595  15.69502625 11.18350261], train_hist_grp_loss: [1.48194983 1.92832157 3.75830399], cur_train_grp_loss: [0.09194333 0.12660922 0.23401667], max_reward_err:  0.0384, max_reward_err_index: 1, max_kl_dist:  0.7402, max_kl_dist_index: 2, max_train_grp_loss:  16.3966, max_train_grp_loss_index: 1, max_val_grp_loss:  15.6950, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2340, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:39,931 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  15, train_loss:  13.4725, val_loss:  12.3936, grad_norm: 0.0779, live_grad: 0.0000, reward_err: 0.0019, 0.0395, 0.0026, KL_dist: 0.6967, 0.3881, 0.7460, param: [3.82859474 9.68017224 4.88121269 9.69248812], weights: [0.07221595 0.11692757 0.81085648], train_wt_loss:  40.4175, val_wt_loss: 37.1808, train_grp_loss: [11.31102714 16.46356682 10.96551145], val_grp_loss: [10.23242344 15.79415207 11.17461395], train_hist_grp_loss: [1.57353349 2.05542729 3.99196366], cur_train_grp_loss: [0.09158366 0.12710572 0.23365967], max_reward_err:  0.0395, max_reward_err_index: 1, max_kl_dist:  0.7460, max_kl_dist_index: 2, max_train_grp_loss:  16.4636, max_train_grp_loss_index: 1, max_val_grp_loss:  15.7942, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2337, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:40,937 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  16, train_loss:  13.4809, val_loss:  12.4117, grad_norm: 0.0902, live_grad: 0.0000, reward_err: 0.0018, 0.0404, 0.0028, KL_dist: 0.7004, 0.3923, 0.7518, param: [3.79450794 9.70612512 4.85763542 9.75533038], weights: [0.0640134  0.10748946 0.82849714], train_wt_loss:  40.4427, val_wt_loss: 37.2352, train_grp_loss: [11.26541223 16.53283277 10.94949574], val_grp_loss: [10.19389446 15.89594612 11.16624626], train_hist_grp_loss: [1.66475145 2.18305184 4.22527242], cur_train_grp_loss: [0.09121796 0.12762455 0.23330875], max_reward_err:  0.0404, max_reward_err_index: 1, max_kl_dist:  0.7518, max_kl_dist_index: 2, max_train_grp_loss:  16.5328, max_train_grp_loss_index: 1, max_val_grp_loss:  15.8959, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2333, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:41,939 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  17, train_loss:  13.4903, val_loss:  12.4308, grad_norm: 0.1032, live_grad: 0.0000, reward_err: 0.0017, 0.0413, 0.0029, KL_dist: 0.7039, 0.3965, 0.7575, param: [3.75968644 9.73254065 4.83256277 9.81595035], weights: [0.05661855 0.09868653 0.84469492], train_wt_loss:  40.4708, val_wt_loss: 37.2925, train_grp_loss: [11.2199639  16.60389138 10.93411632], val_grp_loss: [10.15583044 15.99956387 11.15850964], train_hist_grp_loss: [1.75560155 2.31121333 4.45824041], cur_train_grp_loss: [0.0908501  0.12816149 0.23296799], max_reward_err:  0.0413, max_reward_err_index: 1, max_kl_dist:  0.7575, max_kl_dist_index: 2, max_train_grp_loss:  16.6039, max_train_grp_loss_index: 1, max_val_grp_loss:  15.9996, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2330, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:42,961 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  18, train_loss:  13.5005, val_loss:  12.4508, grad_norm: 0.1168, live_grad: 0.0000, reward_err: 0.0017, 0.0413, 0.0029, KL_dist: 0.7074, 0.4008, 0.7631, param: [3.72438427 9.75926209 4.8062744  9.87386442], weights: [0.04997756 0.09050586 0.85951658], train_wt_loss:  40.5016, val_wt_loss: 37.3523, train_grp_loss: [11.17506595 16.67620689 10.91949861], val_grp_loss: [10.11853931 16.10419396 11.15148329], train_hist_grp_loss: [1.84608513 2.43992567 4.69088119], cur_train_grp_loss: [0.09048358 0.12871234 0.23264077], max_reward_err:  0.0413, max_reward_err_index: 1, max_kl_dist:  0.7631, max_kl_dist_index: 2, max_train_grp_loss:  16.6762, max_train_grp_loss_index: 1, max_val_grp_loss:  16.1042, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2326, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:43,960 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  19, train_loss:  13.5116, val_loss:  12.4714, grad_norm: 0.1309, live_grad: 0.0000, reward_err: 0.0016, 0.0426, 0.0032, KL_dist: 0.7106, 0.4050, 0.7685, param: [3.6888506  9.78616901 4.77905317 9.92867594], weights: [0.04403456 0.0829275  0.87303795], train_wt_loss:  40.5347, val_wt_loss: 37.4142, train_grp_loss: [11.13104902 16.74925977 10.90573572], val_grp_loss: [10.08227851 16.20907031 11.14521715], train_hist_grp_loss: [1.93620663 2.56919859 4.92321094], cur_train_grp_loss: [0.0901215  0.12927292 0.23232976], max_reward_err:  0.0426, max_reward_err_index: 1, max_kl_dist:  0.7685, max_kl_dist_index: 2, max_train_grp_loss:  16.7493, max_train_grp_loss_index: 1, max_val_grp_loss:  16.2091, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2323, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:44,981 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  20, train_loss:  13.5234, val_loss:  12.4925, grad_norm: 0.1452, live_grad: 0.0000, reward_err: 0.0014, 0.0442, 0.0036, KL_dist: 0.7138, 0.4093, 0.7738, param: [3.65332465 9.81316968 4.75117756 9.98007667], weights: [0.03873322 0.07592624 0.88534054], train_wt_loss:  40.5701, val_wt_loss: 37.4776, train_grp_loss: [11.08819037 16.82255397 10.89289054], val_grp_loss: [10.04725613 16.31348193 11.13973462], train_hist_grp_loss: [2.02597315 2.69903782 5.15524787], cur_train_grp_loss: [0.08976652 0.12983922 0.23203693], max_reward_err:  0.0442, max_reward_err_index: 1, max_kl_dist:  0.7738, max_kl_dist_index: 2, max_train_grp_loss:  16.8226, max_train_grp_loss_index: 1, max_val_grp_loss:  16.3135, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2320, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:45,992 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  21, train_loss:  13.5358, val_loss:  12.5140, grad_norm: 0.1597, live_grad: 0.0000, reward_err: 0.0012, 0.0450, 0.0039, KL_dist: 0.7167, 0.4134, 0.7788, param: [ 3.6180312   9.84019343  4.72291507 10.02784539], weights: [0.03401803 0.06947325 0.89650873], train_wt_loss:  40.6074, val_wt_loss: 37.5421, train_grp_loss: [11.04671531 16.89562325 10.88099857], val_grp_loss: [10.01363359 16.41678057 11.13503585], train_hist_grp_loss: [2.11539404 2.82944521 5.3870115 ], cur_train_grp_loss: [0.08942089 0.1304074  0.23176363], max_reward_err:  0.0450, max_reward_err_index: 1, max_kl_dist:  0.7788, max_kl_dist_index: 2, max_train_grp_loss:  16.8956, max_train_grp_loss_index: 1, max_val_grp_loss:  16.4168, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2318, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:46,995 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  22, train_loss:  13.5487, val_loss:  12.5357, grad_norm: 0.1743, live_grad: 0.0000, reward_err: 0.0012, 0.0460, 0.0039, KL_dist: 0.7195, 0.4176, 0.7836, param: [ 3.583177    9.86718348  4.69451676 10.07184377], weights: [0.02983523 0.06353738 0.90662739], train_wt_loss:  40.6461, val_wt_loss: 37.6072, train_grp_loss: [11.00680013 16.96803659 10.87007139], val_grp_loss: [ 9.98152932 16.51838616 11.13110153], train_hist_grp_loss: [2.20448045 2.96041903 5.61852211], cur_train_grp_loss: [0.08908641 0.13097382 0.23151061], max_reward_err:  0.0460, max_reward_err_index: 1, max_kl_dist:  0.7836, max_kl_dist_index: 2, max_train_grp_loss:  16.9680, max_train_grp_loss_index: 1, max_val_grp_loss:  16.5184, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2315, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:47,984 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  23, train_loss:  13.5620, val_loss:  12.5575, grad_norm: 0.1888, live_grad: 0.0000, reward_err: 0.0011, 0.0472, 0.0040, KL_dist: 0.7222, 0.4216, 0.7882, param: [ 3.54894802  9.89409079  4.66621303 10.11201031], weights: [0.02613358 0.05808632 0.9157801 ], train_wt_loss:  40.6861, val_wt_loss: 37.6724, train_grp_loss: [10.96857599 17.03940242 10.86010026], val_grp_loss: [ 9.95102317 16.6177899  11.12789673], train_hist_grp_loss: [2.29324497 3.0919542  5.84980022], cur_train_grp_loss: [0.08876452 0.13153517 0.23127811], max_reward_err:  0.0472, max_reward_err_index: 1, max_kl_dist:  0.7882, max_kl_dist_index: 2, max_train_grp_loss:  17.0394, max_train_grp_loss_index: 1, max_val_grp_loss:  16.6178, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2313, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:48,977 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  24, train_loss:  13.5756, val_loss:  12.5791, grad_norm: 0.2031, live_grad: 0.0000, reward_err: 0.0010, 0.0477, 0.0044, KL_dist: 0.7246, 0.4255, 0.7926, param: [ 3.51550777  9.92086917  4.63821068 10.14835269], weights: [0.0228648  0.05308743 0.92404777], train_wt_loss:  40.7269, val_wt_loss: 37.7373, train_grp_loss: [10.93213345 17.10937161 10.85105991], val_grp_loss: [ 9.92216109 16.71455546 11.12537471], train_hist_grp_loss: [2.38170123 3.22404259 6.08086619], cur_train_grp_loss: [0.08845626 0.13208839 0.23106596], max_reward_err:  0.0477, max_reward_err_index: 1, max_kl_dist:  0.7926, max_kl_dist_index: 2, max_train_grp_loss:  17.1094, max_train_grp_loss_index: 1, max_val_grp_loss:  16.7146, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2311, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:49,983 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  25, train_loss:  13.5894, val_loss:  12.6005, grad_norm: 0.2172, live_grad: 0.0000, reward_err: 0.0007, 0.0477, 0.0049, KL_dist: 0.7269, 0.4293, 0.7967, param: [ 3.48299636  9.9474717   4.61069124 10.1809391 ], weights: [0.01998392 0.04850845 0.93150763], train_wt_loss:  40.7683, val_wt_loss: 37.8016, train_grp_loss: [10.89752737 17.17763932 10.84291211], val_grp_loss: [ 9.89496006 16.80831823 11.12348045], train_hist_grp_loss: [2.4698636  3.35667338 6.3117398 ], cur_train_grp_loss: [0.08816237 0.13263079 0.23087362], max_reward_err:  0.0477, max_reward_err_index: 1, max_kl_dist:  0.7967, max_kl_dist_index: 2, max_train_grp_loss:  17.1776, max_train_grp_loss_index: 1, max_val_grp_loss:  16.8083, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2309, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:50,993 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  26, train_loss:  13.6033, val_loss:  12.6216, grad_norm: 0.2309, live_grad: 0.0000, reward_err: 0.0006, 0.0482, 0.0051, KL_dist: 0.7291, 0.4330, 0.8006, param: [ 3.45153046  9.97384867  4.58381027 10.209889  ], weights: [0.01744939 0.04431801 0.9382326 ], train_wt_loss:  40.8098, val_wt_loss: 37.8647, train_grp_loss: [10.86478194 17.24394555 10.83560904], val_grp_loss: [ 9.86941276 16.898783   11.12215379], train_hist_grp_loss: [2.55774688 3.48983337 6.54244006], cur_train_grp_loss: [0.08788329 0.13315999 0.23070026], max_reward_err:  0.0482, max_reward_err_index: 1, max_kl_dist:  0.8006, max_kl_dist_index: 2, max_train_grp_loss:  17.2439, max_train_grp_loss_index: 1, max_val_grp_loss:  16.8988, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2307, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:51,995 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  27, train_loss:  13.6170, val_loss:  12.6422, grad_norm: 0.2443, live_grad: 0.0000, reward_err: 0.0005, 0.0482, 0.0057, KL_dist: 0.7311, 0.4366, 0.8042, param: [ 3.42120392  9.99994652  4.55769771 10.23536368], weights: [0.01522312 0.04048602 0.94429086], train_wt_loss:  40.8511, val_wt_loss: 37.9265, train_grp_loss: [10.83389549 17.30807461 10.8290963 ], val_grp_loss: [ 9.84549206 16.9857203  11.12133216], train_hist_grp_loss: [2.64536609 3.62350737 6.77298493], cur_train_grp_loss: [0.08761921 0.133674   0.23054487], max_reward_err:  0.0482, max_reward_err_index: 1, max_kl_dist:  0.8042, max_kl_dist_index: 2, max_train_grp_loss:  17.3081, max_train_grp_loss_index: 1, max_val_grp_loss:  16.9857, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2305, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:53,005 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  28, train_loss:  13.6307, val_loss:  12.6622, grad_norm: 0.2572, live_grad: 0.0000, reward_err: 0.0004, 0.0487, 0.0059, KL_dist: 0.7329, 0.4400, 0.8077, param: [ 3.3920889  10.02570793  4.53245879 10.25755709], weights: [0.01327044 0.03698386 0.9497457 ], train_wt_loss:  40.8921, val_wt_loss: 37.9867, train_grp_loss: [10.80484514 17.36985369 10.82331552], val_grp_loss: [ 9.82315518 17.06896181 11.12095286], train_hist_grp_loss: [2.73273622 3.75767849 7.00339124], cur_train_grp_loss: [0.08737012 0.13417112 0.2304063 ], max_reward_err:  0.0487, max_reward_err_index: 1, max_kl_dist:  0.8077, max_kl_dist_index: 2, max_train_grp_loss:  17.3699, max_train_grp_loss_index: 1, max_val_grp_loss:  17.0690, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2304, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:54,017 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  29, train_loss:  13.6441, val_loss:  12.6817, grad_norm: 0.2695, live_grad: 0.0000, reward_err: 0.0003, 0.0502, 0.0061, KL_dist: 0.7346, 0.4432, 0.8109, param: [ 3.36423741 10.05107262  4.50817561 10.27668698], weights: [0.01155997 0.03378458 0.95465545], train_wt_loss:  40.9324, val_wt_loss: 38.0450, train_grp_loss: [10.77759091 17.42915058 10.81820655], val_grp_loss: [ 9.80234726 17.14839496 11.12095479], train_hist_grp_loss: [2.81987206 3.89232852 7.23367455], cur_train_grp_loss: [0.08713585 0.13465003 0.23028331], max_reward_err:  0.0502, max_reward_err_index: 1, max_kl_dist:  0.8109, max_kl_dist_index: 2, max_train_grp_loss:  17.4292, max_train_grp_loss_index: 1, max_val_grp_loss:  17.1484, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2303, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:55,022 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  30, train_loss:  13.6573, val_loss:  12.7004, grad_norm: 0.2814, live_grad: 0.0000, reward_err: 0.0003, 0.0509, 0.0063, KL_dist: 0.7362, 0.4464, 0.8139, param: [ 3.33768312 10.07597865  4.48490904 10.2929869 ], weights: [0.01006342 0.03086296 0.95907362], train_wt_loss:  40.9718, val_wt_loss: 38.1012, train_grp_loss: [10.75207955 17.48587077 10.81370922], val_grp_loss: [ 9.78300467 17.22395714 11.12127982], train_hist_grp_loss: [2.90678812 4.02743821 7.46384915], cur_train_grp_loss: [0.08691606 0.13510969 0.23017461], max_reward_err:  0.0509, max_reward_err_index: 1, max_kl_dist:  0.8139, max_kl_dist_index: 2, max_train_grp_loss:  17.4859, max_train_grp_loss_index: 1, max_val_grp_loss:  17.2240, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2302, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:56,032 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  31, train_loss:  13.6701, val_loss:  12.7184, grad_norm: 0.2927, live_grad: 0.0000, reward_err: 0.0003, 0.0518, 0.0063, KL_dist: 0.7376, 0.4493, 0.8167, param: [ 3.31244331 10.10036402  4.46270082 10.30669893], weights: [0.00875544 0.02819554 0.96304902], train_wt_loss:  41.0102, val_wt_loss: 38.1552, train_grp_loss: [10.72824781 17.53995415 10.80976478], val_grp_loss: [ 9.76505768 17.29562979 11.12187372], train_hist_grp_loss: [2.99349844 4.1629876  7.69392807], cur_train_grp_loss: [0.08671032 0.13554939 0.23007892], max_reward_err:  0.0518, max_reward_err_index: 1, max_kl_dist:  0.8167, max_kl_dist_index: 2, max_train_grp_loss:  17.5400, max_train_grp_loss_index: 1, max_val_grp_loss:  17.2956, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2301, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:57,047 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  32, train_loss:  13.6824, val_loss:  12.7356, grad_norm: 0.3034, live_grad: 0.0000, reward_err: 0.0003, 0.0523, 0.0066, KL_dist: 0.7390, 0.4521, 0.8192, param: [ 3.2885209  10.12416835  4.44157583 10.31806742], weights: [0.00761336 0.02576057 0.96662607], train_wt_loss:  41.0473, val_wt_loss: 38.2068, train_grp_loss: [10.7060252  17.59137147 10.80631686], val_grp_loss: [ 9.74843279 17.36343239 11.12268681], train_hist_grp_loss: [3.08001657 4.29895624 7.92392307], cur_train_grp_loss: [0.08651813 0.13596864 0.229995  ], max_reward_err:  0.0523, max_reward_err_index: 1, max_kl_dist:  0.8192, max_kl_dist_index: 2, max_train_grp_loss:  17.5914, max_train_grp_loss_index: 1, max_val_grp_loss:  17.3634, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2300, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:58,041 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  33, train_loss:  13.6944, val_loss:  12.7520, grad_norm: 0.3136, live_grad: 0.0000, reward_err: 0.0003, 0.0533, 0.0066, KL_dist: 0.7402, 0.4548, 0.8217, param: [ 3.2659064  10.1473345   4.42154426 10.32733366], weights: [0.00661703 0.02353797 0.96984499], train_wt_loss:  41.0831, val_wt_loss: 38.2561, train_grp_loss: [10.68533647 17.64012065 10.80331229], val_grp_loss: [ 9.73305462 17.42741679 11.12367421], train_hist_grp_loss: [3.16635548 4.43532346 8.1538447 ], cur_train_grp_loss: [0.08633891 0.13636722 0.22992164], max_reward_err:  0.0533, max_reward_err_index: 1, max_kl_dist:  0.8217, max_kl_dist_index: 2, max_train_grp_loss:  17.6401, max_train_grp_loss_index: 1, max_val_grp_loss:  17.4274, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2299, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:17:59,053 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  34, train_loss:  13.7058, val_loss:  12.7677, grad_norm: 0.3232, live_grad: 0.0000, reward_err: 0.0003, 0.0543, 0.0066, KL_dist: 0.7413, 0.4573, 0.8239, param: [ 3.24457986 10.16981001  4.40260381 10.33473161], weights: [0.00574853 0.0215093  0.97274217], train_wt_loss:  41.1175, val_wt_loss: 38.3030, train_grp_loss: [10.66610347 17.68622311 10.80070148], val_grp_loss: [ 9.71884743 17.48766169 11.12479599], train_hist_grp_loss: [3.25252755 4.57206858 8.38370241], cur_train_grp_loss: [0.08617207 0.13674512 0.22985771], max_reward_err:  0.0543, max_reward_err_index: 1, max_kl_dist:  0.8239, max_kl_dist_index: 2, max_train_grp_loss:  17.6862, max_train_grp_loss_index: 1, max_val_grp_loss:  17.4877, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2299, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:00,048 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  35, train_loss:  13.7168, val_loss:  12.7825, grad_norm: 0.3323, live_grad: 0.0000, reward_err: 0.0003, 0.0548, 0.0068, KL_dist: 0.7424, 0.4596, 0.8260, param: [ 3.22451265 10.19154826  4.38474171 10.3404845 ], weights: [0.004992   0.01965759 0.97535041], train_wt_loss:  41.1504, val_wt_loss: 38.3475, train_grp_loss: [10.64824685 17.72972028 10.79843877], val_grp_loss: [ 9.70573633 17.54426773 11.1260171 ], train_hist_grp_loss: [3.33854451 4.70917108 8.61350457], cur_train_grp_loss: [0.08601696 0.1371025  0.22980216], max_reward_err:  0.0548, max_reward_err_index: 1, max_kl_dist:  0.8260, max_kl_dist_index: 2, max_train_grp_loss:  17.7297, max_train_grp_loss_index: 1, max_val_grp_loss:  17.5443, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2298, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:01,044 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  36, train_loss:  13.7273, val_loss:  12.7965, grad_norm: 0.3408, live_grad: 0.0000, reward_err: 0.0002, 0.0552, 0.0071, KL_dist: 0.7433, 0.4619, 0.8279, param: [ 3.20566915 10.21250936  4.36793656 10.34480238], weights: [0.00433344 0.01796732 0.97769925], train_wt_loss:  41.1818, val_wt_loss: 38.3896, train_grp_loss: [10.63168726 17.77067026 10.79648244], val_grp_loss: [ 9.69364819 17.59735285 11.12730713], train_hist_grp_loss: [3.42441747 4.84661078 8.84325859], cur_train_grp_loss: [0.08587296 0.13743969 0.22975402], max_reward_err:  0.0552, max_reward_err_index: 1, max_kl_dist:  0.8279, max_kl_dist_index: 2, max_train_grp_loss:  17.7707, max_train_grp_loss_index: 1, max_val_grp_loss:  17.5974, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2298, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:02,049 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  37, train_loss:  13.7372, val_loss:  12.8097, grad_norm: 0.3488, live_grad: 0.0000, reward_err: 0.0001, 0.0561, 0.0075, KL_dist: 0.7442, 0.4639, 0.8297, param: [ 3.18800817 10.23266072  4.35216008 10.34788038], weights: [0.00376049 0.01642428 0.97981522], train_wt_loss:  41.2116, val_wt_loss: 38.4292, train_grp_loss: [10.61634645 17.80914466 10.79479475], val_grp_loss: [ 9.6825123  17.64704824 11.12864006], train_hist_grp_loss: [3.51015688 4.98436791 9.07297098], cur_train_grp_loss: [0.08573941 0.13775713 0.22971239], max_reward_err:  0.0561, max_reward_err_index: 1, max_kl_dist:  0.8297, max_kl_dist_index: 2, max_train_grp_loss:  17.8091, max_train_grp_loss_index: 1, max_val_grp_loss:  17.6470, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2297, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:03,054 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  38, train_loss:  13.7466, val_loss:  12.8222, grad_norm: 0.3562, live_grad: 0.0000, reward_err: 0.0001, 0.0561, 0.0075, KL_dist: 0.7450, 0.4659, 0.8313, param: [ 3.17148436 10.25197733  4.3373786  10.34989772], weights: [0.0032623  0.01501552 0.98172218], train_wt_loss:  41.2399, val_wt_loss: 38.4666, train_grp_loss: [10.60214797 17.84522585 10.79334177], val_grp_loss: [ 9.67226091 17.69349476 11.12999389], train_hist_grp_loss: [3.59577258 5.1224233  9.30264746], cur_train_grp_loss: [0.0856157  0.13805538 0.22967648], max_reward_err:  0.0561, max_reward_err_index: 1, max_kl_dist:  0.8313, max_kl_dist_index: 2, max_train_grp_loss:  17.8452, max_train_grp_loss_index: 1, max_val_grp_loss:  17.6935, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2297, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:04,041 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  39, train_loss:  13.7555, val_loss:  12.8339, grad_norm: 0.3632, live_grad: 0.0000, reward_err: 0.0001, 0.0571, 0.0075, KL_dist: 0.7458, 0.4677, 0.8328, param: [ 3.15604933 10.27044178  4.32355435 10.35101726], weights: [0.00282933 0.01372918 0.98344149], train_wt_loss:  41.2666, val_wt_loss: 38.5017, train_grp_loss: [10.58901779 17.87900441 10.79209321], val_grp_loss: [ 9.66282953 17.73683981 11.13135028], train_hist_grp_loss: [3.68127377 5.26075838 9.53229303], cur_train_grp_loss: [0.08550119 0.13833508 0.22964557], max_reward_err:  0.0571, max_reward_err_index: 1, max_kl_dist:  0.8328, max_kl_dist_index: 2, max_train_grp_loss:  17.8790, max_train_grp_loss_index: 1, max_val_grp_loss:  17.7368, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2296, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:05,041 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  40, train_loss:  13.7639, val_loss:  12.8449, grad_norm: 0.3697, live_grad: 0.0000, reward_err: 0.0001, 0.0576, 0.0075, KL_dist: 0.7465, 0.4694, 0.8342, param: [ 3.14165271 10.28804405  4.31064668 10.35138553], weights: [0.00245319 0.01255447 0.98499233], train_wt_loss:  41.2918, val_wt_loss: 38.5346, train_grp_loss: [10.57688474 17.91057687 10.79102215], val_grp_loss: [ 9.6541571  17.77723468 11.13269417], train_hist_grp_loss: [3.76666908 5.39935531 9.76191204], cur_train_grp_loss: [0.0853953  0.13859693 0.229619  ], max_reward_err:  0.0576, max_reward_err_index: 1, max_kl_dist:  0.8342, max_kl_dist_index: 2, max_train_grp_loss:  17.9106, max_train_grp_loss_index: 1, max_val_grp_loss:  17.7772, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2296, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:06,034 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  41, train_loss:  13.7718, val_loss:  12.8551, grad_norm: 0.3758, live_grad: 0.0000, reward_err: 0.0001, 0.0586, 0.0075, KL_dist: 0.7471, 0.4710, 0.8355, param: [ 3.12824296 10.30478115  4.29861298 10.35113315], weights: [0.00212657 0.01148156 0.98639186], train_wt_loss:  41.3154, val_wt_loss: 38.5653, train_grp_loss: [10.56568075 17.94004378 10.79010482], val_grp_loss: [ 9.64618614 17.81483225 11.1340134 ], train_hist_grp_loss: [3.85196654 5.53819699 9.99150825], cur_train_grp_loss: [0.08529746 0.13884168 0.22959622], max_reward_err:  0.0586, max_reward_err_index: 1, max_kl_dist:  0.8355, max_kl_dist_index: 2, max_train_grp_loss:  17.9400, max_train_grp_loss_index: 1, max_val_grp_loss:  17.8148, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2296, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:07,023 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  42, train_loss:  13.7792, val_loss:  12.8647, grad_norm: 0.3814, live_grad: 0.0000, reward_err: 0.0001, 0.0586, 0.0075, KL_dist: 0.7477, 0.4725, 0.8367, param: [ 3.11576816 10.32065653  4.28740953 10.35037551], weights: [0.00184305 0.01050148 0.98765547], train_wt_loss:  41.3377, val_wt_loss: 38.5941, train_grp_loss: [10.55534109 17.967508   10.78932032], val_grp_loss: [ 9.63886274 17.84978518 11.13529834], train_hist_grp_loss: [ 3.93717364  5.6772671  10.22108495], cur_train_grp_loss: [0.0852071  0.13907011 0.2295767 ], max_reward_err:  0.0586, max_reward_err_index: 1, max_kl_dist:  0.8367, max_kl_dist_index: 2, max_train_grp_loss:  17.9675, max_train_grp_loss_index: 1, max_val_grp_loss:  17.8498, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2296, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:08,040 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  43, train_loss:  13.7862, val_loss:  12.8736, grad_norm: 0.3866, live_grad: 0.0000, reward_err: 0.0001, 0.0586, 0.0080, KL_dist: 0.7482, 0.4739, 0.8378, param: [ 3.10417661 10.33567954  4.27699221 10.34921369], weights: [0.00159702 0.00960606 0.98879692], train_wt_loss:  41.3585, val_wt_loss: 38.6209, train_grp_loss: [10.5458044  17.99307332 10.78865031], val_grp_loss: [ 9.6321365  17.8822443  11.13654156], train_hist_grp_loss: [ 4.02229736  5.81655011 10.45064496], cur_train_grp_loss: [0.08512372 0.13928301 0.22956001], max_reward_err:  0.0586, max_reward_err_index: 1, max_kl_dist:  0.8378, max_kl_dist_index: 2, max_train_grp_loss:  17.9931, max_train_grp_loss_index: 1, max_val_grp_loss:  17.8822, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2296, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:09,027 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  44, train_loss:  13.7927, val_loss:  12.8820, grad_norm: 0.3915, live_grad: 0.0000, reward_err: 0.0001, 0.0591, 0.0080, KL_dist: 0.7487, 0.4752, 0.8388, param: [ 3.09341729 10.34986467  4.26731704 10.34773545], weights: [0.0013836  0.00878787 0.98982854], train_wt_loss:  41.3780, val_wt_loss: 38.6459, train_grp_loss: [10.53701281 18.0168432  10.7880788 ], val_grp_loss: [ 9.62596044 17.91235743 11.13773753], train_hist_grp_loss: [ 4.10734417  5.9560313  10.68019071], cur_train_grp_loss: [0.08504681 0.13948119 0.22954575], max_reward_err:  0.0591, max_reward_err_index: 1, max_kl_dist:  0.8388, max_kl_dist_index: 2, max_train_grp_loss:  18.0168, max_train_grp_loss_index: 1, max_val_grp_loss:  17.9124, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:10,074 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  45, train_loss:  13.7987, val_loss:  12.8897, grad_norm: 0.3960, live_grad: 0.0000, reward_err: 0.0001, 0.0594, 0.0080, KL_dist: 0.7491, 0.4764, 0.8398, param: [ 3.08344031 10.36323092  4.25834065 10.34601639], weights: [0.0011985  0.00804014 0.99076136], train_wt_loss:  41.3962, val_wt_loss: 38.6692, train_grp_loss: [10.52891183 18.03891982 10.78759187], val_grp_loss: [ 9.62029093 17.94026834 11.13888231], train_hist_grp_loss: [ 4.19232008  6.09569675 10.9097243 ], cur_train_grp_loss: [0.08497591 0.13966545 0.22953359], max_reward_err:  0.0594, max_reward_err_index: 1, max_kl_dist:  0.8398, max_kl_dist_index: 2, max_train_grp_loss:  18.0389, max_train_grp_loss_index: 1, max_val_grp_loss:  17.9403, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:11,061 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  46, train_loss:  13.8044, val_loss:  12.8969, grad_norm: 0.4002, live_grad: 0.0000, reward_err: 0.0001, 0.0594, 0.0080, KL_dist: 0.7495, 0.4775, 0.8406, param: [ 3.0741972  10.37580102  4.25002065 10.34412101], weights: [0.00103801 0.0073567  0.99160529], train_wt_loss:  41.4132, val_wt_loss: 38.6908, train_grp_loss: [10.52145035 18.05940327 10.78717745], val_grp_loss: [ 9.61508746 17.96611604 11.1399733 ], train_hist_grp_loss: [ 4.27723066  6.23553334 11.13924753], cur_train_grp_loss: [0.08491058 0.13983659 0.22952323], max_reward_err:  0.0594, max_reward_err_index: 1, max_kl_dist:  0.8406, max_kl_dist_index: 2, max_train_grp_loss:  18.0594, max_train_grp_loss_index: 1, max_val_grp_loss:  17.9661, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:12,061 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  47, train_loss:  13.8097, val_loss:  12.9036, grad_norm: 0.4040, live_grad: 0.0000, reward_err: 0.0001, 0.0594, 0.0080, KL_dist: 0.7499, 0.4785, 0.8414, param: [ 3.06564118 10.38760082  4.24231589 10.34210391], weights: [8.98898011e-04 6.73193048e-03 9.92369172e-01], train_wt_loss:  41.4290, val_wt_loss: 38.7109, train_grp_loss: [10.5145805  18.07839091 10.78682508], val_grp_loss: [ 9.61031254 17.99003419 11.14100904], train_hist_grp_loss: [ 4.36208106  6.37552871 11.36876195], cur_train_grp_loss: [0.08485041 0.13999537 0.22951441], max_reward_err:  0.0594, max_reward_err_index: 1, max_kl_dist:  0.8414, max_kl_dist_index: 2, max_train_grp_loss:  18.0784, max_train_grp_loss_index: 1, max_val_grp_loss:  17.9900, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:13,067 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  48, train_loss:  13.8146, val_loss:  12.9098, grad_norm: 0.4076, live_grad: 0.0000, reward_err: 0.0001, 0.0594, 0.0080, KL_dist: 0.7502, 0.4794, 0.8421, param: [ 3.05772729 10.39865858  4.2351867  10.34001084], weights: [7.78333048e-04 6.16072488e-03 9.93060942e-01], train_wt_loss:  41.4437, val_wt_loss: 38.7295, train_grp_loss: [10.50825759 18.09597688 10.78652576], val_grp_loss: [ 9.60593152 18.01215074 11.14198899], train_hist_grp_loss: [ 4.44687607  6.51567128 11.59826886], cur_train_grp_loss: [0.084795   0.14014257 0.22950692], max_reward_err:  0.0594, max_reward_err_index: 1, max_kl_dist:  0.8421, max_kl_dist_index: 2, max_train_grp_loss:  18.0960, max_train_grp_loss_index: 1, max_val_grp_loss:  18.0122, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:14,067 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  49, train_loss:  13.8191, val_loss:  12.9156, grad_norm: 0.4109, live_grad: 0.0000, reward_err: 0.0001, 0.0594, 0.0080, KL_dist: 0.7505, 0.4803, 0.8428, param: [ 3.05041259 10.40900441  4.22859501 10.33787976], weights: [6.73864421e-04 5.63841920e-03 9.93687716e-01], train_wt_loss:  41.4574, val_wt_loss: 38.7468, train_grp_loss: [10.50243992 18.11225172 10.78627171], val_grp_loss: [ 9.60191239 18.03258763 11.14291333], train_hist_grp_loss: [ 4.53162008  6.65595017 11.82776941], cur_train_grp_loss: [0.08474401 0.14027889 0.22950055], max_reward_err:  0.0594, max_reward_err_index: 1, max_kl_dist:  0.8428, max_kl_dist_index: 2, max_train_grp_loss:  18.1123, max_train_grp_loss_index: 1, max_val_grp_loss:  18.0326, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:15,089 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  50, train_loss:  13.8234, val_loss:  12.9209, grad_norm: 0.4139, live_grad: 0.0000, reward_err: 0.0001, 0.0600, 0.0080, KL_dist: 0.7508, 0.4811, 0.8434, param: [ 3.04365615 10.41866972  4.22250451 10.33574179], weights: [5.83358833e-04 5.16076723e-03 9.94255874e-01], train_wt_loss:  41.4701, val_wt_loss: 38.7627, train_grp_loss: [10.49708867 18.12730211 10.78605624], val_grp_loss: [ 9.59822564 18.05146067 11.14378288], train_hist_grp_loss: [ 4.61631717  6.79635522 12.05726455], cur_train_grp_loss: [0.0846971  0.14040505 0.22949514], max_reward_err:  0.0600, max_reward_err_index: 1, max_kl_dist:  0.8434, max_kl_dist_index: 2, max_train_grp_loss:  18.1273, max_train_grp_loss_index: 1, max_val_grp_loss:  18.0515, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:16,087 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  51, train_loss:  13.8273, val_loss:  12.9258, grad_norm: 0.4167, live_grad: 0.0000, reward_err: 0.0001, 0.0600, 0.0080, KL_dist: 0.7511, 0.4819, 0.8440, param: [ 3.03741917 10.42768672  4.21688064 10.33362213], weights: [5.04962405e-04 4.72389904e-03 9.94771139e-01], train_wt_loss:  41.4818, val_wt_loss: 38.7775, train_grp_loss: [10.49216778 18.14121071 10.78587364], val_grp_loss: [ 9.59484409 18.06887952 11.14459891], train_hist_grp_loss: [ 4.70097112  6.93687694 12.28675511], cur_train_grp_loss: [0.08465394 0.14052172 0.22949056], max_reward_err:  0.0600, max_reward_err_index: 1, max_kl_dist:  0.8440, max_kl_dist_index: 2, max_train_grp_loss:  18.1412, max_train_grp_loss_index: 1, max_val_grp_loss:  18.0689, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:17,097 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  52, train_loss:  13.8309, val_loss:  12.9304, grad_norm: 0.4193, live_grad: 0.0000, reward_err: 0.0001, 0.0603, 0.0080, KL_dist: 0.7513, 0.4826, 0.8445, param: [ 3.03166493 10.43608801  4.21169067 10.33154085], weights: [4.37064779e-04 4.32428691e-03 9.95238648e-01], train_wt_loss:  41.4927, val_wt_loss: 38.7912, train_grp_loss: [10.48764374 18.15405605 10.785719  ], val_grp_loss: [ 9.59174269 18.08494765 11.14536304], train_hist_grp_loss: [ 4.78558537  7.07750648 12.51624179], cur_train_grp_loss: [0.08461426 0.14062954 0.22948667], max_reward_err:  0.0603, max_reward_err_index: 1, max_kl_dist:  0.8445, max_kl_dist_index: 2, max_train_grp_loss:  18.1541, max_train_grp_loss_index: 1, max_val_grp_loss:  18.0849, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:18,117 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  53, train_loss:  13.8343, val_loss:  12.9346, grad_norm: 0.4217, live_grad: 0.0000, reward_err: 0.0001, 0.0606, 0.0080, KL_dist: 0.7515, 0.4832, 0.8450, param: [ 3.02635883 10.44390622  4.2069037  10.32951364], weights: [3.78267690e-04 3.95871427e-03 9.95663018e-01], train_wt_loss:  41.5028, val_wt_loss: 38.8038, train_grp_loss: [10.48348548 18.16591249 10.7855881 ], val_grp_loss: [ 9.58889843 18.0997625  11.14607719], train_hist_grp_loss: [ 4.87016314  7.2182356  12.74572517], cur_train_grp_loss: [0.08457777 0.14072912 0.22948338], max_reward_err:  0.0606, max_reward_err_index: 1, max_kl_dist:  0.8450, max_kl_dist_index: 2, max_train_grp_loss:  18.1659, max_train_grp_loss_index: 1, max_val_grp_loss:  18.0998, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:19,116 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  54, train_loss:  13.8374, val_loss:  12.9385, grad_norm: 0.4239, live_grad: 0.0000, reward_err: 0.0001, 0.0611, 0.0080, KL_dist: 0.7517, 0.4838, 0.8454, param: [ 3.0214683  10.45117367  4.20249059 10.32755247], weights: [3.27357465e-04 3.62424758e-03 9.96048395e-01], train_wt_loss:  41.5121, val_wt_loss: 38.8155, train_grp_loss: [10.47966424 18.17685026 10.78547738], val_grp_loss: [ 9.58629012 18.11341561 11.14674344], train_hist_grp_loss: [ 4.95470738  7.35905662 12.97520577], cur_train_grp_loss: [0.08454424 0.14082103 0.2294806 ], max_reward_err:  0.0611, max_reward_err_index: 1, max_kl_dist:  0.8454, max_kl_dist_index: 2, max_train_grp_loss:  18.1769, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1134, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:20,125 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  55, train_loss:  13.8402, val_loss:  12.9421, grad_norm: 0.4259, live_grad: 0.0000, reward_err: 0.0001, 0.0616, 0.0080, KL_dist: 0.7519, 0.4843, 0.8458, param: [ 3.01696278 10.45792214  4.19842401 10.32566618], weights: [2.83281006e-04 3.31821078e-03 9.96398508e-01], train_wt_loss:  41.5207, val_wt_loss: 38.8262, train_grp_loss: [10.47615336 18.1869355  10.78538377], val_grp_loss: [ 9.58389831 18.12599277 11.14736402], train_hist_grp_loss: [ 5.0392208   7.49996244 13.20468401], cur_train_grp_loss: [0.08451342 0.14090582 0.22947824], max_reward_err:  0.0616, max_reward_err_index: 1, max_kl_dist:  0.8458, max_kl_dist_index: 2, max_train_grp_loss:  18.1869, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1260, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:21,142 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  56, train_loss:  13.8429, val_loss:  12.9454, grad_norm: 0.4278, live_grad: 0.0000, reward_err: 0.0001, 0.0616, 0.0080, KL_dist: 0.7521, 0.4848, 0.8462, param: [ 3.01281366 10.46418264  4.19467831 10.32386095], weights: [2.45124813e-04 3.03816212e-03 9.96716713e-01], train_wt_loss:  41.5287, val_wt_loss: 38.8361, train_grp_loss: [10.47292821 18.19623035 10.78530467], val_grp_loss: [ 9.58170512 18.13757423 11.14794122], train_hist_grp_loss: [ 5.12370591  7.64094644 13.43416026], cur_train_grp_loss: [0.08448511 0.140984   0.22947625], max_reward_err:  0.0616, max_reward_err_index: 1, max_kl_dist:  0.8462, max_kl_dist_index: 2, max_train_grp_loss:  18.1962, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1376, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:22,120 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  57, train_loss:  13.8453, val_loss:  12.9484, grad_norm: 0.4295, live_grad: 0.0000, reward_err: 0.0001, 0.0616, 0.0080, KL_dist: 0.7522, 0.4853, 0.8465, param: [ 3.00899418 10.46998526  4.1912295  10.3221408 ], weights: [2.12096695e-04 2.78187310e-03 9.97006030e-01], train_wt_loss:  41.5360, val_wt_loss: 38.8453, train_grp_loss: [10.46996603 18.2047931  10.78523786], val_grp_loss: [ 9.57969412 18.14823496 11.14847738], train_hist_grp_loss: [ 5.20816501  7.78200249 13.66363483], cur_train_grp_loss: [0.0844591  0.14105605 0.22947457], max_reward_err:  0.0616, max_reward_err_index: 1, max_kl_dist:  0.8465, max_kl_dist_index: 2, max_train_grp_loss:  18.2048, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1482, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:23,124 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  58, train_loss:  13.8476, val_loss:  12.9512, grad_norm: 0.4310, live_grad: 0.0000, reward_err: 0.0001, 0.0622, 0.0080, KL_dist: 0.7524, 0.4857, 0.8468, param: [ 3.00547938 10.47535902  4.18805519 10.32050794], weights: [1.83509833e-04 2.54730940e-03 9.97269181e-01], train_wt_loss:  41.5428, val_wt_loss: 38.8537, train_grp_loss: [10.46724579 18.21267827 10.78518145], val_grp_loss: [ 9.57785024 18.15804488 11.14897483], train_hist_grp_loss: [ 5.29260022  7.92312491 13.89310797], cur_train_grp_loss: [0.08443521 0.14112243 0.22947315], max_reward_err:  0.0622, max_reward_err_index: 1, max_kl_dist:  0.8468, max_kl_dist_index: 2, max_train_grp_loss:  18.2127, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1580, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:24,120 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  59, train_loss:  13.8497, val_loss:  12.9538, grad_norm: 0.4325, live_grad: 0.0000, reward_err: 0.0001, 0.0627, 0.0080, KL_dist: 0.7525, 0.4861, 0.8471, param: [ 3.002246   10.48033178  4.18513449 10.31896314], weights: [1.58768893e-04 2.33261355e-03 9.97508618e-01], train_wt_loss:  41.5490, val_wt_loss: 38.8614, train_grp_loss: [10.46474807 18.21993681 10.78513385], val_grp_loss: [ 9.57615964 18.16706908 11.14943586], train_hist_grp_loss: [ 5.37701349  8.06430846 14.12257992], cur_train_grp_loss: [0.08441327 0.14118355 0.22947195], max_reward_err:  0.0627, max_reward_err_index: 1, max_kl_dist:  0.8471, max_kl_dist_index: 2, max_train_grp_loss:  18.2199, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1671, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:25,156 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  60, train_loss:  13.8516, val_loss:  12.9562, grad_norm: 0.4338, live_grad: 0.0000, reward_err: 0.0001, 0.0627, 0.0080, KL_dist: 0.7526, 0.4865, 0.8474, param: [ 2.99927238 10.48493017  4.18244795 10.31750597], weights: [1.37357950e-04 2.13608919e-03 9.97726553e-01], train_wt_loss:  41.5548, val_wt_loss: 38.8686, train_grp_loss: [10.46245499 18.22661625 10.78509368], val_grp_loss: [ 9.57460963 18.1753681  11.14986271], train_hist_grp_loss: [ 5.46140662  8.20554828 14.35205085], cur_train_grp_loss: [0.08439313 0.14123982 0.22947093], max_reward_err:  0.0627, max_reward_err_index: 1, max_kl_dist:  0.8474, max_kl_dist_index: 2, max_train_grp_loss:  18.2266, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1754, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:26,133 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  61, train_loss:  13.8534, val_loss:  12.9584, grad_norm: 0.4350, live_grad: 0.0000, reward_err: 0.0001, 0.0627, 0.0080, KL_dist: 0.7527, 0.4868, 0.8476, param: [ 2.99653841 10.48917956  4.1799775  10.31613507], weights: [1.18829978e-04 1.95618678e-03 9.97924983e-01], train_wt_loss:  41.5601, val_wt_loss: 38.8752, train_grp_loss: [10.46035003 18.23276085 10.78505982], val_grp_loss: [ 9.57318853 18.18299818 11.15025757], train_hist_grp_loss: [ 5.54578126  8.34683988 14.58152093], cur_train_grp_loss: [0.08437464 0.1412916  0.22947008], max_reward_err:  0.0627, max_reward_err_index: 1, max_kl_dist:  0.8476, max_kl_dist_index: 2, max_train_grp_loss:  18.2328, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1830, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:27,139 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  62, train_loss:  13.8550, val_loss:  12.9604, grad_norm: 0.4362, live_grad: 0.0000, reward_err: 0.0001, 0.0627, 0.0080, KL_dist: 0.7528, 0.4871, 0.8478, param: [ 2.99402541 10.49310402  4.17770634 10.31484838], weights: [1.02797713e-04 1.79149062e-03 9.98105712e-01], train_wt_loss:  41.5650, val_wt_loss: 38.8812, train_grp_loss: [10.45841798 18.23841178 10.78503126], val_grp_loss: [ 9.57188567 18.1900115  11.15062254], train_hist_grp_loss: [ 5.63013892  8.48817911 14.81099029], cur_train_grp_loss: [0.08435766 0.14133923 0.22946936], max_reward_err:  0.0627, max_reward_err_index: 1, max_kl_dist:  0.8478, max_kl_dist_index: 2, max_train_grp_loss:  18.2384, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1900, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:28,138 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  63, train_loss:  13.8565, val_loss:  12.9623, grad_norm: 0.4372, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7529, 0.4874, 0.8480, param: [ 2.99171607 10.49672635  4.1756189  10.31364326], weights: [8.89257178e-05 1.64070703e-03 9.98270367e-01], train_wt_loss:  41.5694, val_wt_loss: 38.8868, train_grp_loss: [10.45664483 18.24360731 10.78500719], val_grp_loss: [ 9.57069123 18.19645645 11.15095964], train_hist_grp_loss: [ 5.714481    8.62956215 15.04045904], cur_train_grp_loss: [0.08434208 0.14138304 0.22946875], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8480, max_kl_dist_index: 2, max_train_grp_loss:  18.2436, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1965, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:29,144 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  64, train_loss:  13.8579, val_loss:  12.9640, grad_norm: 0.4381, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7530, 0.4876, 0.8482, param: [ 2.98959435 10.50006803  4.17370075 10.31251668], weights: [7.69234858e-05 1.50265364e-03 9.98420423e-01], train_wt_loss:  41.5736, val_wt_loss: 38.8919, train_grp_loss: [10.45501767 18.24838293 10.78498692], val_grp_loss: [ 9.56959621 18.20237781 11.15127077], train_hist_grp_loss: [ 5.79880878  8.77098546 15.26992728], cur_train_grp_loss: [0.08432778 0.14142331 0.22946824], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8482, max_kl_dist_index: 2, max_train_grp_loss:  18.2484, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2024, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:30,119 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  65, train_loss:  13.8591, val_loss:  12.9655, grad_norm: 0.4390, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7531, 0.4879, 0.8484, param: [ 2.98764539 10.5031493   4.17193853 10.3114653 ], weights: [6.65394553e-05 1.37624966e-03 9.98557211e-01], train_wt_loss:  41.5774, val_wt_loss: 38.8966, train_grp_loss: [10.45352463 18.25277155 10.78496983], val_grp_loss: [ 9.56859235 18.20781706 11.15155776], train_hist_grp_loss: [ 5.88312344  8.9124458  15.49939508], cur_train_grp_loss: [0.08431466 0.14146033 0.22946781], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8484, max_kl_dist_index: 2, max_train_grp_loss:  18.2528, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2078, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:31,111 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  66, train_loss:  13.8603, val_loss:  12.9670, grad_norm: 0.4398, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7531, 0.4881, 0.8485, param: [ 2.98585546 10.50598916  4.17031986 10.31048563], weights: [5.75558141e-05 1.26050694e-03 9.98681937e-01], train_wt_loss:  41.5809, val_wt_loss: 38.9009, train_grp_loss: [10.45215476 18.25680368 10.78495545], val_grp_loss: [ 9.56767209 18.21281253 11.15182233], train_hist_grp_loss: [ 5.96742606  9.05394015 15.72886253], cur_train_grp_loss: [0.08430262 0.14149435 0.22946744], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8485, max_kl_dist_index: 2, max_train_grp_loss:  18.2568, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2128, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:32,121 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  67, train_loss:  13.8614, val_loss:  12.9683, grad_norm: 0.4406, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7532, 0.4883, 0.8487, param: [ 2.98421187 10.50860543  4.16883332 10.30957404], weights: [4.97839922e-05 1.15452193e-03 9.98795694e-01], train_wt_loss:  41.5841, val_wt_loss: 38.9049, train_grp_loss: [10.45089801 18.26050752 10.78494334], val_grp_loss: [ 9.56682848 18.21739965 11.15206611], train_hist_grp_loss: [ 6.05171763  9.19546576 15.95832966], cur_train_grp_loss: [0.08429157 0.14152561 0.22946714], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8487, max_kl_dist_index: 2, max_train_grp_loss:  18.2605, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2174, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:33,133 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  68, train_loss:  13.8623, val_loss:  12.9695, grad_norm: 0.4412, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7533, 0.4885, 0.8488, param: [ 2.98270289 10.51101478  4.16746834 10.30872688], weights: [4.30607515e-05 1.05746833e-03 9.98899471e-01], train_wt_loss:  41.5870, val_wt_loss: 38.9085, train_grp_loss: [10.44974512 18.26390918 10.78493314], val_grp_loss: [ 9.56605514 18.22161113 11.15229063], train_hist_grp_loss: [ 6.13599906  9.33702008 16.18779654], cur_train_grp_loss: [0.08428144 0.14155432 0.22946688], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8488, max_kl_dist_index: 2, max_train_grp_loss:  18.2639, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2216, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:34,154 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  69, train_loss:  13.8633, val_loss:  12.9706, grad_norm: 0.4418, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7533, 0.4886, 0.8489, param: [ 2.98131771 10.51323276  4.16621518 10.30794049], weights: [3.72447954e-05 9.68590367e-04 9.98994165e-01], train_wt_loss:  41.5898, val_wt_loss: 38.9119, train_grp_loss: [10.4486876  18.26703277 10.78492456], val_grp_loss: [ 9.56534624 18.22547716 11.15249731], train_hist_grp_loss: [ 6.2202712   9.47860077 16.41726321], cur_train_grp_loss: [0.08427214 0.14158069 0.22946666], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8489, max_kl_dist_index: 2, max_train_grp_loss:  18.2670, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2255, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:35,144 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  70, train_loss:  13.8641, val_loss:  12.9716, grad_norm: 0.4424, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0080, KL_dist: 0.7534, 0.4888, 0.8490, param: [ 2.98004635 10.51527389  4.16506484 10.30721127], weights: [3.22138296e-05 8.87196702e-04 9.99080589e-01], train_wt_loss:  41.5923, val_wt_loss: 38.9149, train_grp_loss: [10.44771762 18.26990057 10.78491734], val_grp_loss: [ 9.56469642 18.22902561 11.1526875 ], train_hist_grp_loss: [ 6.30453481  9.62020568 16.64672969], cur_train_grp_loss: [0.08426361 0.14160491 0.22946648], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8490, max_kl_dist_index: 2, max_train_grp_loss:  18.2699, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2290, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:36,140 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  71, train_loss:  13.8648, val_loss:  12.9726, grad_norm: 0.4429, live_grad: 0.0000, reward_err: 0.0001, 0.0632, 0.0083, KL_dist: 0.7534, 0.4889, 0.8491, param: [ 2.9788796  10.51715168  4.16400902 10.30653572], weights: [2.78620145e-05 8.12654859e-04 9.99159483e-01], train_wt_loss:  41.5945, val_wt_loss: 38.9178, train_grp_loss: [10.44682798 18.27253313 10.78491126], val_grp_loss: [ 9.56410076 18.23228214 11.15286246], train_hist_grp_loss: [ 6.3887906   9.76183281 16.87619601], cur_train_grp_loss: [0.08425579 0.14162714 0.22946633], max_reward_err:  0.0632, max_reward_err_index: 1, max_kl_dist:  0.8491, max_kl_dist_index: 2, max_train_grp_loss:  18.2725, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2323, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:37,141 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  72, train_loss:  13.8655, val_loss:  12.9734, grad_norm: 0.4434, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0083, KL_dist: 0.7535, 0.4891, 0.8492, param: [ 2.97780898 10.51887872  4.16304008 10.30591042], weights: [2.40977574e-05 7.44386157e-04 9.99231516e-01], train_wt_loss:  41.5966, val_wt_loss: 38.9203, train_grp_loss: [10.44601209 18.27494941 10.78490615], val_grp_loss: [ 9.56355477 18.23527043 11.15302334], train_hist_grp_loss: [ 6.47303921  9.90348036 17.10566221], cur_train_grp_loss: [0.08424861 0.14164754 0.2294662 ], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8492, max_kl_dist_index: 2, max_train_grp_loss:  18.2749, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2353, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:38,157 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  73, train_loss:  13.8662, val_loss:  12.9742, grad_norm: 0.4438, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0083, KL_dist: 0.7535, 0.4892, 0.8493, param: [ 2.97682667 10.52046667  4.16215097 10.30533211], weights: [2.08417994e-05 6.81861082e-04 9.99297297e-01], train_wt_loss:  41.5986, val_wt_loss: 38.9227, train_grp_loss: [10.44526388 18.2771669  10.78490185], val_grp_loss: [ 9.5630543  18.23801226 11.15317123], train_hist_grp_loss: [ 6.55728124 10.04514663 17.3351283 ], cur_train_grp_loss: [0.08424203 0.14166627 0.22946609], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8493, max_kl_dist_index: 2, max_train_grp_loss:  18.2772, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2380, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:39,197 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  74, train_loss:  13.8668, val_loss:  12.9750, grad_norm: 0.4442, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0083, KL_dist: 0.7535, 0.4893, 0.8494, param: [ 2.9759255  10.52192637  4.16133519 10.30479763], weights: [1.80255587e-05 6.24595059e-04 9.99357379e-01], train_wt_loss:  41.6003, val_wt_loss: 38.9249, train_grp_loss: [10.44457776 18.27920172 10.78489823], val_grp_loss: [ 9.56259558 18.2405277  11.15330716], train_hist_grp_loss: [ 6.64151724 10.1868301  17.56459429], cur_train_grp_loss: [0.084236   0.14168346 0.229466  ], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8494, max_kl_dist_index: 2, max_train_grp_loss:  18.2792, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2405, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:40,216 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  75, train_loss:  13.8673, val_loss:  12.9756, grad_norm: 0.4446, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0083, KL_dist: 0.7536, 0.4894, 0.8495, param: [ 2.97509882 10.52326788  4.16058677 10.304304  ], weights: [1.55896954e-05 5.72144609e-04 9.99412266e-01], train_wt_loss:  41.6020, val_wt_loss: 38.9269, train_grp_loss: [10.44394862 18.2810687  10.78489519], val_grp_loss: [ 9.56217512 18.24283524 11.15343204], train_hist_grp_loss: [ 6.72574771 10.32852934 17.79406021], cur_train_grp_loss: [0.08423047 0.14169924 0.22946592], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8495, max_kl_dist_index: 2, max_train_grp_loss:  18.2811, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2428, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:41,204 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  76, train_loss:  13.8678, val_loss:  12.9762, grad_norm: 0.4450, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7536, 0.4895, 0.8495, param: [ 2.97434056 10.5245005   4.15990022 10.30384837], weights: [1.34828682e-05 5.24103825e-04 9.99462413e-01], train_wt_loss:  41.6035, val_wt_loss: 38.9287, train_grp_loss: [10.44337175 18.28278152 10.78489264], val_grp_loss: [ 9.56178974 18.24495186 11.15354676], train_hist_grp_loss: [ 6.8099731  10.47024305 18.02352607], cur_train_grp_loss: [0.08422539 0.14171371 0.22946586], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8495, max_kl_dist_index: 2, max_train_grp_loss:  18.2828, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2450, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:42,208 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  77, train_loss:  13.8683, val_loss:  12.9768, grad_norm: 0.4453, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7536, 0.4896, 0.8496, param: [ 2.9736451  10.52563284  4.15927047 10.30342805], weights: [1.16606584e-05 4.80101164e-04 9.99508238e-01], train_wt_loss:  41.6048, val_wt_loss: 38.9304, train_grp_loss: [10.44284283 18.28435275 10.78489049], val_grp_loss: [ 9.56143652 18.24689321 11.15365211], train_hist_grp_loss: [ 6.89419384 10.61197003 18.25299187], cur_train_grp_loss: [0.08422074 0.14172699 0.2294658 ], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8496, max_kl_dist_index: 2, max_train_grp_loss:  18.2844, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2469, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:43,209 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  78, train_loss:  13.8687, val_loss:  12.9773, grad_norm: 0.4455, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7536, 0.4896, 0.8497, param: [ 2.97300729 10.52667286  4.15869287 10.3030405 ], weights: [1.00846379e-05 4.39796515e-04 9.99550119e-01], train_wt_loss:  41.6061, val_wt_loss: 38.9320, train_grp_loss: [10.4423579  18.28579397 10.78488868], val_grp_loss: [ 9.56111278 18.24867366 11.15374884], train_hist_grp_loss: [ 6.97841032 10.7537092  18.48245762], cur_train_grp_loss: [0.08421647 0.14173917 0.22946576], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8497, max_kl_dist_index: 2, max_train_grp_loss:  18.2858, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2487, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:44,229 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  79, train_loss:  13.8691, val_loss:  12.9778, grad_norm: 0.4458, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7537, 0.4897, 0.8497, param: [ 2.9724224  10.52762794  4.15816314 10.30268335], weights: [8.72156253e-06 4.02878520e-04 9.99588400e-01], train_wt_loss:  41.6072, val_wt_loss: 38.9334, train_grp_loss: [10.44191332 18.28711584 10.78488716], val_grp_loss: [ 9.56081606 18.25030643 11.15383765], train_hist_grp_loss: [ 7.06262288 10.89545954 18.71192334], cur_train_grp_loss: [0.08421256 0.14175034 0.22946572], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8497, max_kl_dist_index: 2, max_train_grp_loss:  18.2871, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2503, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:45,225 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  80, train_loss:  13.8694, val_loss:  12.9782, grad_norm: 0.4460, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7537, 0.4898, 0.8498, param: [ 2.97188607 10.52850486  4.15767735 10.30235434], weights: [7.54267365e-06 3.69062125e-04 9.99623395e-01], train_wt_loss:  41.6083, val_wt_loss: 38.9347, train_grp_loss: [10.44150575 18.28832813 10.78488589], val_grp_loss: [ 9.56054412 18.25180368 11.15391916], train_hist_grp_loss: [ 7.14683186 11.03722013 18.94138903], cur_train_grp_loss: [0.08420898 0.14176059 0.22946568], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8498, max_kl_dist_index: 2, max_train_grp_loss:  18.2883, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2518, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:46,235 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  81, train_loss:  13.8698, val_loss:  12.9786, grad_norm: 0.4463, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7537, 0.4898, 0.8498, param: [ 2.97139429 10.5293099   4.15723188 10.30205138], weights: [6.52309369e-06 3.38086346e-04 9.99655391e-01], train_wt_loss:  41.6093, val_wt_loss: 38.9359, train_grp_loss: [10.44113212 18.28943986 10.78488482], val_grp_loss: [ 9.56029489 18.25317656 11.15399397], train_hist_grp_loss: [ 7.23103755 11.17899012 19.17085468], cur_train_grp_loss: [0.08420569 0.14176999 0.22946566], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8498, max_kl_dist_index: 2, max_train_grp_loss:  18.2894, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2532, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:47,237 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  82, train_loss:  13.8701, val_loss:  12.9790, grad_norm: 0.4465, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7537, 0.4899, 0.8498, param: [ 2.9709434  10.53004886  4.15682341 10.30177252], weights: [5.64130299e-06 3.09712225e-04 9.99684646e-01], train_wt_loss:  41.6102, val_wt_loss: 38.9370, train_grp_loss: [10.44078961 18.29045931 10.78488391], val_grp_loss: [ 9.56006647 18.25443534 11.15406261], train_hist_grp_loss: [ 7.31524023 11.32076872 19.40032032], cur_train_grp_loss: [0.08420268 0.1417786  0.22946563], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8498, max_kl_dist_index: 2, max_train_grp_loss:  18.2905, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2544, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:48,255 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  83, train_loss:  13.8703, val_loss:  12.9793, grad_norm: 0.4466, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7537, 0.4899, 0.8499, param: [ 2.97053001 10.53072707  4.15644888 10.30151592], weights: [4.87868701e-06 2.83720958e-04 9.99711400e-01], train_wt_loss:  41.6110, val_wt_loss: 38.9380, train_grp_loss: [10.44047565 18.29139406 10.78488316], val_grp_loss: [ 9.55985713 18.25558944 11.15412559], train_hist_grp_loss: [ 7.39944014 11.46255523 19.62978593], cur_train_grp_loss: [0.08419992 0.14178651 0.22946562], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8499, max_kl_dist_index: 2, max_train_grp_loss:  18.2914, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2556, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:49,433 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  84, train_loss:  13.8706, val_loss:  12.9796, grad_norm: 0.4468, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4900, 0.8499, param: [ 2.97015103 10.53134945  4.1561055  10.30127988], weights: [4.21914453e-06 2.59912189e-04 9.99735869e-01], train_wt_loss:  41.6117, val_wt_loss: 38.9389, train_grp_loss: [10.44018786 18.29225112 10.78488252], val_grp_loss: [ 9.55966529 18.2566475  11.15418337], train_hist_grp_loss: [ 7.48363753 11.60434898 19.85925153], cur_train_grp_loss: [0.08419738 0.14179375 0.2294656 ], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8499, max_kl_dist_index: 2, max_train_grp_loss:  18.2923, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2566, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:50,514 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  85, train_loss:  13.8708, val_loss:  12.9799, grad_norm: 0.4470, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4900, 0.8499, param: [ 2.9698036  10.53192053  4.15579069 10.30106281], weights: [3.64874857e-06 2.38102449e-04 9.99758249e-01], train_wt_loss:  41.6124, val_wt_loss: 38.9397, train_grp_loss: [10.43992406 18.29303689 10.78488199], val_grp_loss: [ 9.55948947 18.25761749 11.15423636], train_hist_grp_loss: [ 7.56783259 11.74614938 20.08871712], cur_train_grp_loss: [0.08419506 0.1418004  0.22946559], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8499, max_kl_dist_index: 2, max_train_grp_loss:  18.2930, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2576, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:51,519 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  86, train_loss:  13.8710, val_loss:  12.9802, grad_norm: 0.4471, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4901, 0.8500, param: [ 2.96948511 10.53244448  4.15550208 10.30086325], weights: [3.15545307e-06 2.18123721e-04 9.99778721e-01], train_wt_loss:  41.6130, val_wt_loss: 38.9405, train_grp_loss: [10.43968227 18.29375728 10.78488154], val_grp_loss: [ 9.55932835 18.25850668 11.15428497], train_hist_grp_loss: [ 7.65202553 11.88795586 20.31818269], cur_train_grp_loss: [0.08419294 0.14180649 0.22946557], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8500, max_kl_dist_index: 2, max_train_grp_loss:  18.2938, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2585, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:52,539 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  87, train_loss:  13.8712, val_loss:  12.9804, grad_norm: 0.4472, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4901, 0.8500, param: [ 2.96919317 10.53292516  4.15523751 10.30067983], weights: [2.72883900e-06 1.99822139e-04 9.99797449e-01], train_wt_loss:  41.6136, val_wt_loss: 38.9412, train_grp_loss: [10.43946066 18.29441768 10.78488116], val_grp_loss: [ 9.55918069 18.25932177 11.15432955], train_hist_grp_loss: [ 7.73621651 12.02976794 20.54764826], cur_train_grp_loss: [0.08419099 0.14181207 0.22946556], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8500, max_kl_dist_index: 2, max_train_grp_loss:  18.2944, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2593, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:53,578 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  88, train_loss:  13.8714, val_loss:  12.9806, grad_norm: 0.4474, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4901, 0.8500, param: [ 2.96892557 10.53336609  4.15499498 10.30051128], weights: [2.35989476e-06 1.83056791e-04 9.99814583e-01], train_wt_loss:  41.6142, val_wt_loss: 38.9419, train_grp_loss: [10.43925754 18.29502307 10.78488084], val_grp_loss: [ 9.55904538 18.26006893 11.15437043], train_hist_grp_loss: [ 7.82040571 12.17158513 20.77711381], cur_train_grp_loss: [0.0841892  0.14181719 0.22946556], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8500, max_kl_dist_index: 2, max_train_grp_loss:  18.2950, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2601, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:54,577 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  89, train_loss:  13.8715, val_loss:  12.9808, grad_norm: 0.4475, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4902, 0.8500, param: [ 2.96868029 10.53377052  4.15477267 10.30035643], weights: [2.04082618e-06 1.67698622e-04 9.99830261e-01], train_wt_loss:  41.6146, val_wt_loss: 38.9425, train_grp_loss: [10.43907138 18.29557801 10.78488058], val_grp_loss: [ 9.55892138 18.26075377 11.15440792], train_hist_grp_loss: [ 7.90459327 12.31340701 21.00657936], cur_train_grp_loss: [0.08418756 0.14182188 0.22946555], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8500, max_kl_dist_index: 2, max_train_grp_loss:  18.2956, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2608, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:55,556 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  90, train_loss:  13.8717, val_loss:  12.9810, grad_norm: 0.4476, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4902, 0.8500, param: [ 2.96845548 10.53414146  4.15456889 10.3002142 ], weights: [1.76489214e-06 1.53629436e-04 9.99844606e-01], train_wt_loss:  41.6151, val_wt_loss: 38.9430, train_grp_loss: [10.43890077 18.29608669 10.78488035], val_grp_loss: [ 9.55880775 18.26138148 11.15444229], train_hist_grp_loss: [ 7.98877933 12.4552332  21.23604491], cur_train_grp_loss: [0.08418606 0.14182619 0.22946554], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8500, max_kl_dist_index: 2, max_train_grp_loss:  18.2961, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2614, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:56,561 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  91, train_loss:  13.8718, val_loss:  12.9812, grad_norm: 0.4477, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4902, 0.8501, param: [ 2.96824942 10.53448164  4.15438211 10.30008357], weights: [1.52626237e-06 1.40740977e-04 9.99857733e-01], train_wt_loss:  41.6155, val_wt_loss: 38.9435, train_grp_loss: [10.4387444  18.29655294 10.78488017], val_grp_loss: [ 9.55870362 18.2619568  11.1544738 ], train_hist_grp_loss: [ 8.07296402 12.59706333 21.46551045], cur_train_grp_loss: [0.08418468 0.14183013 0.22946554], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8501, max_kl_dist_index: 2, max_train_grp_loss:  18.2966, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2620, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:57,555 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  92, train_loss:  13.8720, val_loss:  12.9813, grad_norm: 0.4477, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4902, 0.8501, param: [ 2.96806058 10.5347936   4.15421092 10.29996362], weights: [1.31989443e-06 1.28934095e-04 9.99869746e-01], train_wt_loss:  41.6159, val_wt_loss: 38.9440, train_grp_loss: [10.4386011  18.29698029 10.78488001], val_grp_loss: [ 9.5586082  18.2624841  11.15450269], train_hist_grp_loss: [ 8.15714744 12.73889707 21.69497598], cur_train_grp_loss: [0.08418342 0.14183374 0.22946554], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8501, max_kl_dist_index: 2, max_train_grp_loss:  18.2970, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2625, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:58,590 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  93, train_loss:  13.8721, val_loss:  12.9815, grad_norm: 0.4478, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4903, 0.8501, param: [ 2.9678875  10.53507966  4.15405402 10.2998535 ], weights: [1.14142729e-06 1.18117974e-04 9.99880741e-01], train_wt_loss:  41.6162, val_wt_loss: 38.9444, train_grp_loss: [10.43846977 18.29737197 10.78487988], val_grp_loss: [ 9.55852076 18.26296737 11.15452917], train_hist_grp_loss: [ 8.24132971 12.88073413 21.92444151], cur_train_grp_loss: [0.08418227 0.14183706 0.22946553], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8501, max_kl_dist_index: 2, max_train_grp_loss:  18.2974, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2630, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:18:59,583 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  94, train_loss:  13.8722, val_loss:  12.9816, grad_norm: 0.4479, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4903, 0.8501, param: [ 2.96772888 10.53534195  4.15391021 10.29975242], weights: [9.87089294e-07 1.08209432e-04 9.99890803e-01], train_wt_loss:  41.6165, val_wt_loss: 38.9448, train_grp_loss: [10.43834942 18.29773095 10.78487977], val_grp_loss: [ 9.55844064 18.26341028 11.15455344], train_hist_grp_loss: [ 8.32551091 13.02257422 22.15390704], cur_train_grp_loss: [0.08418121 0.14184009 0.22946553], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8501, max_kl_dist_index: 2, max_train_grp_loss:  18.2977, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2634, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:00,560 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  95, train_loss:  13.8723, val_loss:  12.9817, grad_norm: 0.4480, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7538, 0.4903, 0.8501, param: [ 2.96758352 10.53558244  4.15377842 10.29965964], weights: [8.53618547e-07 9.91322773e-05 9.99900014e-01], train_wt_loss:  41.6168, val_wt_loss: 38.9451, train_grp_loss: [10.43823913 18.29805996 10.78487967], val_grp_loss: [ 9.55836722 18.26381618 11.1545757 ], train_hist_grp_loss: [ 8.40969115 13.1644171  22.38337257], cur_train_grp_loss: [0.08418024 0.14184288 0.22946553], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8501, max_kl_dist_index: 2, max_train_grp_loss:  18.2981, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2638, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:01,546 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  96, train_loss:  13.8724, val_loss:  12.9818, grad_norm: 0.4480, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7539, 0.4903, 0.8501, param: [ 2.9674503  10.53580293  4.15365764 10.2995745 ], weights: [7.38194014e-07 9.08167215e-05 9.99908445e-01], train_wt_loss:  41.6171, val_wt_loss: 38.9455, train_grp_loss: [10.43813806 18.29836148 10.78487959], val_grp_loss: [ 9.55829994 18.26418816 11.15459609], train_hist_grp_loss: [ 8.4938705  13.30626252 22.61283809], cur_train_grp_loss: [0.08417935 0.14184543 0.22946552], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8501, max_kl_dist_index: 2, max_train_grp_loss:  18.2984, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2642, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:02,545 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  97, train_loss:  13.8724, val_loss:  12.9819, grad_norm: 0.4481, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7539, 0.4903, 0.8501, param: [ 2.96732822 10.53600507  4.15354694 10.29949637], weights: [6.38375973e-07 8.31988382e-05 9.99916163e-01], train_wt_loss:  41.6173, val_wt_loss: 38.9457, train_grp_loss: [10.43804544 18.2986378  10.78487953], val_grp_loss: [ 9.5582383  18.26452904 11.15461479], train_hist_grp_loss: [ 8.57804903 13.44811028 22.84230362], cur_train_grp_loss: [0.08417853 0.14184776 0.22946552], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8501, max_kl_dist_index: 2, max_train_grp_loss:  18.2986, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2645, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:03,561 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  98, train_loss:  13.8725, val_loss:  12.9820, grad_norm: 0.4481, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7539, 0.4904, 0.8502, param: [ 2.96721635 10.53619037  4.1534455  10.29942469], weights: [5.52054481e-07 7.62200715e-05 9.99923228e-01], train_wt_loss:  41.6175, val_wt_loss: 38.9460, train_grp_loss: [10.43796056 18.29889104 10.78487947], val_grp_loss: [ 9.55818181 18.26484143 11.15463192], train_hist_grp_loss: [ 8.66222682 13.58996019 23.07176914], cur_train_grp_loss: [0.08417779 0.14184991 0.22946552], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8502, max_kl_dist_index: 2, max_train_grp_loss:  18.2989, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2648, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:04,535 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  99, train_loss:  13.8726, val_loss:  12.9821, grad_norm: 0.4482, live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7539, 0.4904, 0.8502, param: [ 2.96711384 10.53636025  4.15335254 10.29935893], weights: [4.77404810e-07 6.98267832e-05 9.99929696e-01], train_wt_loss:  41.6177, val_wt_loss: 38.9463, train_grp_loss: [10.43788279 18.2991231  10.78487943], val_grp_loss: [ 9.55813005 18.2651277  11.15464763], train_hist_grp_loss: [ 8.74640392 13.73181206 23.30123466], cur_train_grp_loss: [0.0841771  0.14185187 0.22946552], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8502, max_kl_dist_index: 2, max_train_grp_loss:  18.2991, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2651, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:05,441 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1654] - INFO: Iteration:  99, train_loss:  13.8726, val_loss:  12.9821, grad_norm: 0.4482,  live_grad: 0.0000, reward_err: 0.0001, 0.0635, 0.0086, KL_dist: 0.7539, 0.4904, 0.8502, param: [ 2.96711384 10.53636025  4.15335254 10.29935893], weights: [4.77404810e-07 6.98267832e-05 9.99929696e-01], train_wt_loss:  41.6177, val_wt_loss: 38.9463, train_grp_loss: [10.43788279 18.2991231  10.78487943], val_grp_loss: [ 9.55813005 18.2651277  11.15464763], train_hist_grp_loss: [ 8.74640392 13.73181206 23.30123466], cur_train_grp_loss: [0.0841771  0.14185187 0.22946552], max_reward_err:  0.0635, max_reward_err_index: 1, max_kl_dist:  0.8502, max_kl_dist_index: 2, max_train_grp_loss:  18.2991, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2651, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2295, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:05,664 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:388] - INFO: Policy parameter learned solely on the preference data rdpo: [ 2.96711384 10.53636025  4.15335254 10.29935893].
2024-10-07 17:19:06,014 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Uniform reward: 3.8269, 3.8269, 3.1987
2024-10-07 17:19:06,015 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Optimal reward: 3.8681, 7.1581, 3.3364
2024-10-07 17:19:06,016 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Policy reward: 3.8678, 6.7036, 3.3079
2024-10-07 17:19:06,016 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:402] - INFO: Reward Error: 0.0001, 0.0635, 0.0086
2024-10-07 17:19:06,689 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:417] - INFO: Optimal reward: 3.8681, 7.1581, 3.3364
Known param reward: [[3.8680634765625, 6.699916015625, 3.30809521484375], [3.46072314453125, 7.1581357421875, 3.12821826171875], [3.83464501953125, 7.0068994140625, 3.336449462890625]], Known param reward error: [[0.0, 0.06401383587376193, 0.00849832984501714], [0.10530859550248342, 0.0, 0.06241101610796404], [0.008639583407495802, 0.02112789329121925, 0.0]].
