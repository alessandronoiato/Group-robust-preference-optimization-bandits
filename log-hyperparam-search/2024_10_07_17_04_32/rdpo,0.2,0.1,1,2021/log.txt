2024-10-07 17:31:52,557 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log-hyperparam-search/2024_10_07_17_04_32/rdpo,0.2,0.1,1,2021
2024-10-07 17:31:52,559 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2021
2024-10-07 17:31:52,559 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-10-07 17:31:52,647 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.2351, l2 distance: 36.4442, acc: 0.90.
2024-10-07 17:31:52,648 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-10-07 17:31:52,649 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [ 9.05939251 17.1373287   9.50347978 12.39752865]
2024-10-07 17:31:52,858 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.8285, 7.1348, 3.2752
2024-10-07 17:31:53,086 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1314] - INFO: unique_group_ids: {0, 1, 2}
2024-10-07 17:31:54,297 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  0, train_loss:  12.1566, val_loss:  12.4246, grad_norm: 0.4595, live_grad: 0.0000, reward_err: 0.0098, 0.0149, 0.0001, KL_dist: 0.6659, 0.3100, 0.6608, param: [5.43158951 8.3444196  5.16696908 8.70178998], weights: [0.31016477 0.30659363 0.38324159], train_wt_loss:  36.4697, val_wt_loss: 37.2738, train_grp_loss: [11.74866721 13.14550239 10.70395786], val_grp_loss: [12.62047609 12.53046358 12.12225306], train_hist_grp_loss: [0.15100158 0.13942111 0.36256349], cur_train_grp_loss: [0.15100158 0.13942111 0.36256349], max_reward_err:  0.0149, max_reward_err_index: 1, max_kl_dist:  0.6659, max_kl_dist_index: 0, max_train_grp_loss:  13.1455, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6205, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.3626, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:31:55,318 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  1, train_loss:  12.1571, val_loss:  12.4284, grad_norm: 0.0044, live_grad: 0.0000, reward_err: 0.0100, 0.0149, 0.0001, KL_dist: 0.6692, 0.3117, 0.6639, param: [5.43721625 8.3261515  5.19704009 8.74951509], weights: [0.29473644 0.29461686 0.41064671], train_wt_loss:  36.4714, val_wt_loss: 37.2852, train_grp_loss: [11.75504324 13.14470465 10.6933317 ], val_grp_loss: [12.63198191 12.52981828 12.1228303 ], train_hist_grp_loss: [0.24499092 0.24458513 0.57664264], cur_train_grp_loss: [0.09398934 0.10516402 0.21407916], max_reward_err:  0.0149, max_reward_err_index: 1, max_kl_dist:  0.6692, max_kl_dist_index: 0, max_train_grp_loss:  13.1447, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6320, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2141, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:31:56,370 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  2, train_loss:  12.1581, val_loss:  12.4327, grad_norm: 0.0068, live_grad: 0.0000, reward_err: 0.0100, 0.0149, 0.0001, KL_dist: 0.6729, 0.3138, 0.6674, param: [5.44199623 8.30916465 5.22839291 8.80350362], weights: [0.27922034 0.28222729 0.43855237], train_wt_loss:  36.4742, val_wt_loss: 37.2982, train_grp_loss: [11.75932104 13.14735959 10.68160485], val_grp_loss: [12.64173758 12.53311046 12.12269047], train_hist_grp_loss: [0.33903126 0.34974276 0.79050928], cur_train_grp_loss: [0.09404035 0.10515764 0.21386663], max_reward_err:  0.0149, max_reward_err_index: 1, max_kl_dist:  0.6729, max_kl_dist_index: 0, max_train_grp_loss:  13.1474, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6417, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2139, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:31:57,426 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  3, train_loss:  12.1594, val_loss:  12.4376, grad_norm: 0.0101, live_grad: 0.0000, reward_err: 0.0100, 0.0143, 0.0001, KL_dist: 0.6771, 0.3160, 0.6713, param: [5.44587471 8.29331377 5.26109411 8.86391878], weights: [0.26370233 0.26951842 0.46677926], train_wt_loss:  36.4783, val_wt_loss: 37.3128, train_grp_loss: [11.76158873 13.15353508 10.66880705], val_grp_loss: [12.64981159 12.54042117 12.12190364], train_hist_grp_loss: [0.43310583 0.45492164 1.00414138], cur_train_grp_loss: [0.09407457 0.10517888 0.2136321 ], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.6771, max_kl_dist_index: 0, max_train_grp_loss:  13.1535, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6498, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2136, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:31:58,553 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  4, train_loss:  12.1614, val_loss:  12.4431, grad_norm: 0.0140, live_grad: 0.0000, reward_err: 0.0102, 0.0143, 0.0001, KL_dist: 0.6816, 0.3185, 0.6758, param: [5.44879451 8.27843064 5.29519229 8.93087991], weights: [0.24827129 0.25658846 0.49514025], train_wt_loss:  36.4841, val_wt_loss: 37.3293, train_grp_loss: [11.76196313 13.16328585 10.65498831], val_grp_loss: [12.65629178 12.55181829 12.12055766], train_hist_grp_loss: [0.52719854 0.56014992 1.21751752], cur_train_grp_loss: [0.09409271 0.10522828 0.21337614], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.6816, max_kl_dist_index: 0, max_train_grp_loss:  13.1633, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6563, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2134, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:31:59,627 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  5, train_loss:  12.1639, val_loss:  12.4493, grad_norm: 0.0187, live_grad: 0.0000, reward_err: 0.0100, 0.0143, 0.0001, KL_dist: 0.6866, 0.3212, 0.6808, param: [5.45069795 8.26432365 5.33071462 9.0044548 ], weights: [0.23301667 0.24353775 0.52344557], train_wt_loss:  36.4917, val_wt_loss: 37.3480, train_grp_loss: [11.76059044 13.17664898 10.64021957], val_grp_loss: [12.66128554 12.56735182 12.11875771], train_hist_grp_loss: [0.62129424 0.66545621 1.43061728], cur_train_grp_loss: [0.09409571 0.10530629 0.21309977], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.6866, max_kl_dist_index: 0, max_train_grp_loss:  13.1766, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6613, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2131, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:00,729 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  6, train_loss:  12.1671, val_loss:  12.4564, grad_norm: 0.0240, live_grad: 0.0000, reward_err: 0.0100, 0.0143, 0.0001, KL_dist: 0.6920, 0.3242, 0.6863, param: [5.45152933 8.25077819 5.3676635  9.0846527 ], weights: [0.21802585 0.23046616 0.55150799], train_wt_loss:  36.5014, val_wt_loss: 37.3692, train_grp_loss: [11.75764565 13.19363948 10.62459254], val_grp_loss: [12.66491909 12.587049   12.1166249 ], train_hist_grp_loss: [0.71537897 0.7708694  1.64342167], cur_train_grp_loss: [0.09408472 0.10541319 0.21280439], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.6920, max_kl_dist_index: 0, max_train_grp_loss:  13.1936, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6649, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2128, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:01,734 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  7, train_loss:  12.1712, val_loss:  12.4644, grad_norm: 0.0300, live_grad: 0.0000, reward_err: 0.0098, 0.0143, 0.0001, KL_dist: 0.6978, 0.3273, 0.6923, param: [5.45123767 8.23755827 5.40601359 9.17141843], weights: [0.20338169 0.21747042 0.57914789], train_wt_loss:  36.5136, val_wt_loss: 37.3931, train_grp_loss: [11.75333051 13.21424594 10.6082187 ], val_grp_loss: [12.66733581 12.61090967 12.11429395], train_hist_grp_loss: [0.80944013 0.87641852 1.85591353], cur_train_grp_loss: [0.09406117 0.10554912 0.21249185], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.6978, max_kl_dist_index: 0, max_train_grp_loss:  13.2142, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6673, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2125, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:02,732 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  8, train_loss:  12.1762, val_loss:  12.4734, grad_norm: 0.0367, live_grad: 0.0000, reward_err: 0.0097, 0.0143, 0.0001, KL_dist: 0.7039, 0.3307, 0.6987, param: [5.44977975 8.22440932 5.44570951 9.26462788], weights: [0.18916039 0.20464177 0.60619784], train_wt_loss:  36.5285, val_wt_loss: 37.4201, train_grp_loss: [11.74787011 13.23842672 10.59122735], val_grp_loss: [12.66869352 12.638902   12.11190992], train_hist_grp_loss: [0.90346678 0.98213248 2.0680779 ], cur_train_grp_loss: [0.09402664 0.10571397 0.21216437], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.7039, max_kl_dist_index: 0, max_train_grp_loss:  13.2384, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6687, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  0.2122, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:03,732 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  9, train_loss:  12.1821, val_loss:  12.4834, grad_norm: 0.0441, live_grad: 0.0000, reward_err: 0.0097, 0.0143, 0.0001, KL_dist: 0.7105, 0.3343, 0.7055, param: [5.44712305 8.21106231 5.48666425 9.36408529], weights: [0.1754296  0.19206387 0.63250653], train_wt_loss:  36.5464, val_wt_loss: 37.4503, train_grp_loss: [11.74150794 13.26610671 10.57376285], val_grp_loss: [12.66916089 12.67095893 12.10962396], train_hist_grp_loss: [0.99744974 1.0880399  2.27990245], cur_train_grp_loss: [0.09398296 0.10590741 0.21182455], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.7105, max_kl_dist_index: 0, max_train_grp_loss:  13.2661, max_train_grp_loss_index: 1, max_val_grp_loss:  12.6710, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2118, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:04,754 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  10, train_loss:  12.1892, val_loss:  12.4947, grad_norm: 0.0521, live_grad: 0.0000, reward_err: 0.0093, 0.0143, 0.0001, KL_dist: 0.7174, 0.3381, 0.7128, param: [5.44324873 8.19723916 5.52875876 9.46952261], weights: [0.16224709 0.1798112  0.65794171], train_wt_loss:  36.5676, val_wt_loss: 37.4840, train_grp_loss: [11.73449962 13.29717512 10.5559809 ], val_grp_loss: [12.66891287 12.70697556 12.10758845], train_hist_grp_loss: [1.0913818  1.19416875 2.4913777 ], cur_train_grp_loss: [0.09393206 0.10612885 0.21147526], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.7174, max_kl_dist_index: 0, max_train_grp_loss:  13.2972, max_train_grp_loss_index: 1, max_val_grp_loss:  12.7070, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2115, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:05,746 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  11, train_loss:  12.1974, val_loss:  12.5071, grad_norm: 0.0608, live_grad: 0.0000, reward_err: 0.0093, 0.0143, 0.0002, KL_dist: 0.7246, 0.3421, 0.7204, param: [5.43815407 8.18265928 5.5718426  9.58060084], weights: [0.14965987 0.16794785 0.68239228], train_wt_loss:  36.5923, val_wt_loss: 37.5214, train_grp_loss: [11.72710564 13.3314844  10.53804425], val_grp_loss: [12.66812573 12.7468076  12.10595161], train_hist_grp_loss: [1.1852578  1.30054615 2.70249732], cur_train_grp_loss: [0.093876   0.1063774  0.21111962], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.7246, max_kl_dist_index: 0, max_train_grp_loss:  13.3315, max_train_grp_loss_index: 1, max_val_grp_loss:  12.7468, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2111, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:06,728 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  12, train_loss:  12.2069, val_loss:  12.5208, grad_norm: 0.0701, live_grad: 0.0000, reward_err: 0.0093, 0.0143, 0.0002, KL_dist: 0.7320, 0.3463, 0.7283, param: [5.43185445 8.16704699 5.61573603 9.69691359], weights: [0.13770377 0.15652695 0.70576928], train_wt_loss:  36.6206, val_wt_loss: 37.5625, train_grp_loss: [11.71958346 13.36885044 10.52011794], val_grp_loss: [12.66697165 12.79027123 12.10485193], train_hist_grp_loss: [1.27907464 1.40719803 2.91325821], cur_train_grp_loss: [0.09381685 0.10665188 0.21076089], max_reward_err:  0.0143, max_reward_err_index: 1, max_kl_dist:  0.7320, max_kl_dist_index: 0, max_train_grp_loss:  13.3689, max_train_grp_loss_index: 1, max_val_grp_loss:  12.7903, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2108, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:07,722 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  13, train_loss:  12.2176, val_loss:  12.5359, grad_norm: 0.0799, live_grad: 0.0000, reward_err: 0.0093, 0.0146, 0.0002, KL_dist: 0.7397, 0.3505, 0.7366, param: [5.42438446 8.15013959 5.66023331 9.81799259], weights: [0.1264036  0.14559044 0.72800596], train_wt_loss:  36.6527, val_wt_loss: 37.6076, train_grp_loss: [11.71217944 13.4090542  10.50236438], val_grp_loss: [12.66561345 12.83714424 12.10441289], train_hist_grp_loss: [1.37283131 1.51414883 3.12366057], cur_train_grp_loss: [0.09375667 0.1069508  0.21040236], max_reward_err:  0.0146, max_reward_err_index: 1, max_kl_dist:  0.7397, max_kl_dist_index: 0, max_train_grp_loss:  13.4091, max_train_grp_loss_index: 1, max_val_grp_loss:  12.8371, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2104, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:08,723 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  14, train_loss:  12.2296, val_loss:  12.5521, grad_norm: 0.0902, live_grad: 0.0000, reward_err: 0.0093, 0.0153, 0.0003, KL_dist: 0.7476, 0.3549, 0.7450, param: [5.41579824 8.13169555 5.70510735 9.94331492], weights: [0.11577355 0.13516937 0.74905708], train_wt_loss:  36.6887, val_wt_loss: 37.6564, train_grp_loss: [11.70512115 13.45184464 10.48493855], val_grp_loss: [12.6641997  12.88716863 12.10473819], train_hist_grp_loss: [1.46652875 1.62142126 3.33370785], cur_train_grp_loss: [0.09369744 0.10727243 0.21004729], max_reward_err:  0.0153, max_reward_err_index: 1, max_kl_dist:  0.7476, max_kl_dist_index: 0, max_train_grp_loss:  13.4518, max_train_grp_loss_index: 1, max_val_grp_loss:  12.8872, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2100, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:09,716 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  15, train_loss:  12.2428, val_loss:  12.5697, grad_norm: 0.1009, live_grad: 0.0000, reward_err: 0.0091, 0.0156, 0.0003, KL_dist: 0.7557, 0.3595, 0.7537, param: [ 5.40616874  8.11150256  5.75011523 10.07231171], weights: [0.10581801 0.12528449 0.7688975 ], train_wt_loss:  36.7284, val_wt_loss: 37.7091, train_grp_loss: [11.69861057 13.49694296 10.46798365], val_grp_loss: [12.66286062 12.9400544  12.10590794], train_hist_grp_loss: [1.56016972 1.72903602 3.54340662], cur_train_grp_loss: [0.09364097 0.10761476 0.20969877], max_reward_err:  0.0156, max_reward_err_index: 1, max_kl_dist:  0.7557, max_kl_dist_index: 0, max_train_grp_loss:  13.4969, max_train_grp_loss_index: 1, max_val_grp_loss:  12.9401, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2097, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:10,718 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  16, train_loss:  12.2573, val_loss:  12.5885, grad_norm: 0.1119, live_grad: 0.0000, reward_err: 0.0087, 0.0156, 0.0003, KL_dist: 0.7638, 0.3641, 0.7625, param: [ 5.39558618  8.08938483  5.79500469 10.20437782], weights: [0.09653265 0.11594713 0.78752022], train_wt_loss:  36.7719, val_wt_loss: 37.7654, train_grp_loss: [11.69281858 13.54404786 10.45162738], val_grp_loss: [12.66170498 12.99548449 12.10797602], train_hist_grp_loss: [1.6537586  1.83701157 3.7527663 ], cur_train_grp_loss: [0.09358888 0.10797554 0.20935967], max_reward_err:  0.0156, max_reward_err_index: 1, max_kl_dist:  0.7638, max_kl_dist_index: 0, max_train_grp_loss:  13.5440, max_train_grp_loss_index: 1, max_val_grp_loss:  12.9955, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2094, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:11,701 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  17, train_loss:  12.2730, val_loss:  12.6084, grad_norm: 0.1231, live_grad: 0.0000, reward_err: 0.0087, 0.0159, 0.0003, KL_dist: 0.7720, 0.3687, 0.7713, param: [ 5.38415563  8.06520936  5.83952085 10.33888216], weights: [0.08790554 0.10716028 0.80493418], train_wt_loss:  36.8189, val_wt_loss: 37.8251, train_grp_loss: [11.68788104 13.59284155 10.43597903], val_grp_loss: [12.66081813 13.05312049 12.1109688 ], train_hist_grp_loss: [1.74730115 1.94536395 3.96179884], cur_train_grp_loss: [0.09354255 0.10835238 0.20903255], max_reward_err:  0.0159, max_reward_err_index: 1, max_kl_dist:  0.7720, max_kl_dist_index: 0, max_train_grp_loss:  13.5928, max_train_grp_loss_index: 1, max_val_grp_loss:  13.0531, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2090, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:12,712 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  18, train_loss:  12.2897, val_loss:  12.6293, grad_norm: 0.1345, live_grad: 0.0000, reward_err: 0.0085, 0.0159, 0.0004, KL_dist: 0.7802, 0.3734, 0.7802, param: [ 5.3719939   8.03889072  5.88341314 10.47517817], weights: [0.0799184  0.09891971 0.82116189], train_wt_loss:  36.8692, val_wt_loss: 37.8878, train_grp_loss: [11.68389677 13.64299623 10.42112768], val_grp_loss: [12.6602614  13.1126089  12.11488525], train_hist_grp_loss: [1.8408042  2.05410668 4.17051842], cur_train_grp_loss: [0.09350305 0.10874273 0.20871958], max_reward_err:  0.0159, max_reward_err_index: 1, max_kl_dist:  0.7802, max_kl_dist_index: 0, max_train_grp_loss:  13.6430, max_train_grp_loss_index: 1, max_val_grp_loss:  13.1126, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2087, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:13,748 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  19, train_loss:  12.3075, val_loss:  12.6511, grad_norm: 0.1459, live_grad: 0.0000, reward_err: 0.0088, 0.0159, 0.0006, KL_dist: 0.7884, 0.3781, 0.7891, param: [ 5.35922612  8.0103942   5.92644181 10.61261425], weights: [0.07254785 0.09121521 0.83623694], train_wt_loss:  36.9225, val_wt_loss: 37.9534, train_grp_loss: [11.6809273  13.69418068 10.40714123], val_grp_loss: [12.66007264 13.17358764 12.11969835], train_hist_grp_loss: [1.93427537 2.16325065 4.37894098], cur_train_grp_loss: [0.09347117 0.10914397 0.20842255], max_reward_err:  0.0159, max_reward_err_index: 1, max_kl_dist:  0.7891, max_kl_dist_index: 2, max_train_grp_loss:  13.6942, max_train_grp_loss_index: 1, max_val_grp_loss:  13.1736, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2084, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:14,754 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  20, train_loss:  12.3261, val_loss:  12.6738, grad_norm: 0.1573, live_grad: 0.0000, reward_err: 0.0088, 0.0163, 0.0006, KL_dist: 0.7965, 0.3828, 0.7978, param: [ 5.34598198  7.97973701  5.96838385 10.75054359], weights: [0.06576663 0.08403168 0.85020169], train_wt_loss:  36.9784, val_wt_loss: 38.0213, train_grp_loss: [11.67899826 13.74606662 10.39406651], val_grp_loss: [12.66026783 13.23569243 12.12535769], train_hist_grp_loss: [2.02772279 2.2728041  4.5870838 ], cur_train_grp_loss: [0.09344742 0.10955345 0.20814282], max_reward_err:  0.0163, max_reward_err_index: 1, max_kl_dist:  0.7978, max_kl_dist_index: 2, max_train_grp_loss:  13.7461, max_train_grp_loss_index: 1, max_val_grp_loss:  13.2357, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2081, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:15,794 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  21, train_loss:  12.3455, val_loss:  12.6970, grad_norm: 0.1686, live_grad: 0.0000, reward_err: 0.0088, 0.0167, 0.0006, KL_dist: 0.8044, 0.3875, 0.8065, param: [ 5.33239209  7.94698766  6.00903794 10.88833338], weights: [0.05954463 0.07735026 0.86310511], train_wt_loss:  37.0365, val_wt_loss: 38.0911, train_grp_loss: [11.67810239 13.79833458 10.38193018], val_grp_loss: [12.66084359 13.29856279 12.13179293], train_hist_grp_loss: [2.12115478 2.38277263 4.79496513], cur_train_grp_loss: [0.09343199 0.10996853 0.20788133], max_reward_err:  0.0167, max_reward_err_index: 1, max_kl_dist:  0.8065, max_kl_dist_index: 2, max_train_grp_loss:  13.7983, max_train_grp_loss_index: 1, max_val_grp_loss:  13.2986, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2079, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:16,792 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  22, train_loss:  12.3655, val_loss:  12.7208, grad_norm: 0.1797, live_grad: 0.0000, reward_err: 0.0086, 0.0167, 0.0005, KL_dist: 0.8122, 0.3921, 0.8150, param: [ 5.31858456  7.91226334  6.04822832 11.0253731 ], weights: [0.05384991 0.07114929 0.8750008 ], train_wt_loss:  37.0965, val_wt_loss: 38.1624, train_grp_loss: [11.67820349 13.85067904 10.37074032], val_grp_loss: [12.66178027 13.3618476  12.13891793], train_hist_grp_loss: [2.2145796  2.49315931 5.00260374], cur_train_grp_loss: [0.09342482 0.11038668 0.2076386 ], max_reward_err:  0.0167, max_reward_err_index: 1, max_kl_dist:  0.8150, max_kl_dist_index: 2, max_train_grp_loss:  13.8507, max_train_grp_loss_index: 1, max_val_grp_loss:  13.3618, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2076, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:17,785 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  23, train_loss:  12.3859, val_loss:  12.7449, grad_norm: 0.1906, live_grad: 0.0000, reward_err: 0.0087, 0.0167, 0.0007, KL_dist: 0.8198, 0.3967, 0.8233, param: [ 5.30468196  7.87572581  6.08580743 11.16108181], weights: [0.04864952 0.06540518 0.88594531], train_wt_loss:  37.1578, val_wt_loss: 38.2346, train_grp_loss: [11.67924129 13.90281275 10.36048856], val_grp_loss: [12.66304538 13.42520977 12.14663505], train_hist_grp_loss: [2.30800522 2.60396474 5.21001854], cur_train_grp_loss: [0.09342563 0.11080543 0.20741481], max_reward_err:  0.0167, max_reward_err_index: 1, max_kl_dist:  0.8233, max_kl_dist_index: 2, max_train_grp_loss:  13.9028, max_train_grp_loss_index: 1, max_val_grp_loss:  13.4252, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2074, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:18,770 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  24, train_loss:  12.4067, val_loss:  12.7692, grad_norm: 0.2011, live_grad: 0.0000, reward_err: 0.0087, 0.0171, 0.0007, KL_dist: 0.8272, 0.4012, 0.8314, param: [ 5.29079882  7.83757567  6.12165728 11.2949144 ], weights: [0.04391024 0.06009313 0.89599663], train_wt_loss:  37.2201, val_wt_loss: 38.3075, train_grp_loss: [11.68113658 13.95447002 10.35115249], val_grp_loss: [12.66459706 13.48833028 12.15483957], train_hist_grp_loss: [2.40143915 2.71518724 5.41722831], cur_train_grp_loss: [0.09343393 0.1112225  0.20720977], max_reward_err:  0.0171, max_reward_err_index: 1, max_kl_dist:  0.8314, max_kl_dist_index: 2, max_train_grp_loss:  13.9545, max_train_grp_loss_index: 1, max_val_grp_loss:  13.4883, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2072, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:19,754 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  25, train_loss:  12.4276, val_loss:  12.7934, grad_norm: 0.2114, live_grad: 0.0000, reward_err: 0.0091, 0.0174, 0.0009, KL_dist: 0.8343, 0.4056, 0.8392, param: [ 5.2770397   7.79804566  6.15568977 11.42636669], weights: [0.03959913 0.0551878  0.90521306], train_wt_loss:  37.2829, val_wt_loss: 38.3803, train_grp_loss: [11.68379645 14.00540916 10.34269825], val_grp_loss: [12.66638746 13.55091134 12.16342381], train_hist_grp_loss: [2.49488825 2.826823   5.62425136], cur_train_grp_loss: [0.09344909 0.11163576 0.20702305], max_reward_err:  0.0174, max_reward_err_index: 1, max_kl_dist:  0.8392, max_kl_dist_index: 2, max_train_grp_loss:  14.0054, max_train_grp_loss_index: 1, max_val_grp_loss:  13.5509, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2070, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:20,742 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  26, train_loss:  12.4486, val_loss:  12.8176, grad_norm: 0.2212, live_grad: 0.0000, reward_err: 0.0094, 0.0177, 0.0011, KL_dist: 0.8412, 0.4099, 0.8467, param: [ 5.26349786  7.75739314  6.18784589 11.5549796 ], weights: [0.03568403 0.05066376 0.91365221], train_wt_loss:  37.3457, val_wt_loss: 38.4529, train_grp_loss: [11.68711929 14.05541394 10.33508304], val_grp_loss: [12.66836574 13.61267869 12.17228079], train_hist_grp_loss: [2.58835862 2.93886627 5.83110533], cur_train_grp_loss: [0.09347037 0.11204327 0.20685396], max_reward_err:  0.0177, max_reward_err_index: 1, max_kl_dist:  0.8467, max_kl_dist_index: 2, max_train_grp_loss:  14.0554, max_train_grp_loss_index: 1, max_val_grp_loss:  13.6127, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2069, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:21,732 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  27, train_loss:  12.4694, val_loss:  12.8416, grad_norm: 0.2306, live_grad: 0.0000, reward_err: 0.0096, 0.0179, 0.0012, KL_dist: 0.8478, 0.4141, 0.8539, param: [ 5.25025446  7.71589219  6.21809409 11.68034208], weights: [0.03213387 0.04649592 0.92137022], train_wt_loss:  37.4082, val_wt_loss: 38.5247, train_grp_loss: [11.69099929 14.1042943  10.32825761], val_grp_loss: [12.67048069 13.67338336 12.18130735], train_hist_grp_loss: [2.68185557 3.05130958 6.03780699], cur_train_grp_loss: [0.09349695 0.11244331 0.20670166], max_reward_err:  0.0179, max_reward_err_index: 1, max_kl_dist:  0.8539, max_kl_dist_index: 2, max_train_grp_loss:  14.1043, max_train_grp_loss_index: 1, max_val_grp_loss:  13.6734, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2067, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:22,736 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  28, train_loss:  12.4900, val_loss:  12.8651, grad_norm: 0.2395, live_grad: 0.0000, reward_err: 0.0096, 0.0179, 0.0012, KL_dist: 0.8541, 0.4181, 0.8609, param: [ 5.23737831  7.67382574  6.24642804 11.80209321], weights: [0.02891895 0.04265981 0.92842124], train_wt_loss:  37.4701, val_wt_loss: 38.5953, train_grp_loss: [11.69533036 14.15188631 10.32216843], val_grp_loss: [12.67268283 13.73280254 12.19040665], train_hist_grp_loss: [2.77538357 3.16414394 6.24437214], cur_train_grp_loss: [0.09352799 0.11283435 0.20656515], max_reward_err:  0.0179, max_reward_err_index: 1, max_kl_dist:  0.8609, max_kl_dist_index: 2, max_train_grp_loss:  14.1519, max_train_grp_loss_index: 1, max_val_grp_loss:  13.7328, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2066, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:23,722 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  29, train_loss:  12.5103, val_loss:  12.8882, grad_norm: 0.2480, live_grad: 0.0000, reward_err: 0.0096, 0.0188, 0.0012, KL_dist: 0.8601, 0.4220, 0.8675, param: [ 5.22492597  7.63147797  6.27286381 11.91992324], weights: [0.02601112 0.03913187 0.93485701], train_wt_loss:  37.5310, val_wt_loss: 38.6645, train_grp_loss: [11.70000944 14.19805167 10.31675961], val_grp_loss: [12.67492606 13.79074014 12.19949004], train_hist_grp_loss: [2.86894621 3.27735903 6.45081551], cur_train_grp_loss: [0.09356264 0.11321509 0.20644337], max_reward_err:  0.0188, max_reward_err_index: 1, max_kl_dist:  0.8675, max_kl_dist_index: 2, max_train_grp_loss:  14.1981, max_train_grp_loss_index: 1, max_val_grp_loss:  13.7907, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2064, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:24,708 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  30, train_loss:  12.5302, val_loss:  12.9107, grad_norm: 0.2561, live_grad: 0.0000, reward_err: 0.0098, 0.0195, 0.0013, KL_dist: 0.8658, 0.4258, 0.8737, param: [ 5.21294234  7.58912733  6.29743695 12.03357375], weights: [0.02338391 0.03588952 0.94072657], train_wt_loss:  37.5905, val_wt_loss: 38.7320, train_grp_loss: [11.70493897 14.24267664 10.31197459], val_grp_loss: [12.67716876 13.84702665 12.20847825], train_hist_grp_loss: [2.96254628 3.39094344 6.6571507 ], cur_train_grp_loss: [0.09360008 0.11358441 0.20633519], max_reward_err:  0.0195, max_reward_err_index: 1, max_kl_dist:  0.8737, max_kl_dist_index: 2, max_train_grp_loss:  14.2427, max_train_grp_loss_index: 1, max_val_grp_loss:  13.8470, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2063, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:25,675 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  31, train_loss:  12.5495, val_loss:  12.9325, grad_norm: 0.2636, live_grad: 0.0000, reward_err: 0.0101, 0.0195, 0.0014, KL_dist: 0.8712, 0.4295, 0.8797, param: [ 5.20146132  7.54704037  6.3201993  12.14283699], weights: [0.02101252 0.03291135 0.94607612], train_wt_loss:  37.6485, val_wt_loss: 38.7974, train_grp_loss: [11.71002887 14.28567077 10.30775738], val_grp_loss: [12.67937448 13.90151878 12.21730212], train_hist_grp_loss: [3.0561858  3.50488486 6.8633902 ], cur_train_grp_loss: [0.09363951 0.11394141 0.20623949], max_reward_err:  0.0195, max_reward_err_index: 1, max_kl_dist:  0.8797, max_kl_dist_index: 2, max_train_grp_loss:  14.2857, max_train_grp_loss_index: 1, max_val_grp_loss:  13.9015, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2062, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:26,671 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  32, train_loss:  12.5682, val_loss:  12.9536, grad_norm: 0.2707, live_grad: 0.0000, reward_err: 0.0102, 0.0195, 0.0016, KL_dist: 0.8763, 0.4330, 0.8853, param: [ 5.19050681  7.50546657  6.34121591 12.24755445], weights: [0.01887392 0.03017716 0.95094892], train_wt_loss:  37.7047, val_wt_loss: 38.8607, train_grp_loss: [11.71519775 14.32696531 10.30405369], val_grp_loss: [12.68151234 13.95409865 12.22590283], train_hist_grp_loss: [3.14986603 3.61917022 7.06954534], cur_train_grp_loss: [0.09368023 0.11428537 0.20615515], max_reward_err:  0.0195, max_reward_err_index: 1, max_kl_dist:  0.8853, max_kl_dist_index: 2, max_train_grp_loss:  14.3270, max_train_grp_loss_index: 1, max_val_grp_loss:  13.9541, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2062, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:27,664 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  33, train_loss:  12.5863, val_loss:  12.9739, grad_norm: 0.2774, live_grad: 0.0000, reward_err: 0.0103, 0.0198, 0.0018, KL_dist: 0.8811, 0.4363, 0.8906, param: [ 5.18009372  7.46463419  6.3605621  12.34761477], weights: [0.01694675 0.02766794 0.95538531], train_wt_loss:  37.7590, val_wt_loss: 38.9216, train_grp_loss: [11.72037368 14.36651157 10.30081167], val_grp_loss: [12.68355703 14.0046728  12.23423176], train_hist_grp_loss: [3.24358761 3.73378594 7.27562642], cur_train_grp_loss: [0.09372158 0.11461572 0.20608107], max_reward_err:  0.0198, max_reward_err_index: 1, max_kl_dist:  0.8906, max_kl_dist_index: 2, max_train_grp_loss:  14.3665, max_train_grp_loss_index: 1, max_val_grp_loss:  14.0047, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2061, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:28,648 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  34, train_loss:  12.6037, val_loss:  12.9933, grad_norm: 0.2836, live_grad: 0.0000, reward_err: 0.0103, 0.0204, 0.0018, KL_dist: 0.8856, 0.4395, 0.8956, param: [ 5.17022908  7.42474725  6.37832075 12.442951  ], weights: [0.01521132 0.02536594 0.95942275], train_wt_loss:  37.8112, val_wt_loss: 38.9800, train_grp_loss: [11.72549448 14.40427914 10.29798244], val_grp_loss: [12.68548863 14.05317092 12.24225   ], train_hist_grp_loss: [3.3373506  3.84871804 7.48164265], cur_train_grp_loss: [0.09376299 0.11493209 0.20601623], max_reward_err:  0.0204, max_reward_err_index: 1, max_kl_dist:  0.8956, max_kl_dist_index: 2, max_train_grp_loss:  14.4043, max_train_grp_loss_index: 1, max_val_grp_loss:  14.0532, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2060, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:29,629 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  35, train_loss:  12.6204, val_loss:  13.0119, grad_norm: 0.2894, live_grad: 0.0000, reward_err: 0.0103, 0.0211, 0.0018, KL_dist: 0.8898, 0.4426, 0.9003, param: [ 5.16091311  7.38598357  6.3945798  12.53353749], weights: [0.01364953 0.0232546  0.96309587], train_wt_loss:  37.8612, val_wt_loss: 39.0358, train_grp_loss: [11.73050762 14.4402541  10.29552048], val_grp_loss: [12.68729228 14.09954443 12.24992775], train_hist_grp_loss: [3.43115455 3.96395227 7.6876023 ], cur_train_grp_loss: [0.09380396 0.11523423 0.20595965], max_reward_err:  0.0211, max_reward_err_index: 1, max_kl_dist:  0.9003, max_kl_dist_index: 2, max_train_grp_loss:  14.4403, max_train_grp_loss_index: 1, max_val_grp_loss:  14.0995, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2060, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:30,602 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  36, train_loss:  12.6363, val_loss:  13.0296, grad_norm: 0.2947, live_grad: 0.0000, reward_err: 0.0103, 0.0220, 0.0020, KL_dist: 0.8938, 0.4455, 0.9046, param: [ 5.15214028  7.34849377  6.40943012 12.61938632], weights: [0.0122448  0.02131854 0.96643665], train_wt_loss:  37.9090, val_wt_loss: 39.0889, train_grp_loss: [11.7353699  14.47443726 10.2933838 ], val_grp_loss: [12.68895773 14.14376488 12.25724351], train_hist_grp_loss: [3.52499862 4.0794743  7.89351271], cur_train_grp_loss: [0.09384406 0.11552203 0.20591041], max_reward_err:  0.0220, max_reward_err_index: 1, max_kl_dist:  0.9046, max_kl_dist_index: 2, max_train_grp_loss:  14.4744, max_train_grp_loss_index: 1, max_val_grp_loss:  14.1438, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2059, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:31,590 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  37, train_loss:  12.6515, val_loss:  13.0465, grad_norm: 0.2997, live_grad: 0.0000, reward_err: 0.0106, 0.0226, 0.0021, KL_dist: 0.8975, 0.4482, 0.9088, param: [ 5.14390033  7.31240119  6.42296366 12.70054352], weights: [0.01098198 0.01954352 0.96947451], train_wt_loss:  37.9544, val_wt_loss: 39.1394, train_grp_loss: [11.74004685 14.50684242 10.291534  ], val_grp_loss: [12.69047881 14.18582229 12.26418317], train_hist_grp_loss: [3.61888158 4.1952698  8.09938038], cur_train_grp_loss: [0.09388296 0.1157955  0.20586768], max_reward_err:  0.0226, max_reward_err_index: 1, max_kl_dist:  0.9088, max_kl_dist_index: 2, max_train_grp_loss:  14.5068, max_train_grp_loss_index: 1, max_val_grp_loss:  14.1858, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2059, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:32,582 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  38, train_loss:  12.6658, val_loss:  13.0624, grad_norm: 0.3043, live_grad: 0.0000, reward_err: 0.0106, 0.0236, 0.0021, KL_dist: 0.9009, 0.4508, 0.9126, param: [ 5.13617914  7.2778025   6.43527193 12.77708507], weights: [0.00984723 0.01791633 0.97223644], train_wt_loss:  37.9975, val_wt_loss: 39.1872, train_grp_loss: [11.74451202 14.53749468 10.28993625], val_grp_loss: [12.69185287 14.22572336 12.27073917], train_hist_grp_loss: [3.71280195 4.31132454 8.30521106], cur_train_grp_loss: [0.09392037 0.11605474 0.20583068], max_reward_err:  0.0236, max_reward_err_index: 1, max_kl_dist:  0.9126, max_kl_dist_index: 2, max_train_grp_loss:  14.5375, max_train_grp_loss_index: 1, max_val_grp_loss:  14.2257, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2058, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:33,629 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  39, train_loss:  12.6794, val_loss:  13.0774, grad_norm: 0.3086, live_grad: 0.0000, reward_err: 0.0104, 0.0240, 0.0022, KL_dist: 0.9041, 0.4533, 0.9162, param: [ 5.12895958  7.24476904  6.44644472 12.84911296], weights: [0.00882798 0.01642481 0.97474721], train_wt_loss:  38.0382, val_wt_loss: 39.2323, train_grp_loss: [11.7487462  14.56642888 10.28855915], val_grp_loss: [12.69308029 14.2634897  12.27690952], train_hist_grp_loss: [3.80675805 4.4276245  8.51100979], cur_train_grp_loss: [0.0939561  0.11629996 0.20579873], max_reward_err:  0.0240, max_reward_err_index: 1, max_kl_dist:  0.9162, max_kl_dist_index: 2, max_train_grp_loss:  14.5664, max_train_grp_loss_index: 1, max_val_grp_loss:  14.2635, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2058, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:34,612 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  40, train_loss:  12.6922, val_loss:  13.0916, grad_norm: 0.3125, live_grad: 0.0000, reward_err: 0.0104, 0.0242, 0.0022, KL_dist: 0.9071, 0.4556, 0.9195, param: [ 5.12222225  7.21334851  6.45656914 12.91675116], weights: [0.00791282 0.01505773 0.97702945], train_wt_loss:  38.0767, val_wt_loss: 39.2748, train_grp_loss: [11.7527366  14.59368806 10.28737458], val_grp_loss: [12.6941639  14.29915601 12.282697  ], train_hist_grp_loss: [3.90074802 4.54415593 8.71678097], cur_train_grp_loss: [0.09398997 0.11653143 0.20577118], max_reward_err:  0.0242, max_reward_err_index: 1, max_kl_dist:  0.9195, max_kl_dist_index: 2, max_train_grp_loss:  14.5937, max_train_grp_loss_index: 1, max_val_grp_loss:  14.2992, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2058, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:35,602 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  41, train_loss:  12.7043, val_loss:  13.1049, grad_norm: 0.3161, live_grad: 0.0000, reward_err: 0.0104, 0.0247, 0.0022, KL_dist: 0.9099, 0.4577, 0.9226, param: [ 5.11594608  7.18356705  6.46572886 12.98014184], weights: [0.00709137 0.01380478 0.97910385], train_wt_loss:  38.1129, val_wt_loss: 39.3148, train_grp_loss: [11.75647595 14.61932202 10.28635749], val_grp_loss: [12.69510851 14.33276825 12.28810828], train_hist_grp_loss: [3.99476991 4.66090543 8.92252846], cur_train_grp_loss: [0.09402189 0.1167495  0.20574749], max_reward_err:  0.0247, max_reward_err_index: 1, max_kl_dist:  0.9226, max_kl_dist_index: 2, max_train_grp_loss:  14.6193, max_train_grp_loss_index: 1, max_val_grp_loss:  14.3328, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:36,598 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  42, train_loss:  12.7156, val_loss:  13.1174, grad_norm: 0.3195, live_grad: 0.0000, reward_err: 0.0104, 0.0254, 0.0022, KL_dist: 0.9125, 0.4598, 0.9254, param: [ 5.11010894  7.15543151  6.47400357 13.03944165], weights: [0.00635427 0.01265644 0.98098929], train_wt_loss:  38.1469, val_wt_loss: 39.3522, train_grp_loss: [11.75996174 14.64338604 10.2854857 ], val_grp_loss: [12.6959205  14.36438196 12.29315322], train_hist_grp_loss: [4.08882172 4.77786001 9.12825561], cur_train_grp_loss: [0.09405181 0.11695458 0.20572715], max_reward_err:  0.0254, max_reward_err_index: 1, max_kl_dist:  0.9254, max_kl_dist_index: 2, max_train_grp_loss:  14.6434, max_train_grp_loss_index: 1, max_val_grp_loss:  14.3644, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:37,583 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  43, train_loss:  12.7263, val_loss:  13.1291, grad_norm: 0.3225, live_grad: 0.0000, reward_err: 0.0104, 0.0264, 0.0022, KL_dist: 0.9148, 0.4617, 0.9281, param: [ 5.10468804  7.12893191  6.48146861 13.09481837], weights: [0.00569303 0.01160399 0.98270298], train_wt_loss:  38.1788, val_wt_loss: 39.3872, train_grp_loss: [11.76319542 14.66593964 10.28473962], val_grp_loss: [12.69660735 14.3940605  12.29784419], train_hist_grp_loss: [4.18290141 4.8950071  9.33396533], cur_train_grp_loss: [0.09407969 0.11714709 0.20570971], max_reward_err:  0.0264, max_reward_err_index: 1, max_kl_dist:  0.9281, max_kl_dist_index: 2, max_train_grp_loss:  14.6659, max_train_grp_loss_index: 1, max_val_grp_loss:  14.3941, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:38,578 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  44, train_loss:  12.7362, val_loss:  13.1400, grad_norm: 0.3253, live_grad: 0.0000, reward_err: 0.0103, 0.0268, 0.0023, KL_dist: 0.9170, 0.4635, 0.9306, param: [ 5.0996604   7.10404374  6.48819476 13.14644773], weights: [0.00509998 0.01063941 0.9842606 ], train_wt_loss:  38.2086, val_wt_loss: 39.4199, train_grp_loss: [11.76618167 14.68704552 10.28410206], val_grp_loss: [12.69717738 14.42187358 12.30219541], train_hist_grp_loss: [4.27700697 5.01233462 9.53966012], cur_train_grp_loss: [0.09410556 0.11732752 0.20569479], max_reward_err:  0.0268, max_reward_err_index: 1, max_kl_dist:  0.9306, max_kl_dist_index: 2, max_train_grp_loss:  14.6870, max_train_grp_loss_index: 1, max_val_grp_loss:  14.4219, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:39,566 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  45, train_loss:  12.7455, val_loss:  13.1501, grad_norm: 0.3279, live_grad: 0.0000, reward_err: 0.0103, 0.0268, 0.0023, KL_dist: 0.9191, 0.4651, 0.9329, param: [ 5.09500307  7.08073038  6.49424816 13.19451062], weights: [0.00456821 0.00975536 0.98567642], train_wt_loss:  38.2364, val_wt_loss: 39.4504, train_grp_loss: [11.76892775 14.7067685  10.28355796], val_grp_loss: [12.69763938 14.4478957  12.30622251], train_hist_grp_loss: [4.37113643 5.12983098 9.74534216], cur_train_grp_loss: [0.09412945 0.11749636 0.20568204], max_reward_err:  0.0268, max_reward_err_index: 1, max_kl_dist:  0.9329, max_kl_dist_index: 2, max_train_grp_loss:  14.7068, max_train_grp_loss_index: 1, max_val_grp_loss:  14.4479, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:40,562 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  46, train_loss:  12.7541, val_loss:  13.1596, grad_norm: 0.3303, live_grad: 0.0000, reward_err: 0.0103, 0.0273, 0.0023, KL_dist: 0.9209, 0.4667, 0.9350, param: [ 5.09069346  7.05894527  6.49969027 13.23919059], weights: [0.00409148 0.00894509 0.98696343], train_wt_loss:  38.2623, val_wt_loss: 39.4788, train_grp_loss: [11.77144294 14.7251747  10.28309419], val_grp_loss: [12.6980024  14.47220486 12.30994201], train_hist_grp_loss: [4.46528785 5.24748513 9.95101332], cur_train_grp_loss: [0.09415142 0.11765415 0.20567116], max_reward_err:  0.0273, max_reward_err_index: 1, max_kl_dist:  0.9350, max_kl_dist_index: 2, max_train_grp_loss:  14.7252, max_train_grp_loss_index: 1, max_val_grp_loss:  14.4722, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:41,544 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  47, train_loss:  12.7621, val_loss:  13.1684, grad_norm: 0.3325, live_grad: 0.0000, reward_err: 0.0103, 0.0275, 0.0023, KL_dist: 0.9227, 0.4681, 0.9369, param: [ 5.08670955  7.03863399  6.504578   13.2806716 ], weights: [0.00366417 0.00820242 0.98813341], train_wt_loss:  38.2864, val_wt_loss: 39.5051, train_grp_loss: [11.77373798 14.74233072 10.28269934], val_grp_loss: [12.69827553 14.49488134 12.31337096], train_hist_grp_loss: [ 4.55945939  5.36528653 10.1566752 ], cur_train_grp_loss: [0.09417154 0.1178014  0.20566188], max_reward_err:  0.0275, max_reward_err_index: 1, max_kl_dist:  0.9369, max_kl_dist_index: 2, max_train_grp_loss:  14.7423, max_train_grp_loss_index: 1, max_val_grp_loss:  14.4949, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:42,524 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  48, train_loss:  12.7696, val_loss:  13.1765, grad_norm: 0.3344, live_grad: 0.0000, reward_err: 0.0105, 0.0278, 0.0025, KL_dist: 0.9243, 0.4695, 0.9387, param: [ 5.08302998  7.01973616  6.50896381 13.31913623], weights: [0.00328121 0.00752168 0.9891971 ], train_wt_loss:  38.3088, val_wt_loss: 39.5296, train_grp_loss: [11.77582467 14.75830302 10.28236351], val_grp_loss: [12.69846773 14.51600663 12.31652663], train_hist_grp_loss: [ 4.6536493   5.48322517 10.36232919], cur_train_grp_loss: [0.0941899  0.11793865 0.20565399], max_reward_err:  0.0278, max_reward_err_index: 1, max_kl_dist:  0.9387, max_kl_dist_index: 2, max_train_grp_loss:  14.7583, max_train_grp_loss_index: 1, max_val_grp_loss:  14.5160, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2057, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:43,530 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  49, train_loss:  12.7765, val_loss:  13.1841, grad_norm: 0.3363, live_grad: 0.0000, reward_err: 0.0105, 0.0283, 0.0025, KL_dist: 0.9258, 0.4707, 0.9404, param: [ 5.07963422  7.00218705  6.5128959  13.35476404], weights: [0.00293806 0.00689769 0.99016425], train_wt_loss:  38.3296, val_wt_loss: 39.5522, train_grp_loss: [11.77771545 14.77315728 10.28207816], val_grp_loss: [12.69858768 14.53566249 12.31942623], train_hist_grp_loss: [ 4.74785589  5.60129159 10.56797646], cur_train_grp_loss: [0.0942066  0.11806642 0.20564727], max_reward_err:  0.0283, max_reward_err_index: 1, max_kl_dist:  0.9404, max_kl_dist_index: 2, max_train_grp_loss:  14.7732, max_train_grp_loss_index: 1, max_val_grp_loss:  14.5357, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:44,513 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  50, train_loss:  12.7830, val_loss:  13.1911, grad_norm: 0.3379, live_grad: 0.0000, reward_err: 0.0105, 0.0285, 0.0025, KL_dist: 0.9271, 0.4719, 0.9419, param: [ 5.07650266  6.98591914  6.51641843 13.3877303 ], weights: [0.00263062 0.00632568 0.9910437 ], train_wt_loss:  38.3489, val_wt_loss: 39.5732, train_grp_loss: [11.77942311 14.78695798 10.2818359 ], val_grp_loss: [12.69864372 14.55393014 12.32208669], train_hist_grp_loss: [ 4.84207762  5.71947685 10.77361802], cur_train_grp_loss: [0.09422172 0.11818526 0.20564156], max_reward_err:  0.0285, max_reward_err_index: 1, max_kl_dist:  0.9419, max_kl_dist_index: 2, max_train_grp_loss:  14.7870, max_train_grp_loss_index: 1, max_val_grp_loss:  14.5539, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:45,532 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  51, train_loss:  12.7889, val_loss:  13.1975, grad_norm: 0.3394, live_grad: 0.0000, reward_err: 0.0105, 0.0285, 0.0025, KL_dist: 0.9284, 0.4730, 0.9434, param: [ 5.0736166   6.97086331  6.51957175 13.41820496], weights: [0.0023552  0.00580131 0.9918435 ], train_wt_loss:  38.3667, val_wt_loss: 39.5926, train_grp_loss: [11.78096051 14.79976799 10.28163039], val_grp_loss: [12.69864373 14.57088956 12.32452452], train_hist_grp_loss: [ 4.936313    5.83777252 10.97925474], cur_train_grp_loss: [0.09423538 0.11829566 0.20563672], max_reward_err:  0.0285, max_reward_err_index: 1, max_kl_dist:  0.9434, max_kl_dist_index: 2, max_train_grp_loss:  14.7998, max_train_grp_loss_index: 1, max_val_grp_loss:  14.5709, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:46,523 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  52, train_loss:  12.7944, val_loss:  13.2035, grad_norm: 0.3408, live_grad: 0.0000, reward_err: 0.0105, 0.0291, 0.0025, KL_dist: 0.9295, 0.4740, 0.9447, param: [ 5.07095837  6.95694999  6.5223926  13.44635181], weights: [0.00210849 0.00532058 0.99257093], train_wt_loss:  38.3832, val_wt_loss: 39.6105, train_grp_loss: [11.7823404  14.81164824 10.28145619], val_grp_loss: [12.69859511 14.58661898 12.32675565], train_hist_grp_loss: [ 5.03056069  5.95617066 11.18488735], cur_train_grp_loss: [0.09424768 0.11839814 0.20563261], max_reward_err:  0.0291, max_reward_err_index: 1, max_kl_dist:  0.9447, max_kl_dist_index: 2, max_train_grp_loss:  14.8116, max_train_grp_loss_index: 1, max_val_grp_loss:  14.5866, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:47,501 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  53, train_loss:  12.7995, val_loss:  13.2090, grad_norm: 0.3421, live_grad: 0.0000, reward_err: 0.0105, 0.0291, 0.0025, KL_dist: 0.9306, 0.4749, 0.9459, param: [ 5.06851127  6.94411001  6.52491442 13.4723279 ], weights: [0.00188753 0.00487984 0.99323263], train_wt_loss:  38.3984, val_wt_loss: 39.6270, train_grp_loss: [11.78357519 14.82265746 10.28130861], val_grp_loss: [12.69850473 14.60119435 12.32879536], train_hist_grp_loss: [ 5.12481941  6.07466385 11.39051647], cur_train_grp_loss: [0.09425872 0.11849319 0.20562912], max_reward_err:  0.0291, max_reward_err_index: 1, max_kl_dist:  0.9459, max_kl_dist_index: 2, max_train_grp_loss:  14.8227, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6012, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:48,489 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  54, train_loss:  12.8042, val_loss:  13.2141, grad_norm: 0.3433, live_grad: 0.0000, reward_err: 0.0105, 0.0295, 0.0025, KL_dist: 0.9316, 0.4757, 0.9470, param: [ 5.06625958  6.93227536  6.52716754 13.49628312], weights: [0.00168964 0.00447575 0.99383461], train_wt_loss:  38.4125, val_wt_loss: 39.6422, train_grp_loss: [11.78467689 14.83285204 10.28118366], val_grp_loss: [12.69837893 14.61468905 12.3306582 ], train_hist_grp_loss: [ 5.21908801  6.19324511 11.59614265], cur_train_grp_loss: [0.0942686  0.11858126 0.20562617], max_reward_err:  0.0295, max_reward_err_index: 1, max_kl_dist:  0.9470, max_kl_dist_index: 2, max_train_grp_loss:  14.8329, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6147, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:49,528 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  55, train_loss:  12.8085, val_loss:  13.2187, grad_norm: 0.3443, live_grad: 0.0000, reward_err: 0.0105, 0.0299, 0.0025, KL_dist: 0.9325, 0.4765, 0.9480, param: [ 5.06418859  6.92137978  6.52917943 13.5183599 ], weights: [0.00151243 0.00410524 0.99438233], train_wt_loss:  38.4255, val_wt_loss: 39.6562, train_grp_loss: [11.78565698 14.84228588 10.28107793], val_grp_loss: [12.69822351 14.62717359 12.33235796], train_hist_grp_loss: [ 5.31336543  6.31190792 11.80176632], cur_train_grp_loss: [0.09427742 0.11866282 0.20562367], max_reward_err:  0.0299, max_reward_err_index: 1, max_kl_dist:  0.9480, max_kl_dist_index: 2, max_train_grp_loss:  14.8423, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6272, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:50,523 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  56, train_loss:  12.8125, val_loss:  13.2230, grad_norm: 0.3453, live_grad: 0.0000, reward_err: 0.0105, 0.0306, 0.0025, KL_dist: 0.9333, 0.4772, 0.9489, param: [ 5.06228451  6.91135921  6.53097495 13.53869313], weights: [0.00135376 0.0037655  0.99488074], train_wt_loss:  38.4374, val_wt_loss: 39.6691, train_grp_loss: [11.78652633 14.85101029 10.28098851], val_grp_loss: [12.69804371 14.6387154  12.33390762], train_hist_grp_loss: [ 5.40765068  6.43064621 12.00738788], cur_train_grp_loss: [0.09428526 0.11873829 0.20562156], max_reward_err:  0.0306, max_reward_err_index: 1, max_kl_dist:  0.9489, max_kl_dist_index: 2, max_train_grp_loss:  14.8510, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6387, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:51,513 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  57, train_loss:  12.8161, val_loss:  13.2270, grad_norm: 0.3462, live_grad: 0.0000, reward_err: 0.0105, 0.0306, 0.0025, KL_dist: 0.9340, 0.4779, 0.9498, param: [ 5.06053448  6.90215213  6.53257654 13.55741009], weights: [0.00121169 0.00345397 0.99533434], train_wt_loss:  38.4484, val_wt_loss: 39.6810, train_grp_loss: [11.78729521 14.85907399 10.28091291], val_grp_loss: [12.69784428 14.64937872 12.33531936], train_hist_grp_loss: [ 5.50194289  6.54945429 12.21300765], cur_train_grp_loss: [0.09429221 0.11880808 0.20561977], max_reward_err:  0.0306, max_reward_err_index: 1, max_kl_dist:  0.9498, max_kl_dist_index: 2, max_train_grp_loss:  14.8591, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6494, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:52,493 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  58, train_loss:  12.8195, val_loss:  13.2306, grad_norm: 0.3470, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9347, 0.4785, 0.9506, param: [ 5.05892653  6.89369981  6.53400444 13.57463052], weights: [0.00108449 0.0031683  0.99574721], train_wt_loss:  38.4585, val_wt_loss: 39.6919, train_grp_loss: [11.78797319 14.86652307 10.28084901], val_grp_loss: [12.69762946 14.65922454 12.33660458], train_hist_grp_loss: [ 5.59624125  6.66832688 12.41862591], cur_train_grp_loss: [0.09429836 0.11887259 0.20561826], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9506, max_kl_dist_index: 2, max_train_grp_loss:  14.8665, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6592, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:53,542 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  59, train_loss:  12.8226, val_loss:  13.2340, grad_norm: 0.3477, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9354, 0.4791, 0.9513, param: [ 5.05744952  6.88594647  6.53527688 13.59046681], weights: [9.70613402e-04 2.90631802e-03 9.96123069e-01], train_wt_loss:  38.4679, val_wt_loss: 39.7019, train_grp_loss: [11.78856921 14.87340101 10.28079504], val_grp_loss: [12.69740302 14.66831054 12.33777389], train_hist_grp_loss: [ 5.69054504  6.78725907 12.62424289], cur_train_grp_loss: [0.09430379 0.11893218 0.20561698], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9513, max_kl_dist_index: 2, max_train_grp_loss:  14.8734, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6683, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:54,520 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  60, train_loss:  12.8255, val_loss:  13.2370, grad_norm: 0.3484, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9360, 0.4796, 0.9520, param: [ 5.05609311  6.87883937  6.53641027 13.60502413], weights: [8.68670925e-04 2.66606013e-03 9.96465269e-01], train_wt_loss:  38.4764, val_wt_loss: 39.7111, train_grp_loss: [11.78909152 14.87974876 10.28074946], val_grp_loss: [12.69716829 14.67669118 12.33883715], train_hist_grp_loss: [ 5.78485359  6.90624628 12.82985879], cur_train_grp_loss: [0.09430855 0.11898721 0.2056159 ], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9520, max_kl_dist_index: 2, max_train_grp_loss:  14.8797, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6767, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:55,543 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  61, train_loss:  12.8281, val_loss:  13.2398, grad_norm: 0.3490, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9365, 0.4801, 0.9526, param: [ 5.05484773  6.87232884  6.53741935 13.61840072], weights: [7.77415356e-04 2.44571480e-03 9.96776870e-01], train_wt_loss:  38.4843, val_wt_loss: 39.7195, train_grp_loss: [11.78954773 14.88560477 10.28071097], val_grp_loss: [12.69692817 14.68441765 12.33980349], train_hist_grp_loss: [ 5.87916633  7.02528427 13.03547378], cur_train_grp_loss: [0.09431273 0.11903799 0.20561499], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9526, max_kl_dist_index: 2, max_train_grp_loss:  14.8856, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6844, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:56,526 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  62, train_loss:  12.8305, val_loss:  13.2424, grad_norm: 0.3495, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9370, 0.4805, 0.9532, param: [ 5.0537045   6.86636829  6.53831736 13.63068815], weights: [6.95729953e-04 2.24362465e-03 9.97060645e-01], train_wt_loss:  38.4915, val_wt_loss: 39.7273, train_grp_loss: [11.78994483 14.89100508 10.28067849], val_grp_loss: [12.69668522 14.69153803 12.34068135], train_hist_grp_loss: [ 5.97348271  7.14436911 13.241088  ], cur_train_grp_loss: [0.09431638 0.11908484 0.20561422], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9532, max_kl_dist_index: 2, max_train_grp_loss:  14.8910, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6915, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:57,500 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  63, train_loss:  12.8327, val_loss:  13.2448, grad_norm: 0.3501, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9374, 0.4809, 0.9537, param: [ 5.05265521  6.86091411  6.5391162  13.64197163], weights: [6.22614018e-04 2.05827113e-03 9.97319115e-01], train_wt_loss:  38.4982, val_wt_loss: 39.7344, train_grp_loss: [11.79028921 14.89598342 10.28065107], val_grp_loss: [12.6964416  14.69809735 12.3414785 ], train_hist_grp_loss: [ 6.06780227  7.26349715 13.44670157], cur_train_grp_loss: [0.09431956 0.11912804 0.20561357], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9537, max_kl_dist_index: 2, max_train_grp_loss:  14.8960, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6981, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:58,491 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  64, train_loss:  12.8348, val_loss:  13.2470, grad_norm: 0.3505, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9379, 0.4813, 0.9542, param: [ 5.0516923   6.85592564  6.53982651 13.65233033], weights: [5.57170959e-04 1.88826284e-03 9.97554566e-01], train_wt_loss:  38.5043, val_wt_loss: 39.7409, train_grp_loss: [11.79058669 14.90057129 10.28062794], val_grp_loss: [12.6961992 14.7041377 12.3422021], train_hist_grp_loss: [ 6.16212458  7.38266501 13.65231459], cur_train_grp_loss: [0.09432231 0.11916787 0.20561302], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9542, max_kl_dist_index: 2, max_train_grp_loss:  14.9006, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7041, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:32:59,475 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  65, train_loss:  12.8366, val_loss:  13.2490, grad_norm: 0.3509, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9382, 0.4816, 0.9546, param: [ 5.05080876  6.85136504  6.54045785 13.66183766], weights: [4.98597557e-04 1.73232477e-03 9.97769078e-01], train_wt_loss:  38.5099, val_wt_loss: 39.7469, train_grp_loss: [11.79084255 14.90479807 10.28060843], val_grp_loss: [12.69595959 14.70969836 12.34285873], train_hist_grp_loss: [ 6.25644927  7.50186958 13.85792715], cur_train_grp_loss: [0.09432469 0.11920457 0.20561256], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9546, max_kl_dist_index: 2, max_train_grp_loss:  14.9048, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7097, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:00,457 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  66, train_loss:  12.8383, val_loss:  13.2508, grad_norm: 0.3513, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9386, 0.4819, 0.9550, param: [ 5.04999816  6.84719719  6.54101877 13.67056165], weights: [4.46174316e-04 1.58928844e-03 9.97964537e-01], train_wt_loss:  38.5150, val_wt_loss: 39.7523, train_grp_loss: [11.79106158 14.90869112 10.28059197], val_grp_loss: [12.6957241  14.71481593 12.34345441], train_hist_grp_loss: [ 6.35077601  7.62110797 14.06353931], cur_train_grp_loss: [0.09432674 0.11923838 0.20561217], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9550, max_kl_dist_index: 2, max_train_grp_loss:  14.9087, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7148, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:01,445 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  67, train_loss:  12.8399, val_loss:  13.2524, grad_norm: 0.3517, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9389, 0.4822, 0.9554, param: [ 5.04925453  6.84338955  6.54151691 13.67856524], weights: [3.99256810e-04 1.45808299e-03 9.98142660e-01], train_wt_loss:  38.5197, val_wt_loss: 39.7573, train_grp_loss: [11.79124809 14.9122759  10.28057809], val_grp_loss: [12.69549381 14.71952446 12.34399467], train_hist_grp_loss: [ 6.44510451  7.7403775  14.26915115], cur_train_grp_loss: [0.09432849 0.11926953 0.20561184], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9554, max_kl_dist_index: 2, max_train_grp_loss:  14.9123, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7195, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:02,436 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  68, train_loss:  12.8413, val_loss:  13.2540, grad_norm: 0.3520, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9392, 0.4825, 0.9558, param: [ 5.04857241  6.83991202  6.54195912 13.68590661], weights: [3.57267893e-04 1.33772691e-03 9.98305005e-01], train_wt_loss:  38.5240, val_wt_loss: 39.7619, train_grp_loss: [11.79140597 14.91557607 10.28056639], val_grp_loss: [12.69526962 14.7238556  12.34448456], train_hist_grp_loss: [ 6.53943449  7.8596757  14.47476272], cur_train_grp_loss: [0.09432998 0.11929821 0.20561156], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9558, max_kl_dist_index: 2, max_train_grp_loss:  14.9156, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7239, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:03,464 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  69, train_loss:  12.8427, val_loss:  13.2554, grad_norm: 0.3523, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9395, 0.4827, 0.9561, param: [ 5.04794675  6.83673681  6.54235152 13.6926395 ], weights: [3.19690725e-04 1.22732050e-03 9.98452989e-01], train_wt_loss:  38.5280, val_wt_loss: 39.7661, train_grp_loss: [11.7915387  14.91861357 10.28055652], val_grp_loss: [12.69505223 14.72783872 12.34492868], train_hist_grp_loss: [ 6.63376574  7.97900031 14.68037404], cur_train_grp_loss: [0.09433125 0.11932461 0.20561133], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9561, max_kl_dist_index: 2, max_train_grp_loss:  14.9186, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7278, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:04,442 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  70, train_loss:  12.8439, val_loss:  13.2567, grad_norm: 0.3526, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9397, 0.4830, 0.9564, param: [ 5.04737293  6.83383832  6.54269957 13.69881346], weights: [2.86062504e-04 1.12603907e-03 9.98587898e-01], train_wt_loss:  38.5316, val_wt_loss: 39.7700, train_grp_loss: [11.79164943 14.92140879 10.2805482 ], val_grp_loss: [12.69484217 14.73150103 12.34533125], train_hist_grp_loss: [ 6.72809805  8.09834922 14.88598517], cur_train_grp_loss: [0.09433231 0.11934891 0.20561113], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9564, max_kl_dist_index: 2, max_train_grp_loss:  14.9214, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7315, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:05,440 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  71, train_loss:  12.8450, val_loss:  13.2578, grad_norm: 0.3528, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9399, 0.4832, 0.9566, param: [ 5.04684668  6.83119297  6.54300816 13.70447419], weights: [2.55968845e-04 1.03312657e-03 9.98710905e-01], train_wt_loss:  38.5349, val_wt_loss: 39.7735, train_grp_loss: [11.79174093 14.92398057 10.28054119], val_grp_loss: [12.69463985 14.73486776 12.34569611], train_hist_grp_loss: [ 6.82243124  8.21772049 15.09159614], cur_train_grp_loss: [0.0943332  0.11937127 0.20561096], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9566, max_kl_dist_index: 2, max_train_grp_loss:  14.9240, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7349, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:06,436 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  72, train_loss:  12.8460, val_loss:  13.2589, grad_norm: 0.3530, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9402, 0.4834, 0.9569, param: [ 5.04636407  6.82877906  6.54328163 13.70966379], weights: [2.29038737e-04 9.47889857e-04 9.98823071e-01], train_wt_loss:  38.5380, val_wt_loss: 39.7767, train_grp_loss: [11.79181571 14.9263464  10.28053528], val_grp_loss: [12.69444555 14.73796226 12.34602675], train_hist_grp_loss: [ 6.91676517  8.33711234 15.29720696], cur_train_grp_loss: [0.09433393 0.11939184 0.20561082], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9569, max_kl_dist_index: 2, max_train_grp_loss:  14.9263, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7380, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:07,413 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  73, train_loss:  12.8469, val_loss:  13.2599, grad_norm: 0.3532, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9403, 0.4835, 0.9571, param: [ 5.04592151  6.82657668  6.54352388 13.71442102], weights: [2.04940025e-04 8.69693444e-04 9.98925367e-01], train_wt_loss:  38.5408, val_wt_loss: 39.7797, train_grp_loss: [11.79187597 14.92852244 10.2805303 ], val_grp_loss: [12.69425945 14.74080609 12.34632635], train_hist_grp_loss: [ 7.0110997   8.45652311 15.50281767], cur_train_grp_loss: [0.09433453 0.11941077 0.20561071], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9571, max_kl_dist_index: 2, max_train_grp_loss:  14.9285, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7408, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:08,402 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  74, train_loss:  12.8478, val_loss:  13.2608, grad_norm: 0.3534, live_grad: 0.0000, reward_err: 0.0105, 0.0309, 0.0025, KL_dist: 0.9405, 0.4837, 0.9573, param: [ 5.04551569  6.82456755  6.54373837 13.71878153], weights: [1.83375356e-04 7.97954645e-04 9.99018670e-01], train_wt_loss:  38.5433, val_wt_loss: 39.7824, train_grp_loss: [11.79192369 14.93052362 10.2805261 ], val_grp_loss: [12.69408163 14.74341921 12.34659781], train_hist_grp_loss: [ 7.1054347   8.57595129 15.70842827], cur_train_grp_loss: [0.09433501 0.11942818 0.20561061], max_reward_err:  0.0309, max_reward_err_index: 1, max_kl_dist:  0.9573, max_kl_dist_index: 2, max_train_grp_loss:  14.9305, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7434, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:09,379 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  75, train_loss:  12.8486, val_loss:  13.2616, grad_norm: 0.3536, live_grad: 0.0000, reward_err: 0.0105, 0.0312, 0.0025, KL_dist: 0.9407, 0.4838, 0.9575, param: [ 5.04514356  6.82273491  6.5439282  13.72277812], weights: [1.64078543e-04 7.32139170e-04 9.99103782e-01], train_wt_loss:  38.5457, val_wt_loss: 39.7849, train_grp_loss: [11.79196059 14.93236378 10.28052256], val_grp_loss: [12.6939121  14.74582002 12.34684375], train_hist_grp_loss: [ 7.19977009  8.69539548 15.9140388 ], cur_train_grp_loss: [0.09433539 0.11944419 0.20561052], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9575, max_kl_dist_index: 2, max_train_grp_loss:  14.9324, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7458, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:10,362 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  76, train_loss:  12.8493, val_loss:  13.2624, grad_norm: 0.3537, live_grad: 0.0000, reward_err: 0.0105, 0.0312, 0.0025, KL_dist: 0.9408, 0.4840, 0.9577, param: [ 5.04480235  6.82106339  6.54409611 13.72644093], weights: [1.46811314e-04 6.71757084e-04 9.99181432e-01], train_wt_loss:  38.5478, val_wt_loss: 39.7871, train_grp_loss: [11.79198823 14.93405565 10.28051958], val_grp_loss: [12.69375081 14.74802552 12.34706657], train_hist_grp_loss: [ 7.29410578  8.81485439 16.11964925], cur_train_grp_loss: [0.09433568 0.11945891 0.20561045], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9577, max_kl_dist_index: 2, max_train_grp_loss:  14.9341, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7480, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:11,338 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  77, train_loss:  12.8499, val_loss:  13.2631, grad_norm: 0.3539, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9410, 0.4841, 0.9578, param: [ 5.04448948  6.81953895  6.54424457 13.72979766], weights: [1.31360389e-04 6.16359102e-04 9.99252281e-01], train_wt_loss:  38.5498, val_wt_loss: 39.7892, train_grp_loss: [11.79200795 14.93561103 10.28051706], val_grp_loss: [12.69359763 14.7500514  12.34726844], train_hist_grp_loss: [ 7.38844168  8.93432683 16.32525964], cur_train_grp_loss: [0.09433591 0.11947245 0.20561039], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9578, max_kl_dist_index: 2, max_train_grp_loss:  14.9356, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7501, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:12,319 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  78, train_loss:  12.8505, val_loss:  13.2637, grad_norm: 0.3540, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9411, 0.4842, 0.9580, param: [ 5.0442026   6.81814871  6.54437576 13.73287376], weights: [1.17534866e-04 5.65533196e-04 9.99316932e-01], train_wt_loss:  38.5516, val_wt_loss: 39.7911, train_grp_loss: [11.79202094 14.93704077 10.28051495], val_grp_loss: [12.69345241 14.7519121  12.34745131], train_hist_grp_loss: [ 7.48277775  9.05381172 16.53086998], cur_train_grp_loss: [0.09433606 0.11948489 0.20561034], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9580, max_kl_dist_index: 2, max_train_grp_loss:  14.9370, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7519, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:13,300 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  79, train_loss:  12.8511, val_loss:  13.2643, grad_norm: 0.3541, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9412, 0.4843, 0.9581, param: [ 5.04393957  6.8168809   6.54449163 13.73569257], weights: [1.05163884e-04 5.18901494e-04 9.99375935e-01], train_wt_loss:  38.5532, val_wt_loss: 39.7928, train_grp_loss: [11.79202826 14.9383549  10.28051316], val_grp_loss: [12.69331497 14.75362094 12.34761699], train_hist_grp_loss: [ 7.57711392  9.17330805 16.73648028], cur_train_grp_loss: [0.09433617 0.11949633 0.2056103 ], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9581, max_kl_dist_index: 2, max_train_grp_loss:  14.9384, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7536, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:14,281 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  80, train_loss:  12.8516, val_loss:  13.2648, grad_norm: 0.3542, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9413, 0.4844, 0.9582, param: [ 5.0436984   6.81572479  6.5445939  13.73827555], weights: [9.40945176e-05 4.76117430e-04 9.99429788e-01], train_wt_loss:  38.5547, val_wt_loss: 39.7944, train_grp_loss: [11.79203082 14.93956267 10.28051166], val_grp_loss: [12.69318509 14.7551902  12.34776709], train_hist_grp_loss: [ 7.67145014  9.29281488 16.94209054], cur_train_grp_loss: [0.09433623 0.11950684 0.20561026], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9582, max_kl_dist_index: 2, max_train_grp_loss:  14.9396, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7552, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:15,261 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  81, train_loss:  12.8520, val_loss:  13.2653, grad_norm: 0.3543, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9414, 0.4845, 0.9583, param: [ 5.04347727  6.81467054  6.54468413 13.74064236], weights: [8.41899018e-05 4.36863147e-04 9.99478947e-01], train_wt_loss:  38.5561, val_wt_loss: 39.7959, train_grp_loss: [11.79202941 14.94067259 10.28051039], val_grp_loss: [12.69306251 14.75663117 12.34790308], train_hist_grp_loss: [ 7.76578639  9.41233139 17.14770077], cur_train_grp_loss: [0.09433625 0.1195165  0.20561023], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9583, max_kl_dist_index: 2, max_train_grp_loss:  14.9407, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7566, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:16,254 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  82, train_loss:  12.8525, val_loss:  13.2657, grad_norm: 0.3544, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9415, 0.4846, 0.9584, param: [ 5.04327452  6.81370918  6.54476367 13.74281105], weights: [7.53275480e-05 4.00847099e-04 9.99523825e-01], train_wt_loss:  38.5574, val_wt_loss: 39.7972, train_grp_loss: [11.79202474 14.94169251 10.28050932], val_grp_loss: [12.69294698 14.75795425 12.34802629], train_hist_grp_loss: [ 7.86012262  9.53185677 17.35331098], cur_train_grp_loss: [0.09433624 0.11952538 0.20561021], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9584, max_kl_dist_index: 2, max_train_grp_loss:  14.9417, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7580, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:17,245 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  83, train_loss:  12.8529, val_loss:  13.2661, grad_norm: 0.3545, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9416, 0.4846, 0.9585, param: [ 5.04308863  6.81283252  6.54483376 13.74479819], weights: [6.73978364e-05 3.67801872e-04 9.99564800e-01], train_wt_loss:  38.5586, val_wt_loss: 39.7984, train_grp_loss: [11.7920174  14.94262967 10.28050842], val_grp_loss: [12.69283823 14.75916902 12.34813793], train_hist_grp_loss: [ 7.95445882  9.65139031 17.55892117], cur_train_grp_loss: [0.0943362  0.11953354 0.20561019], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9585, max_kl_dist_index: 2, max_train_grp_loss:  14.9426, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7592, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:18,234 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  84, train_loss:  12.8532, val_loss:  13.2665, grad_norm: 0.3546, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9416, 0.4847, 0.9586, param: [ 5.04291818  6.81203308  6.54489546 13.74661895], weights: [6.03026661e-05 3.37482177e-04 9.99602215e-01], train_wt_loss:  38.5596, val_wt_loss: 39.7995, train_grp_loss: [11.79200792 14.94349072 10.28050767], val_grp_loss: [12.69273598 14.76028429 12.34823908], train_hist_grp_loss: [ 8.04879496  9.77093134 17.76453134], cur_train_grp_loss: [0.09433614 0.11954104 0.20561017], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9586, max_kl_dist_index: 2, max_train_grp_loss:  14.9435, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7603, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:19,224 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  85, train_loss:  12.8535, val_loss:  13.2669, grad_norm: 0.3547, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9417, 0.4848, 0.9587, param: [ 5.0427619   6.81130405  6.54494975 13.74828726], weights: [5.39542469e-05 3.09663014e-04 9.99636383e-01], train_wt_loss:  38.5606, val_wt_loss: 39.8006, train_grp_loss: [11.79199674 14.9442818  10.28050703], val_grp_loss: [12.69263993 14.76130815 12.34833075], train_hist_grp_loss: [ 8.14313102  9.89047927 17.97014149], cur_train_grp_loss: [0.09433606 0.11954793 0.20561015], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9587, max_kl_dist_index: 2, max_train_grp_loss:  14.9443, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7613, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:20,218 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  86, train_loss:  12.8538, val_loss:  13.2672, grad_norm: 0.3547, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9418, 0.4848, 0.9588, param: [ 5.04261861  6.81063921  6.54499748 13.74981587], weights: [4.82740166e-05 2.84137985e-04 9.99667588e-01], train_wt_loss:  38.5615, val_wt_loss: 39.8015, train_grp_loss: [11.79198426 14.94500856 10.28050649], val_grp_loss: [12.69254982 14.76224805 12.34841382], train_hist_grp_loss: [ 8.237467   10.01003352 18.17575163], cur_train_grp_loss: [0.09433597 0.11955425 0.20561014], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9588, max_kl_dist_index: 2, max_train_grp_loss:  14.9450, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7622, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:21,199 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  87, train_loss:  12.8541, val_loss:  13.2674, grad_norm: 0.3548, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9418, 0.4849, 0.9588, param: [ 5.04248723  6.81003287  6.54503941 13.75121649], weights: [4.31916726e-05 2.60717758e-04 9.99696091e-01], train_wt_loss:  38.5623, val_wt_loss: 39.8023, train_grp_loss: [11.7919708  14.94567618 10.28050604], val_grp_loss: [12.69246534 14.76311085 12.34848911], train_hist_grp_loss: [ 8.33180287 10.12959359 18.38136176], cur_train_grp_loss: [0.09433587 0.11956007 0.20561013], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9588, max_kl_dist_index: 2, max_train_grp_loss:  14.9457, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7631, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:22,186 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  88, train_loss:  12.8544, val_loss:  13.2677, grad_norm: 0.3548, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9419, 0.4849, 0.9589, param: [ 5.04236677  6.80947987  6.54507621 13.75249984], weights: [3.86443045e-05 2.39228645e-04 9.99722127e-01], train_wt_loss:  38.5631, val_wt_loss: 39.8031, train_grp_loss: [11.79195666 14.94628945 10.28050565], val_grp_loss: [12.69238622 14.76390284 12.34855735], train_hist_grp_loss: [ 8.42613864 10.249159   18.58697188], cur_train_grp_loss: [0.09433577 0.11956541 0.20561012], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9589, max_kl_dist_index: 2, max_train_grp_loss:  14.9463, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7639, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:23,186 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  89, train_loss:  12.8546, val_loss:  13.2679, grad_norm: 0.3549, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9419, 0.4849, 0.9589, param: [ 5.04225631  6.8089755   6.54510848 13.75367575], weights: [3.45756174e-05 2.19511311e-04 9.99745913e-01], train_wt_loss:  38.5637, val_wt_loss: 39.8038, train_grp_loss: [11.79194206 14.94685277 10.28050533], val_grp_loss: [12.69231219 14.76462981 12.3486192 ], train_hist_grp_loss: [ 8.52047429 10.36872932 18.792582  ], cur_train_grp_loss: [0.09433565 0.11957032 0.20561011], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9589, max_kl_dist_index: 2, max_train_grp_loss:  14.9469, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7646, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:24,206 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  90, train_loss:  12.8548, val_loss:  13.2682, grad_norm: 0.3549, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9420, 0.4850, 0.9590, param: [ 5.04215503  6.80851544  6.54513675 13.75475321], weights: [3.09352374e-05 2.01419584e-04 9.99767645e-01], train_wt_loss:  38.5644, val_wt_loss: 39.8045, train_grp_loss: [11.79192721 14.94737019 10.28050506], val_grp_loss: [12.69224296 14.76529708 12.34867528], train_hist_grp_loss: [ 8.61480983 10.48830414 18.9981921 ], cur_train_grp_loss: [0.09433554 0.11957482 0.20561011], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9590, max_kl_dist_index: 2, max_train_grp_loss:  14.9474, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7653, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:25,191 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  91, train_loss:  12.8550, val_loss:  13.2684, grad_norm: 0.3550, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9420, 0.4850, 0.9590, param: [ 5.04206217  6.80809578  6.54516148 13.7557405 ], weights: [2.76780888e-05 1.84819364e-04 9.99787503e-01], train_wt_loss:  38.5649, val_wt_loss: 39.8051, train_grp_loss: [11.79191227 14.94784542 10.28050483], val_grp_loss: [12.69217828 14.76590952 12.34872613], train_hist_grp_loss: [ 8.70914525 10.6078831  19.2038022 ], cur_train_grp_loss: [0.09433542 0.11957896 0.2056101 ], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9590, max_kl_dist_index: 2, max_train_grp_loss:  14.9478, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7659, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:26,170 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  92, train_loss:  12.8552, val_loss:  13.2685, grad_norm: 0.3550, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9420, 0.4851, 0.9591, param: [ 5.04197702  6.80771296  6.54518311 13.75664515], weights: [2.47638377e-05 1.69587625e-04 9.99805649e-01], train_wt_loss:  38.5655, val_wt_loss: 39.8056, train_grp_loss: [11.79189739 14.94828188 10.28050464], val_grp_loss: [12.69211789 14.76647164 12.34877223], train_hist_grp_loss: [ 8.80348054 10.72746587 19.4094123 ], cur_train_grp_loss: [0.0943353  0.11958276 0.2056101 ], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9591, max_kl_dist_index: 2, max_train_grp_loss:  14.9483, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7665, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:27,167 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  93, train_loss:  12.8553, val_loss:  13.2687, grad_norm: 0.3550, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9421, 0.4851, 0.9591, param: [ 5.04189894  6.80736371  6.54520199 13.75747411], weights: [2.21563936e-05 1.55611498e-04 9.99822232e-01], train_wt_loss:  38.5660, val_wt_loss: 39.8061, train_grp_loss: [11.79188269 14.94868274 10.28050448], val_grp_loss: [12.69206155 14.76698755 12.34881404], train_hist_grp_loss: [ 8.89781572 10.84705212 19.61502239], cur_train_grp_loss: [0.09433518 0.11958626 0.20561009], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9591, max_kl_dist_index: 2, max_train_grp_loss:  14.9487, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7670, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:28,155 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  94, train_loss:  12.8555, val_loss:  13.2689, grad_norm: 0.3551, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9421, 0.4851, 0.9592, param: [ 5.04182734  6.80704506  6.54521846 13.75823371], weights: [1.98234630e-05 1.42787432e-04 9.99837389e-01], train_wt_loss:  38.5664, val_wt_loss: 39.8066, train_grp_loss: [11.79186824 14.94905087 10.28050434], val_grp_loss: [12.69200901 14.76746105 12.34885195], train_hist_grp_loss: [ 8.99215078 10.96664158 19.82063248], cur_train_grp_loss: [0.09433506 0.11958946 0.20561009], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9592, max_kl_dist_index: 2, max_train_grp_loss:  14.9491, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7675, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:29,134 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  95, train_loss:  12.8556, val_loss:  13.2690, grad_norm: 0.3551, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9421, 0.4851, 0.9592, param: [ 5.04176169  6.80675432  6.5452328  13.75892978], weights: [1.77361504e-05 1.31020423e-04 9.99851243e-01], train_wt_loss:  38.5668, val_wt_loss: 39.8070, train_grp_loss: [11.79185415 14.94938894 10.28050423], val_grp_loss: [12.69196006 14.76789561 12.34888635], train_hist_grp_loss: [ 9.08648573 11.08623399 20.02624257], cur_train_grp_loss: [0.09433495 0.11959241 0.20561009], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9592, max_kl_dist_index: 2, max_train_grp_loss:  14.9494, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7679, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:30,125 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  96, train_loss:  12.8557, val_loss:  13.2691, grad_norm: 0.3551, live_grad: 0.0000, reward_err: 0.0107, 0.0312, 0.0026, KL_dist: 0.9422, 0.4852, 0.9592, param: [ 5.04170149  6.80648903  6.54524527 13.75956763], weights: [1.58686008e-05 1.20223307e-04 9.99863908e-01], train_wt_loss:  38.5672, val_wt_loss: 39.8074, train_grp_loss: [11.79184045 14.9496994  10.28050413], val_grp_loss: [12.69191447 14.76829443 12.34891755], train_hist_grp_loss: [ 9.18082056 11.2058291  20.23185265], cur_train_grp_loss: [0.09433483 0.11959511 0.20561008], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.9592, max_kl_dist_index: 2, max_train_grp_loss:  14.9497, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7683, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:31,107 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  97, train_loss:  12.8558, val_loss:  13.2692, grad_norm: 0.3551, live_grad: 0.0000, reward_err: 0.0107, 0.0316, 0.0026, KL_dist: 0.9422, 0.4852, 0.9593, param: [ 5.04164629  6.80624693  6.54525609 13.76015215], weights: [1.41976802e-05 1.10316113e-04 9.99875486e-01], train_wt_loss:  38.5675, val_wt_loss: 39.8077, train_grp_loss: [11.79182719 14.94998449 10.28050405], val_grp_loss: [12.69187203 14.76866044 12.34894585], train_hist_grp_loss: [ 9.27515529 11.3254267  20.43746274], cur_train_grp_loss: [0.09433472 0.1195976  0.20561008], max_reward_err:  0.0316, max_reward_err_index: 1, max_kl_dist:  0.9593, max_kl_dist_index: 2, max_train_grp_loss:  14.9500, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7687, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:32,094 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  98, train_loss:  12.8559, val_loss:  13.2694, grad_norm: 0.3552, live_grad: 0.0000, reward_err: 0.0107, 0.0316, 0.0026, KL_dist: 0.9422, 0.4852, 0.9593, param: [ 5.04159567  6.80602598  6.54526547 13.76068781], weights: [1.27026891e-05 1.01225467e-04 9.99886072e-01], train_wt_loss:  38.5678, val_wt_loss: 39.8081, train_grp_loss: [11.79181442 14.95024628 10.28050398], val_grp_loss: [12.69183255 14.76899634 12.34897154], train_hist_grp_loss: [ 9.3694899  11.44502657 20.64307282], cur_train_grp_loss: [0.09433462 0.11959988 0.20561008], max_reward_err:  0.0316, max_reward_err_index: 1, max_kl_dist:  0.9593, max_kl_dist_index: 2, max_train_grp_loss:  14.9502, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7690, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:33,126 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  99, train_loss:  12.8560, val_loss:  13.2695, grad_norm: 0.3552, live_grad: 0.0000, reward_err: 0.0107, 0.0316, 0.0026, KL_dist: 0.9422, 0.4852, 0.9593, param: [ 5.04154925  6.80582432  6.54527359 13.76117869], weights: [1.13651065e-05 9.28840507e-05 9.99895751e-01], train_wt_loss:  38.5681, val_wt_loss: 39.8084, train_grp_loss: [11.79180214 14.95048666 10.28050392], val_grp_loss: [12.69179584 14.7693046  12.34899485], train_hist_grp_loss: [ 9.46382442 11.56462854 20.8486829 ], cur_train_grp_loss: [0.09433452 0.11960197 0.20561008], max_reward_err:  0.0316, max_reward_err_index: 1, max_kl_dist:  0.9593, max_kl_dist_index: 2, max_train_grp_loss:  14.9505, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7693, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:34,033 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1654] - INFO: Iteration:  99, train_loss:  12.8560, val_loss:  13.2695, grad_norm: 0.3552,  live_grad: 0.0000, reward_err: 0.0107, 0.0316, 0.0026, KL_dist: 0.9422, 0.4852, 0.9593, param: [ 5.04154925  6.80582432  6.54527359 13.76117869], weights: [1.13651065e-05 9.28840507e-05 9.99895751e-01], train_wt_loss:  38.5681, val_wt_loss: 39.8084, train_grp_loss: [11.79180214 14.95048666 10.28050392], val_grp_loss: [12.69179584 14.7693046  12.34899485], train_hist_grp_loss: [ 9.46382442 11.56462854 20.8486829 ], cur_train_grp_loss: [0.09433452 0.11960197 0.20561008], max_reward_err:  0.0316, max_reward_err_index: 1, max_kl_dist:  0.9593, max_kl_dist_index: 2, max_train_grp_loss:  14.9505, max_train_grp_loss_index: 1, max_val_grp_loss:  14.7693, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2056, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:33:34,260 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:388] - INFO: Policy parameter learned solely on the preference data rdpo: [ 5.04154925  6.80582432  6.54527359 13.76117869].
2024-10-07 17:33:34,597 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Uniform reward: 3.8318, 3.8318, 3.1647
2024-10-07 17:33:34,598 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Optimal reward: 3.8678, 7.2446, 3.2763
2024-10-07 17:33:34,598 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Policy reward: 3.8263, 7.0156, 3.2679
2024-10-07 17:33:34,599 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:402] - INFO: Reward Error: 0.0107, 0.0316, 0.0026
2024-10-07 17:33:35,284 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:417] - INFO: Optimal reward: 3.8678, 7.2446, 3.2763
Known param reward: [[3.867779541015625, 6.77779443359375, 3.255072021484375], [3.441058349609375, 7.24455908203125, 3.05710986328125], [3.841189453125, 7.1339345703125, 3.27631982421875]], Known param reward error: [[0.0, 0.06442968345654335, 0.0064852651372159075], [0.11032717529039895, 0.0, 0.06690737556116683], [0.006874768225192712, 0.015270013049259644, 0.0]].
