2024-10-07 17:19:12,513 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log-hyperparam-search/2024_10_07_17_04_32/rdpo,0.1,0.1,1,2023
2024-10-07 17:19:12,515 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2023
2024-10-07 17:19:12,516 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-10-07 17:19:12,605 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.2729, l2 distance: 29.4548, acc: 0.92.
2024-10-07 17:19:12,606 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-10-07 17:19:12,606 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [ 6.99981245 12.85202501  8.22225193 12.39503134]
2024-10-07 17:19:12,814 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.8503, 7.1054, 3.3198
2024-10-07 17:19:13,044 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1314] - INFO: unique_group_ids: {0, 1, 2}
2024-10-07 17:19:14,350 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  0, train_loss:  12.6263, val_loss:  12.2213, grad_norm: 0.4066, live_grad: 0.0000, reward_err: 0.0092, 0.0189, 0.0010, KL_dist: 0.6793, 0.3255, 0.6811, param: [ 4.86984388  6.96415822  5.68975979 10.30138106], weights: [0.30678156 0.31019421 0.38302423], train_wt_loss:  37.8789, val_wt_loss: 36.6638, train_grp_loss: [11.42332967 13.78302986 12.99372611], val_grp_loss: [12.57498377 12.87149034 11.21534873], train_hist_grp_loss: [0.1342519  0.14531453 0.3562142 ], cur_train_grp_loss: [0.1342519  0.14531453 0.3562142 ], max_reward_err:  0.0189, max_reward_err_index: 1, max_kl_dist:  0.6811, max_kl_dist_index: 2, max_train_grp_loss:  13.7830, max_train_grp_loss_index: 1, max_val_grp_loss:  12.8715, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.3562, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:15,413 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  1, train_loss:  12.6265, val_loss:  12.2256, grad_norm: 0.0024, live_grad: 0.0000, reward_err: 0.0095, 0.0189, 0.0011, KL_dist: 0.6791, 0.3255, 0.6802, param: [ 4.85856705  6.92260117  5.71822439 10.31137653], weights: [0.28448712 0.29580564 0.41970725], train_wt_loss:  37.8795, val_wt_loss: 36.6769, train_grp_loss: [11.44165721 13.76346244 12.99379072], val_grp_loss: [12.59424599 12.85583058 11.22494335], train_hist_grp_loss: [0.22212367 0.26113831 0.61099314], cur_train_grp_loss: [0.08787177 0.11582378 0.25477894], max_reward_err:  0.0189, max_reward_err_index: 1, max_kl_dist:  0.6802, max_kl_dist_index: 2, max_train_grp_loss:  13.7635, max_train_grp_loss_index: 1, max_val_grp_loss:  12.8558, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2548, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:16,484 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  2, train_loss:  12.6269, val_loss:  12.2306, grad_norm: 0.0037, live_grad: 0.0000, reward_err: 0.0095, 0.0189, 0.0011, KL_dist: 0.6796, 0.3258, 0.6803, param: [ 4.84295833  6.8841225   5.74502291 10.33503432], weights: [0.26233058 0.28041389 0.45725553], train_wt_loss:  37.8808, val_wt_loss: 36.6919, train_grp_loss: [11.45207245 13.75416735 12.99149404], val_grp_loss: [12.60625113 12.85264316 11.23113449], train_hist_grp_loss: [0.31013642 0.37679766 0.86577335], cur_train_grp_loss: [0.08801275 0.11565935 0.25478021], max_reward_err:  0.0189, max_reward_err_index: 1, max_kl_dist:  0.6803, max_kl_dist_index: 2, max_train_grp_loss:  13.7542, max_train_grp_loss_index: 1, max_val_grp_loss:  12.8526, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2548, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:17,552 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  3, train_loss:  12.6276, val_loss:  12.2363, grad_norm: 0.0060, live_grad: 0.0000, reward_err: 0.0102, 0.0189, 0.0014, KL_dist: 0.6806, 0.3266, 0.6814, param: [ 4.82278154  6.84847161  5.76999024 10.37330216], weights: [0.24050919 0.26425325 0.49523756], train_wt_loss:  37.8829, val_wt_loss: 36.7089, train_grp_loss: [11.45415236 13.75568555 12.98667677], val_grp_loss: [12.61058494 12.86266869 11.23374807], train_hist_grp_loss: [0.39822928 0.4923789  1.12050853], cur_train_grp_loss: [0.08809286 0.11558124 0.25473518], max_reward_err:  0.0189, max_reward_err_index: 1, max_kl_dist:  0.6814, max_kl_dist_index: 2, max_train_grp_loss:  13.7557, max_train_grp_loss_index: 1, max_val_grp_loss:  12.8627, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2547, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:18,645 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  4, train_loss:  12.6286, val_loss:  12.2427, grad_norm: 0.0088, live_grad: 0.0000, reward_err: 0.0099, 0.0193, 0.0016, KL_dist: 0.6823, 0.3278, 0.6834, param: [ 4.7978042   6.81529067  5.79293314 10.42706935], weights: [0.21922239 0.24757695 0.53320065], train_wt_loss:  37.8858, val_wt_loss: 36.7282, train_grp_loss: [11.44756752 13.76857931 12.97921656], val_grp_loss: [12.60691565 12.88667552 11.23269763], train_hist_grp_loss: [0.48633814 0.60797289 1.37514925], cur_train_grp_loss: [0.08810886 0.115594   0.25464072], max_reward_err:  0.0193, max_reward_err_index: 1, max_kl_dist:  0.6834, max_kl_dist_index: 2, max_train_grp_loss:  13.7686, max_train_grp_loss_index: 1, max_val_grp_loss:  12.8867, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2546, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:19,728 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  5, train_loss:  12.6300, val_loss:  12.2502, grad_norm: 0.0121, live_grad: 0.0000, reward_err: 0.0102, 0.0193, 0.0017, KL_dist: 0.6847, 0.3295, 0.6865, param: [ 4.76780882  6.7841072   5.81363742 10.49712601], weights: [0.19866172 0.23064561 0.57069267], train_wt_loss:  37.8901, val_wt_loss: 36.7505, train_grp_loss: [11.43211313 13.7934142  12.96903812], val_grp_loss: [12.5950231  12.92543625 11.22800228], train_hist_grp_loss: [0.57439636 0.72367524 1.62964369], cur_train_grp_loss: [0.08805821 0.11570235 0.25449444], max_reward_err:  0.0193, max_reward_err_index: 1, max_kl_dist:  0.6865, max_kl_dist_index: 2, max_train_grp_loss:  13.7934, max_train_grp_loss_index: 1, max_val_grp_loss:  12.9254, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2545, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:20,809 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  6, train_loss:  12.6321, val_loss:  12.2588, grad_norm: 0.0159, live_grad: 0.0000, reward_err: 0.0100, 0.0193, 0.0020, KL_dist: 0.6878, 0.3317, 0.6906, param: [ 4.73260716  6.75433511  5.83187851 10.58411821], weights: [0.1790013  0.21371507 0.60728363], train_wt_loss:  37.8963, val_wt_loss: 36.7765, train_grp_loss: [11.40773974 13.83073717 12.95612268], val_grp_loss: [12.57482755 12.97969857 11.21980172], train_hist_grp_loss: [0.66233569 0.83958628 1.88393856], cur_train_grp_loss: [0.08793933 0.11591104 0.25429487], max_reward_err:  0.0193, max_reward_err_index: 1, max_kl_dist:  0.6906, max_kl_dist_index: 2, max_train_grp_loss:  13.8307, max_train_grp_loss_index: 1, max_val_grp_loss:  12.9797, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2543, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:21,788 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  7, train_loss:  12.6350, val_loss:  12.2691, grad_norm: 0.0206, live_grad: 0.0000, reward_err: 0.0099, 0.0209, 0.0022, KL_dist: 0.6915, 0.3344, 0.6957, param: [ 4.69205719  6.72528641  5.84743501 10.68850083], weights: [0.16038977 0.19702494 0.64258529], train_wt_loss:  37.9051, val_wt_loss: 36.8073, train_grp_loss: [11.37458082 13.88105086 12.9405162 ], val_grp_loss: [12.54641585 13.05015056 11.208366  ], train_hist_grp_loss: [0.75008753 0.95581097 2.13798018], cur_train_grp_loss: [0.08775184 0.11622468 0.25404162], max_reward_err:  0.0209, max_reward_err_index: 1, max_kl_dist:  0.6957, max_kl_dist_index: 2, max_train_grp_loss:  13.8811, max_train_grp_loss_index: 1, max_val_grp_loss:  13.0502, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2540, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:22,812 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  8, train_loss:  12.6392, val_loss:  12.2814, grad_norm: 0.0263, live_grad: 0.0000, reward_err: 0.0099, 0.0217, 0.0022, KL_dist: 0.6960, 0.3376, 0.7018, param: [ 4.64608206  6.69619495  5.86010451 10.81049074], weights: [0.14294443 0.18078891 0.67626665], train_wt_loss:  37.9176, val_wt_loss: 36.8441, train_grp_loss: [11.33297393 13.94478424 12.92233529], val_grp_loss: [12.5100618  13.13738068 11.19409805], train_hist_grp_loss: [0.83758431 1.07245845 2.39171579], cur_train_grp_loss: [0.08749678 0.11664749 0.25373561], max_reward_err:  0.0217, max_reward_err_index: 1, max_kl_dist:  0.7018, max_kl_dist_index: 2, max_train_grp_loss:  13.9448, max_train_grp_loss_index: 1, max_val_grp_loss:  13.1374, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2537, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:23,799 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  9, train_loss:  12.6450, val_loss:  12.2961, grad_norm: 0.0334, live_grad: 0.0000, reward_err: 0.0095, 0.0229, 0.0023, KL_dist: 0.7011, 0.3415, 0.7090, param: [ 4.59468975  6.66625197  5.86972023 10.9500232 ], weights: [0.12674804 0.16518757 0.70806439], train_wt_loss:  37.9349, val_wt_loss: 36.8884, train_grp_loss: [11.28347256 14.02226011 12.90176997], val_grp_loss: [12.46623784 13.24183334 11.17752721], train_hist_grp_loss: [0.92476103 1.18964151 2.64509491], cur_train_grp_loss: [0.08717672 0.11718306 0.25337912], max_reward_err:  0.0229, max_reward_err_index: 1, max_kl_dist:  0.7090, max_kl_dist_index: 2, max_train_grp_loss:  14.0223, max_train_grp_loss_index: 1, max_val_grp_loss:  13.2418, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2534, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:24,786 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  10, train_loss:  12.6528, val_loss:  12.3139, grad_norm: 0.0420, live_grad: 0.0000, reward_err: 0.0097, 0.0242, 0.0024, KL_dist: 0.7068, 0.3459, 0.7171, param: [ 4.53799131  6.6346513   5.87616695 11.106715  ], weights: [0.11184815 0.15036427 0.73778758], train_wt_loss:  37.9585, val_wt_loss: 36.9418, train_grp_loss: [11.22684552 14.11366037 12.87908251], val_grp_loss: [12.41561547 13.3637617  11.15929292], train_hist_grp_loss: [1.01155697 1.30747563 2.89807079], cur_train_grp_loss: [0.08679594 0.11783412 0.25297588], max_reward_err:  0.0242, max_reward_err_index: 1, max_kl_dist:  0.7171, max_kl_dist_index: 2, max_train_grp_loss:  14.1137, max_train_grp_loss_index: 1, max_val_grp_loss:  13.3638, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2530, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:25,777 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  11, train_loss:  12.6632, val_loss:  12.3353, grad_norm: 0.0523, live_grad: 0.0000, reward_err: 0.0092, 0.0250, 0.0026, KL_dist: 0.7132, 0.3510, 0.7262, param: [ 4.47621554  6.60064059  5.87939432 11.27983797], weights: [0.09825891 0.13642383 0.76531725], train_wt_loss:  37.9897, val_wt_loss: 37.0059, train_grp_loss: [11.16406215 14.21899056 12.85460176], val_grp_loss: [12.35905259 13.50318    11.14011862], train_hist_grp_loss: [1.09791732 1.42607782 3.15060182], cur_train_grp_loss: [0.08636035 0.11860219 0.25253103], max_reward_err:  0.0250, max_reward_err_index: 1, max_kl_dist:  0.7262, max_kl_dist_index: 2, max_train_grp_loss:  14.2190, max_train_grp_loss_index: 1, max_val_grp_loss:  13.5032, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2525, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:26,794 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  12, train_loss:  12.6767, val_loss:  12.3607, grad_norm: 0.0645, live_grad: 0.0000, reward_err: 0.0089, 0.0263, 0.0026, KL_dist: 0.7202, 0.3568, 0.7362, param: [ 4.40971762  6.56357314  5.8794259  11.46830678], weights: [0.08596462 0.12343396 0.79060142], train_wt_loss:  38.0301, val_wt_loss: 37.0821, train_grp_loss: [11.0962625  14.33804615 12.82871281], val_grp_loss: [12.29756716 13.65981915 11.12077721], train_hist_grp_loss: [1.18379472 1.54556514 3.40265284], cur_train_grp_loss: [0.0858774  0.11948732 0.25205101], max_reward_err:  0.0263, max_reward_err_index: 1, max_kl_dist:  0.7362, max_kl_dist_index: 2, max_train_grp_loss:  14.3380, max_train_grp_loss_index: 1, max_val_grp_loss:  13.6598, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2521, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:27,864 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  13, train_loss:  12.6936, val_loss:  12.3906, grad_norm: 0.0784, live_grad: 0.0000, reward_err: 0.0088, 0.0275, 0.0026, KL_dist: 0.7279, 0.3633, 0.7471, param: [ 4.33897966  6.5229545   5.8763627  11.67068447], weights: [0.0749245  0.11142852 0.81364698], train_wt_loss:  38.0808, val_wt_loss: 37.1718, train_grp_loss: [11.02471364 14.47038363 12.80184228], val_grp_loss: [12.23229824 13.83309016 11.10205049], train_hist_grp_loss: [1.26915059 1.66605292 3.65419623], cur_train_grp_loss: [0.08535587 0.12048778 0.25154339], max_reward_err:  0.0275, max_reward_err_index: 1, max_kl_dist:  0.7471, max_kl_dist_index: 2, max_train_grp_loss:  14.4704, max_train_grp_loss_index: 1, max_val_grp_loss:  13.8331, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2515, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:28,858 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  14, train_loss:  12.7144, val_loss:  12.4253, grad_norm: 0.0941, live_grad: 0.0000, reward_err: 0.0078, 0.0298, 0.0029, KL_dist: 0.7360, 0.3705, 0.7587, param: [ 4.2646021   6.47847809  5.87038083 11.88520797], weights: [0.06507807 0.10041227 0.83450966], train_wt_loss:  38.1432, val_wt_loss: 37.2760, train_grp_loss: [10.95075481 14.61530036 12.77443994], val_grp_loss: [12.16445685 14.02206036 11.08468612], train_hist_grp_loss: [1.35395608 1.78765278 3.90521274], cur_train_grp_loss: [0.08480549 0.12159986 0.25101652], max_reward_err:  0.0298, max_reward_err_index: 1, max_kl_dist:  0.7587, max_kl_dist_index: 2, max_train_grp_loss:  14.6153, max_train_grp_loss_index: 1, max_val_grp_loss:  14.0221, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2510, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:29,859 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  15, train_loss:  12.7393, val_loss:  12.4651, grad_norm: 0.1114, live_grad: 0.0000, reward_err: 0.0079, 0.0312, 0.0035, KL_dist: 0.7447, 0.3785, 0.7710, param: [ 4.1872858   6.43004571  5.8617234  12.10983437], weights: [0.05635056 0.09036618 0.85328326], train_wt_loss:  38.2179, val_wt_loss: 37.3954, train_grp_loss: [10.87573613 14.77182698 12.74695823], val_grp_loss: [12.09527098 14.22544748 11.06935617], train_hist_grp_loss: [1.43819265 1.91047043 4.15569196], cur_train_grp_loss: [0.08423658 0.12281765 0.25047921], max_reward_err:  0.0312, max_reward_err_index: 1, max_kl_dist:  0.7710, max_kl_dist_index: 2, max_train_grp_loss:  14.7718, max_train_grp_loss_index: 1, max_val_grp_loss:  14.2254, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2505, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:30,860 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  16, train_loss:  12.7685, val_loss:  12.5100, grad_norm: 0.1301, live_grad: 0.0000, reward_err: 0.0075, 0.0326, 0.0039, KL_dist: 0.7538, 0.3873, 0.7839, param: [ 4.10780612  6.37777035  5.85068778 12.34230605], weights: [0.04865796 0.08125292 0.87008912], train_wt_loss:  38.3055, val_wt_loss: 37.5300, train_grp_loss: [10.80095649 14.93873558 12.71983133], val_grp_loss: [12.02592967 14.4416351  11.05662136], train_hist_grp_loss: [1.52185216 2.03460343 4.40563231], cur_train_grp_loss: [0.08365951 0.124133   0.24994036], max_reward_err:  0.0326, max_reward_err_index: 1, max_kl_dist:  0.7839, max_kl_dist_index: 2, max_train_grp_loss:  14.9387, max_train_grp_loss_index: 1, max_val_grp_loss:  14.4416, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2499, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:31,846 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  17, train_loss:  12.8020, val_loss:  12.5598, grad_norm: 0.1500, live_grad: 0.0000, reward_err: 0.0074, 0.0349, 0.0040, KL_dist: 0.7633, 0.3968, 0.7972, param: [ 4.02698143  6.32196179  5.83760982 12.58023078], weights: [0.04191143 0.07302202 0.88506655], train_wt_loss:  38.4059, val_wt_loss: 37.6795, train_grp_loss: [10.72760673 15.1145652  12.69345584], val_grp_loss: [11.95753166 14.66871103 11.04690471], train_hist_grp_loss: [1.60493644 2.16013902 4.65504077], cur_train_grp_loss: [0.08308428 0.12553559 0.24940846], max_reward_err:  0.0349, max_reward_err_index: 1, max_kl_dist:  0.7972, max_kl_dist_index: 2, max_train_grp_loss:  15.1146, max_train_grp_loss_index: 1, max_val_grp_loss:  14.6687, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2494, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:32,858 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  18, train_loss:  12.8396, val_loss:  12.6143, grad_norm: 0.1709, live_grad: 0.0000, reward_err: 0.0075, 0.0378, 0.0043, KL_dist: 0.7731, 0.4069, 0.8109, param: [ 3.94563891  6.26309705  5.82284665 12.82117022], weights: [0.03602094 0.06561446 0.8983646 ], train_wt_loss:  38.5187, val_wt_loss: 37.8429, train_grp_loss: [10.65672328 15.29766415 12.66817479], val_grp_loss: [11.89104312 14.90452687 11.04047674], train_hist_grp_loss: [1.6874565  2.28715218 4.90393206], cur_train_grp_loss: [0.08252005 0.12701315 0.24889129], max_reward_err:  0.0378, max_reward_err_index: 1, max_kl_dist:  0.8109, max_kl_dist_index: 2, max_train_grp_loss:  15.2977, max_train_grp_loss_index: 1, max_val_grp_loss:  14.9045, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2489, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:33,859 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  19, train_loss:  12.8810, val_loss:  12.6730, grad_norm: 0.1924, live_grad: 0.0000, reward_err: 0.0075, 0.0401, 0.0045, KL_dist: 0.7831, 0.4177, 0.8247, param: [ 3.86458117  6.2017804   5.80676003 13.0627295 ], weights: [0.03089817 0.05896651 0.91013532], train_wt_loss:  38.6431, val_wt_loss: 38.0189, train_grp_loss: [10.58915621 15.48624636 12.64426652], val_grp_loss: [11.82726807 15.14677461 11.0374533 ], train_hist_grp_loss: [1.76943129 2.41570398 5.15232765], cur_train_grp_loss: [0.08197479 0.1285518  0.24839558], max_reward_err:  0.0401, max_reward_err_index: 1, max_kl_dist:  0.8247, max_kl_dist_index: 2, max_train_grp_loss:  15.4862, max_train_grp_loss_index: 1, max_val_grp_loss:  15.1468, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2484, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:34,864 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  20, train_loss:  12.9259, val_loss:  12.7352, grad_norm: 0.2143, live_grad: 0.0000, reward_err: 0.0077, 0.0421, 0.0056, KL_dist: 0.7932, 0.4289, 0.8387, param: [ 3.78455654  6.13869774  5.78970148 13.30264032], weights: [0.02645857 0.05301284 0.9205286 ], train_wt_loss:  38.7778, val_wt_loss: 38.2057, train_grp_loss: [10.5255532  15.67845663 12.62193887], val_grp_loss: [11.76683263 15.39307323 11.0378049 ], train_hist_grp_loss: [1.85088634 2.5458405  5.40025444], cur_train_grp_loss: [0.08145505 0.13013652 0.24792679], max_reward_err:  0.0421, max_reward_err_index: 1, max_kl_dist:  0.8387, max_kl_dist_index: 2, max_train_grp_loss:  15.6785, max_train_grp_loss_index: 1, max_val_grp_loss:  15.3931, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2479, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:35,860 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  21, train_loss:  12.9737, val_loss:  12.8004, grad_norm: 0.2363, live_grad: 0.0000, reward_err: 0.0074, 0.0445, 0.0058, KL_dist: 0.8033, 0.4406, 0.8525, param: [ 3.70623531  6.07457079  5.7720002  13.53883106], weights: [0.0226228  0.04768888 0.92968833], train_wt_loss:  38.9211, val_wt_loss: 38.4011, train_grp_loss: [10.46635898 15.87243906 12.60132884], val_grp_loss: [11.71018304 15.64105743 11.04137562], train_hist_grp_loss: [1.93185213 2.67759224 5.64774344], cur_train_grp_loss: [0.08096579 0.13175174 0.247489  ], max_reward_err:  0.0445, max_reward_err_index: 1, max_kl_dist:  0.8525, max_kl_dist_index: 2, max_train_grp_loss:  15.8724, max_train_grp_loss_index: 1, max_val_grp_loss:  15.6411, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2475, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:36,853 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  22, train_loss:  13.0238, val_loss:  12.8677, grad_norm: 0.2582, live_grad: 0.0000, reward_err: 0.0076, 0.0476, 0.0064, KL_dist: 0.8133, 0.4526, 0.8662, param: [ 3.6301932   6.01011522  5.75395441 13.76947943], weights: [0.01931762 0.0429325  0.93774989], train_wt_loss:  39.0715, val_wt_loss: 38.6030, train_grp_loss: [10.41182794 16.06640231 12.58250688], val_grp_loss: [11.65759531 15.88846064 11.0479082 ], train_hist_grp_loss: [2.01236259 2.81097408 5.89482832], cur_train_grp_loss: [0.08051045 0.13338184 0.24708488], max_reward_err:  0.0476, max_reward_err_index: 1, max_kl_dist:  0.8662, max_kl_dist_index: 2, max_train_grp_loss:  16.0664, max_train_grp_loss_index: 1, max_val_grp_loss:  15.8885, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2471, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:37,848 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  23, train_loss:  13.0756, val_loss:  12.9364, grad_norm: 0.2797, live_grad: 0.0000, reward_err: 0.0075, 0.0511, 0.0070, KL_dist: 0.8233, 0.4647, 0.8797, param: [ 3.55690239  5.94600621  5.73582581 13.99304589], weights: [0.01647631 0.03868515 0.94483854], train_wt_loss:  39.2269, val_wt_loss: 38.8091, train_grp_loss: [10.36204646 16.25867635 12.56548461], val_grp_loss: [11.60919365 16.13318576 11.05707222], train_hist_grp_loss: [2.09245357 2.94598586 6.14154414], cur_train_grp_loss: [0.08009098 0.13501178 0.24671582], max_reward_err:  0.0511, max_reward_err_index: 1, max_kl_dist:  0.8797, max_kl_dist_index: 2, max_train_grp_loss:  16.2587, max_train_grp_loss_index: 1, max_val_grp_loss:  16.1332, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2467, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:38,857 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  24, train_loss:  13.1285, val_loss:  13.0056, grad_norm: 0.3007, live_grad: 0.0000, reward_err: 0.0075, 0.0534, 0.0070, KL_dist: 0.8330, 0.4769, 0.8928, param: [ 3.48672934  5.88285257  5.71783723 14.20828786], weights: [0.01403875 0.03489256 0.95106869], train_wt_loss:  39.3855, val_wt_loss: 39.0169, train_grp_loss: [10.31696109 16.44775714 12.55022486], val_grp_loss: [11.56497427 16.37335937 11.06849215], train_hist_grp_loss: [2.17216162 3.08261339 6.38792619], cur_train_grp_loss: [0.07970805 0.13662753 0.24638205], max_reward_err:  0.0534, max_reward_err_index: 1, max_kl_dist:  0.8928, max_kl_dist_index: 2, max_train_grp_loss:  16.4478, max_train_grp_loss_index: 1, max_val_grp_loss:  16.3734, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2464, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:39,859 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  25, train_loss:  13.1818, val_loss:  13.0749, grad_norm: 0.3210, live_grad: 0.0000, reward_err: 0.0074, 0.0543, 0.0070, KL_dist: 0.8424, 0.4891, 0.9054, param: [ 3.4199386   5.82118025  5.70017256 14.41425711], weights: [0.0119513  0.03150503 0.95654367], train_wt_loss:  39.5455, val_wt_loss: 39.2246, train_grp_loss: [10.27640903 16.63233719 12.53665245], val_grp_loss: [11.52483152 16.6073674  11.08177282], train_hist_grp_loss: [2.25152286 3.22082984 6.63400903], cur_train_grp_loss: [0.07936124 0.13821645 0.24608284], max_reward_err:  0.0543, max_reward_err_index: 1, max_kl_dist:  0.9054, max_kl_dist_index: 2, max_train_grp_loss:  16.6323, max_train_grp_loss_index: 1, max_val_grp_loss:  16.6074, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2461, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:40,860 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  26, train_loss:  13.2351, val_loss:  13.1434, grad_norm: 0.3405, live_grad: 0.0000, reward_err: 0.0074, 0.0555, 0.0075, KL_dist: 0.8515, 0.5012, 0.9176, param: [ 3.35670077  5.76142402  5.6829787  14.61028359], weights: [0.01016643 0.02847756 0.96135601], train_wt_loss:  39.7052, val_wt_loss: 39.4301, train_grp_loss: [10.24014781 16.81132203 12.52466488], val_grp_loss: [11.48858375 16.83387277 11.09652093], train_hist_grp_loss: [2.33057216 3.36059738 6.87982575], cur_train_grp_loss: [0.0790493  0.13976754 0.24581671], max_reward_err:  0.0555, max_reward_err_index: 1, max_kl_dist:  0.9176, max_kl_dist_index: 2, max_train_grp_loss:  16.8113, max_train_grp_loss_index: 1, max_val_grp_loss:  16.8339, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2458, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:41,840 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  27, train_loss:  13.2877, val_loss:  13.2106, grad_norm: 0.3591, live_grad: 0.0000, reward_err: 0.0074, 0.0574, 0.0075, KL_dist: 0.8602, 0.5130, 0.9292, param: [ 3.29710339  5.7039263   5.6663686  14.79594963], weights: [0.00864233 0.02576969 0.96558798], train_wt_loss:  39.8632, val_wt_loss: 39.6317, train_grp_loss: [10.20788231 16.98383392 12.51414201], val_grp_loss: [11.45599693 17.05181683 11.11236161], train_hist_grp_loss: [2.40934253 3.50186899 7.12540741], cur_train_grp_loss: [0.07877037 0.14127161 0.24558166], max_reward_err:  0.0574, max_reward_err_index: 1, max_kl_dist:  0.9292, max_kl_dist_index: 2, max_train_grp_loss:  16.9838, max_train_grp_loss_index: 1, max_val_grp_loss:  17.0518, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2456, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:42,821 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  28, train_loss:  13.3394, val_loss:  13.2760, grad_norm: 0.3767, live_grad: 0.0000, reward_err: 0.0077, 0.0574, 0.0082, KL_dist: 0.8686, 0.5245, 0.9402, param: [ 3.24116327  5.64894118  5.65042517 14.97105821], weights: [0.00734242 0.02334527 0.96931231], train_wt_loss:  40.0182, val_wt_loss: 39.8281, train_grp_loss: [10.17928767 17.14920494 12.50495437], val_grp_loss: [11.42680507 17.26040795 11.12895011], train_hist_grp_loss: [2.4878647  3.64459029 7.37078274], cur_train_grp_loss: [0.07852217 0.14272129 0.24537533], max_reward_err:  0.0574, max_reward_err_index: 1, max_kl_dist:  0.9402, max_kl_dist_index: 2, max_train_grp_loss:  17.1492, max_train_grp_loss_index: 1, max_val_grp_loss:  17.2604, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2454, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:43,819 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  29, train_loss:  13.3897, val_loss:  13.3394, grad_norm: 0.3934, live_grad: 0.0000, reward_err: 0.0077, 0.0586, 0.0082, KL_dist: 0.8765, 0.5356, 0.9507, param: [ 3.18883915  5.59664217  5.6352054  15.1355987 ], weights: [0.00623485 0.0211722  0.97259295], train_wt_loss:  40.1690, val_wt_loss: 40.0182, train_grp_loss: [10.15402773 17.30696229 12.49696982], val_grp_loss: [11.40072685 17.45910054 11.14597909], train_hist_grp_loss: [2.56616691 3.78870125 7.61597793], cur_train_grp_loss: [0.07830221 0.14411097 0.24519518], max_reward_err:  0.0586, max_reward_err_index: 1, max_kl_dist:  0.9507, max_kl_dist_index: 2, max_train_grp_loss:  17.3070, max_train_grp_loss_index: 1, max_val_grp_loss:  17.4591, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2452, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:44,817 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  30, train_loss:  13.4383, val_loss:  13.4003, grad_norm: 0.4091, live_grad: 0.0000, reward_err: 0.0079, 0.0595, 0.0084, KL_dist: 0.8840, 0.5463, 0.9606, param: [ 3.14004387  5.54713197  5.62074445 15.28971256], weights: [0.00529199 0.01922203 0.97548598], train_wt_loss:  40.3148, val_wt_loss: 40.2010, train_grp_loss: [10.13176906 17.45680829 12.49005856], val_grp_loss: [11.37747849 17.64756808 11.16318208], train_hist_grp_loss: [2.64427482 3.93413791 7.86101655], cur_train_grp_loss: [0.07810791 0.14543666 0.24503862], max_reward_err:  0.0595, max_reward_err_index: 1, max_kl_dist:  0.9606, max_kl_dist_index: 2, max_train_grp_loss:  17.4568, max_train_grp_loss_index: 1, max_val_grp_loss:  17.6476, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2450, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:45,811 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  31, train_loss:  13.4850, val_loss:  13.4586, grad_norm: 0.4237, live_grad: 0.0000, reward_err: 0.0079, 0.0599, 0.0087, KL_dist: 0.8911, 0.5565, 0.9699, param: [ 3.09465554  5.50045318  5.60705955 15.43366105], weights: [0.00448996 0.01746965 0.97804039], train_wt_loss:  40.4551, val_wt_loss: 40.3759, train_grp_loss: [10.11219098 17.59859767 12.48409677], val_grp_loss: [11.35678323 17.82567309 11.18033404], train_hist_grp_loss: [2.7222115  4.08083378 8.10591966], cur_train_grp_loss: [0.07793669 0.14669587 0.24490311], max_reward_err:  0.0599, max_reward_err_index: 1, max_kl_dist:  0.9699, max_kl_dist_index: 2, max_train_grp_loss:  17.5986, max_train_grp_loss_index: 1, max_val_grp_loss:  17.8257, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2449, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:46,797 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  32, train_loss:  13.5297, val_loss:  13.5142, grad_norm: 0.4375, live_grad: 0.0000, reward_err: 0.0080, 0.0615, 0.0090, KL_dist: 0.8977, 0.5662, 0.9786, param: [ 3.05252727  5.45659905  5.59415354 15.56779591], weights: [0.00380817 0.01589291 0.98029892], train_wt_loss:  40.5892, val_wt_loss: 40.5425, train_grp_loss: [10.0949922  17.73231384 12.47896895], val_grp_loss: [11.33837778 17.99343642 11.19724983], train_hist_grp_loss: [2.79999759 4.22872115 8.35070587], cur_train_grp_loss: [0.07778608 0.14788738 0.24478621], max_reward_err:  0.0615, max_reward_err_index: 1, max_kl_dist:  0.9786, max_kl_dist_index: 2, max_train_grp_loss:  17.7323, max_train_grp_loss_index: 1, max_val_grp_loss:  17.9934, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2448, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:47,787 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  33, train_loss:  13.5723, val_loss:  13.5669, grad_norm: 0.4502, live_grad: 0.0000, reward_err: 0.0079, 0.0620, 0.0093, KL_dist: 0.9039, 0.5755, 0.9867, param: [ 3.01349552  5.4155235   5.58201793 15.69253391], weights: [0.00322894 0.0144723  0.98229876], train_wt_loss:  40.7170, val_wt_loss: 40.7006, train_grp_loss: [10.07989477 17.8580458  12.47456921], val_grp_loss: [11.32201638 18.15100769 11.2137812 ], train_hist_grp_loss: [2.87765137 4.3777322  8.59539154], cur_train_grp_loss: [0.07765379 0.14901104 0.24468567], max_reward_err:  0.0620, max_reward_err_index: 1, max_kl_dist:  0.9867, max_kl_dist_index: 2, max_train_grp_loss:  17.8580, max_train_grp_loss_index: 1, max_val_grp_loss:  18.1510, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2447, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:48,774 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  34, train_loss:  13.6127, val_loss:  13.6167, grad_norm: 0.4621, live_grad: 0.0000, reward_err: 0.0080, 0.0625, 0.0096, KL_dist: 0.9098, 0.5842, 0.9943, param: [ 2.97738692  5.37715027  5.57063557 15.80833525], weights: [0.0027371  0.01319065 0.98407225], train_wt_loss:  40.8381, val_wt_loss: 40.8500, train_grp_loss: [10.06664584 17.97596668 12.47080191], val_grp_loss: [11.30747306 18.298638   11.2298131 ], train_hist_grp_loss: [2.95518902 4.52779981 8.83999093], cur_train_grp_loss: [0.07753765 0.15006761 0.2445994 ], max_reward_err:  0.0625, max_reward_err_index: 1, max_kl_dist:  0.9943, max_kl_dist_index: 2, max_train_grp_loss:  17.9760, max_train_grp_loss_index: 1, max_val_grp_loss:  18.2986, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2446, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:49,760 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  35, train_loss:  13.6509, val_loss:  13.6636, grad_norm: 0.4732, live_grad: 0.0000, reward_err: 0.0080, 0.0630, 0.0096, KL_dist: 0.9152, 0.5925, 1.0014, param: [ 2.94402376  5.34138079  5.55998288 15.9156858 ], weights: [0.00231963 0.01203285 0.98564752], train_wt_loss:  40.9527, val_wt_loss: 40.9908, train_grp_loss: [10.05501806 18.08631433 12.46758163], val_grp_loss: [11.29454242 18.43665564 11.2452595 ], train_hist_grp_loss: [3.03262476 4.67885835 9.08451646], cur_train_grp_loss: [0.07743574 0.15105854 0.24452553], max_reward_err:  0.0630, max_reward_err_index: 1, max_kl_dist:  1.0014, max_kl_dist_index: 2, max_train_grp_loss:  18.0863, max_train_grp_loss_index: 1, max_val_grp_loss:  18.4367, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2445, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:50,741 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  36, train_loss:  13.6869, val_loss:  13.7077, grad_norm: 0.4834, live_grad: 0.0000, reward_err: 0.0079, 0.0641, 0.0098, KL_dist: 0.9203, 0.6002, 1.0080, param: [ 2.91322823  5.3081009   5.55003171 16.01508288], weights: [0.00196543 0.0109856  0.98704897], train_wt_loss:  41.0607, val_wt_loss: 41.1231, train_grp_loss: [10.04480881 18.18937445 12.4648328 ], val_grp_loss: [11.28303944 18.56544492 11.26005924], train_hist_grp_loss: [3.10997105 4.83084419 9.32897885], cur_train_grp_loss: [0.07734629 0.15198583 0.24446238], max_reward_err:  0.0641, max_reward_err_index: 1, max_kl_dist:  1.0080, max_kl_dist_index: 2, max_train_grp_loss:  18.1894, max_train_grp_loss_index: 1, max_val_grp_loss:  18.5654, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2445, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:51,720 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  37, train_loss:  13.7207, val_loss:  13.7491, grad_norm: 0.4928, live_grad: 0.0000, reward_err: 0.0079, 0.0651, 0.0098, KL_dist: 0.9250, 0.6075, 1.0141, param: [ 2.88482567  5.27718637  5.5407508  16.10702412], weights: [0.00166502 0.01003715 0.98829784], train_wt_loss:  41.1622, val_wt_loss: 41.2472, train_grp_loss: [10.03583879 18.28546619 12.46248907], val_grp_loss: [11.27279868 18.68542848 11.27417206], train_hist_grp_loss: [3.18723881 4.98369607 9.57338733], cur_train_grp_loss: [0.07726776 0.15285189 0.24440849], max_reward_err:  0.0651, max_reward_err_index: 1, max_kl_dist:  1.0141, max_kl_dist_index: 2, max_train_grp_loss:  18.2855, max_train_grp_loss_index: 1, max_val_grp_loss:  18.6854, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2444, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:52,718 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  38, train_loss:  13.7525, val_loss:  13.7878, grad_norm: 0.5016, live_grad: 0.0000, reward_err: 0.0078, 0.0656, 0.0100, KL_dist: 0.9294, 0.6143, 1.0197, param: [ 2.85864687  5.24850737  5.53210699 16.19199906], weights: [0.0014103  0.00917715 0.98941255], train_wt_loss:  41.2574, val_wt_loss: 41.3633, train_grp_loss: [10.02795027 18.3749302  12.46049261], val_grp_loss: [11.26367304 18.79705245 11.28757499], train_hist_grp_loss: [3.26443757 5.13735545 9.81774986], cur_train_grp_loss: [0.07719876 0.15365938 0.24436253], max_reward_err:  0.0656, max_reward_err_index: 1, max_kl_dist:  1.0197, max_kl_dist_index: 2, max_train_grp_loss:  18.3749, max_train_grp_loss_index: 1, max_val_grp_loss:  18.7971, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2444, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:53,715 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  39, train_loss:  13.7822, val_loss:  13.8239, grad_norm: 0.5097, live_grad: 0.0000, reward_err: 0.0077, 0.0672, 0.0102, KL_dist: 0.9334, 0.6207, 1.0250, param: [ 2.83452971  5.22193195  5.52406617 16.27048289], weights: [0.00119437 0.00839647 0.99040916], train_wt_loss:  41.3465, val_wt_loss: 41.4718, train_grp_loss: [10.02100508 18.45811885 12.45879321], val_grp_loss: [11.25553226 18.90077464 11.30025903], train_hist_grp_loss: [ 3.34157565  5.29176663 10.06207325], cur_train_grp_loss: [0.07713808 0.15441118 0.24432338], max_reward_err:  0.0672, max_reward_err_index: 1, max_kl_dist:  1.0250, max_kl_dist_index: 2, max_train_grp_loss:  18.4581, max_train_grp_loss_index: 1, max_val_grp_loss:  18.9008, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2443, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:54,708 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  40, train_loss:  13.8099, val_loss:  13.8577, grad_norm: 0.5171, live_grad: 0.0000, reward_err: 0.0076, 0.0694, 0.0107, KL_dist: 0.9372, 0.6266, 1.0299, param: [ 2.81232021  5.19732873  5.516594   16.34293215], weights: [0.00101138 0.00768701 0.99130161], train_wt_loss:  41.4297, val_wt_loss: 41.5730, train_grp_loss: [10.01488261 18.53538845 12.45734752], val_grp_loss: [11.24826139 18.99705508 11.31222632], train_hist_grp_loss: [ 3.41866031  5.44687687 10.30636331], cur_train_grp_loss: [0.07708465 0.15511024 0.24429006], max_reward_err:  0.0694, max_reward_err_index: 1, max_kl_dist:  1.0299, max_kl_dist_index: 2, max_train_grp_loss:  18.5354, max_train_grp_loss_index: 1, max_val_grp_loss:  18.9971, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2443, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:55,748 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  41, train_loss:  13.8358, val_loss:  13.8891, grad_norm: 0.5240, live_grad: 0.0000, reward_err: 0.0076, 0.0700, 0.0110, KL_dist: 0.9407, 0.6321, 1.0344, param: [ 2.79187315  5.17456896  5.50965649 16.40978178], weights: [8.56328532e-04 7.04161393e-03 9.92102058e-01], train_wt_loss:  41.5074, val_wt_loss: 41.6672, train_grp_loss: [10.00947785 18.60709318 12.45611821], val_grp_loss: [11.2417592  19.08634869 11.32348771], train_hist_grp_loss: [ 3.49569787  5.60263644 10.55062503], cur_train_grp_loss: [0.07703756 0.15575957 0.24426172], max_reward_err:  0.0700, max_reward_err_index: 1, max_kl_dist:  1.0344, max_kl_dist_index: 2, max_train_grp_loss:  18.6071, max_train_grp_loss_index: 1, max_val_grp_loss:  19.0863, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2443, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:56,767 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  42, train_loss:  13.8599, val_loss:  13.9183, grad_norm: 0.5304, live_grad: 0.0000, reward_err: 0.0076, 0.0715, 0.0110, KL_dist: 0.9439, 0.6373, 1.0385, param: [ 2.77305234  5.15352795  5.50322035 16.4714434 ], weights: [7.24974869e-04 6.45391180e-03 9.92821113e-01], train_wt_loss:  41.5798, val_wt_loss: 41.7549, train_grp_loss: [10.00469952 18.67358039 12.45507326], val_grp_loss: [11.23593673 19.16909983 11.33406069], train_hist_grp_loss: [ 3.57269385  5.75899857 10.79486264], cur_train_grp_loss: [0.07699598 0.15636213 0.24423761], max_reward_err:  0.0715, max_reward_err_index: 1, max_kl_dist:  1.0385, max_kl_dist_index: 2, max_train_grp_loss:  18.6736, max_train_grp_loss_index: 1, max_val_grp_loss:  19.1691, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2442, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:57,759 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  43, train_loss:  13.8824, val_loss:  13.9454, grad_norm: 0.5362, live_grad: 0.0000, reward_err: 0.0076, 0.0726, 0.0112, KL_dist: 0.9469, 0.6420, 1.0424, param: [ 2.75573061  5.13408608  5.49725337 16.52830445], weights: [6.13714894e-04 5.91825435e-03 9.93468031e-01], train_wt_loss:  41.6471, val_wt_loss: 41.8363, train_grp_loss: [10.00046839 18.73518724 12.45418524], val_grp_loss: [11.23071585 19.24573826 11.34396767], train_hist_grp_loss: [ 3.64965308  5.91591941 11.03907976], cur_train_grp_loss: [0.07695923 0.15692084 0.24421712], max_reward_err:  0.0726, max_reward_err_index: 1, max_kl_dist:  1.0424, max_kl_dist_index: 2, max_train_grp_loss:  18.7352, max_train_grp_loss_index: 1, max_val_grp_loss:  19.2457, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2442, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:58,745 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  44, train_loss:  13.9032, val_loss:  13.9707, grad_norm: 0.5416, live_grad: 0.0000, reward_err: 0.0076, 0.0731, 0.0112, KL_dist: 0.9497, 0.6465, 1.0459, param: [ 2.7397897   5.11612948  5.49172459 16.580728  ], weights: [5.19488140e-04 5.42960780e-03 9.94050904e-01], train_wt_loss:  41.7097, val_wt_loss: 41.9120, train_grp_loss: [ 9.99671574 18.79223818 12.45343071], val_grp_loss: [11.22602799 19.31667629 11.35323455], train_hist_grp_loss: [ 3.72657976  6.07335796 11.28327947], cur_train_grp_loss: [0.07692668 0.15743855 0.24419971], max_reward_err:  0.0731, max_reward_err_index: 1, max_kl_dist:  1.0459, max_kl_dist_index: 2, max_train_grp_loss:  18.7922, max_train_grp_loss_index: 1, max_val_grp_loss:  19.3167, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2442, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:19:59,740 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  45, train_loss:  13.9226, val_loss:  13.9941, grad_norm: 0.5466, live_grad: 0.0000, reward_err: 0.0077, 0.0736, 0.0116, KL_dist: 0.9522, 0.6506, 1.0492, param: [ 2.7251199   5.09955041  5.48660441 16.6290531 ], weights: [4.39696978e-04 4.98348218e-03 9.94576821e-01], train_wt_loss:  41.7679, val_wt_loss: 41.9822, train_grp_loss: [ 9.99338197 18.84504331 12.45278969], val_grp_loss: [11.22181297 19.38230699 11.36188961], train_hist_grp_loss: [ 3.80347757  6.23127593 11.52746439], cur_train_grp_loss: [0.07689781 0.15791797 0.24418492], max_reward_err:  0.0736, max_reward_err_index: 1, max_kl_dist:  1.0492, max_kl_dist_index: 2, max_train_grp_loss:  18.8450, max_train_grp_loss_index: 1, max_val_grp_loss:  19.3823, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2442, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:00,756 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  46, train_loss:  13.9406, val_loss:  14.0158, grad_norm: 0.5512, live_grad: 0.0000, reward_err: 0.0077, 0.0742, 0.0116, KL_dist: 0.9546, 0.6544, 1.0523, param: [ 2.7116197   5.08424743  5.48186476 16.67359538], weights: [3.72137451e-04 4.57586565e-03 9.95051997e-01], train_wt_loss:  41.8219, val_wt_loss: 42.0473, train_grp_loss: [ 9.99041539 18.89389733 12.45224513], val_grp_loss: [11.21801796 19.44300305 11.36996257], train_hist_grp_loss: [ 3.88034974  6.38963763 11.77163673], cur_train_grp_loss: [0.07687217 0.15836171 0.24417235], max_reward_err:  0.0742, max_reward_err_index: 1, max_kl_dist:  1.0523, max_kl_dist_index: 2, max_train_grp_loss:  18.8939, max_train_grp_loss_index: 1, max_val_grp_loss:  19.4430, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2442, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:01,757 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  47, train_loss:  13.9573, val_loss:  14.0359, grad_norm: 0.5554, live_grad: 0.0000, reward_err: 0.0077, 0.0747, 0.0120, KL_dist: 0.9568, 0.6579, 1.0551, param: [ 2.69919537  5.07012544  5.47747903 16.71464804], weights: [3.14940237e-04 4.20316752e-03 9.95481892e-01], train_wt_loss:  41.8720, val_wt_loss: 42.1076, train_grp_loss: [ 9.98777117 18.93907905 12.45178253], val_grp_loss: [11.21459656 19.49911632 11.37748387], train_hist_grp_loss: [ 3.95719909  6.54840988 12.0157984 ], cur_train_grp_loss: [0.07684935 0.15877225 0.24416167], max_reward_err:  0.0747, max_reward_err_index: 1, max_kl_dist:  1.0551, max_kl_dist_index: 2, max_train_grp_loss:  18.9391, max_train_grp_loss_index: 1, max_val_grp_loss:  19.4991, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2442, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:02,761 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  48, train_loss:  13.9728, val_loss:  14.0545, grad_norm: 0.5593, live_grad: 0.0000, reward_err: 0.0077, 0.0752, 0.0120, KL_dist: 0.9588, 0.6612, 1.0577, param: [ 2.68776047  5.05709553  5.47342217 16.75248287], weights: [2.66520301e-04 3.86216863e-03 9.95871311e-01], train_wt_loss:  41.9185, val_wt_loss: 42.1635, train_grp_loss: [ 9.98541036 18.98085116 12.45138955], val_grp_loss: [11.21150799 19.55097773 11.38448405], train_hist_grp_loss: [ 4.0340281   6.70756181 12.259951  ], cur_train_grp_loss: [0.07682901 0.15915192 0.2441526 ], max_reward_err:  0.0752, max_reward_err_index: 1, max_kl_dist:  1.0577, max_kl_dist_index: 2, max_train_grp_loss:  18.9809, max_train_grp_loss_index: 1, max_val_grp_loss:  19.5510, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2442, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:03,755 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  49, train_loss:  13.9872, val_loss:  14.0717, grad_norm: 0.5629, live_grad: 0.0000, reward_err: 0.0078, 0.0764, 0.0124, KL_dist: 0.9607, 0.6643, 1.0600, param: [ 2.67723537  5.04507485  5.46967063 16.78735143], weights: [2.25534006e-04 3.54997827e-03 9.96224488e-01], train_wt_loss:  41.9615, val_wt_loss: 42.2152, train_grp_loss: [ 9.98329916 19.0194604  12.4510557 ], val_grp_loss: [11.20871634 19.59889763 11.3909934 ], train_hist_grp_loss: [ 4.11083895  6.86706476 12.50409589], cur_train_grp_loss: [0.07681085 0.15950295 0.24414489], max_reward_err:  0.0764, max_reward_err_index: 1, max_kl_dist:  1.0600, max_kl_dist_index: 2, max_train_grp_loss:  19.0195, max_train_grp_loss_index: 1, max_val_grp_loss:  19.5989, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:04,759 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  50, train_loss:  14.0004, val_loss:  14.0877, grad_norm: 0.5662, live_grad: 0.0000, reward_err: 0.0078, 0.0767, 0.0124, KL_dist: 0.9624, 0.6671, 1.0622, param: [ 2.66754682  5.03398635  5.46620233 16.81948627], weights: [1.90842609e-04 3.26399652e-03 9.96545161e-01], train_wt_loss:  42.0013, val_wt_loss: 42.2631, train_grp_loss: [ 9.98140813 19.05513792 12.45077207], val_grp_loss: [11.20619    19.64316629 11.39704154], train_hist_grp_loss: [ 4.18763356  7.02689215 12.74823424], cur_train_grp_loss: [0.07679461 0.1598274  0.24413835], max_reward_err:  0.0767, max_reward_err_index: 1, max_kl_dist:  1.0622, max_kl_dist_index: 2, max_train_grp_loss:  19.0551, max_train_grp_loss_index: 1, max_val_grp_loss:  19.6432, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:05,750 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  51, train_loss:  14.0127, val_loss:  14.1025, grad_norm: 0.5692, live_grad: 0.0000, reward_err: 0.0078, 0.0771, 0.0124, KL_dist: 0.9640, 0.6697, 1.0643, param: [ 2.65862743  5.02375848  5.4629966  16.84910215], weights: [1.61481207e-04 3.00188144e-03 9.96836637e-01], train_wt_loss:  42.0382, val_wt_loss: 42.3074, train_grp_loss: [ 9.97971166 19.08809972 12.45053109], val_grp_loss: [11.20390106 19.68405465 11.40265722], train_hist_grp_loss: [ 4.26441362  7.18701936 12.99236703], cur_train_grp_loss: [0.07678006 0.16012721 0.24413279], max_reward_err:  0.0771, max_reward_err_index: 1, max_kl_dist:  1.0643, max_kl_dist_index: 2, max_train_grp_loss:  19.0881, max_train_grp_loss_index: 1, max_val_grp_loss:  19.6841, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:06,743 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  52, train_loss:  14.0241, val_loss:  14.1161, grad_norm: 0.5720, live_grad: 0.0000, reward_err: 0.0078, 0.0776, 0.0124, KL_dist: 0.9654, 0.6721, 1.0662, param: [ 2.65041528  5.01432493  5.46003416 16.87639724], weights: [1.36632350e-04 2.76152039e-03 9.97101847e-01], train_wt_loss:  42.0724, val_wt_loss: 42.3483, train_grp_loss: [ 9.97818741 19.1185473  12.45032633], val_grp_loss: [11.2018249  19.72181513 11.40786813], train_hist_grp_loss: [ 4.34118063  7.34742356 13.23649509], cur_train_grp_loss: [0.07676701 0.1604042  0.24412806], max_reward_err:  0.0776, max_reward_err_index: 1, max_kl_dist:  1.0662, max_kl_dist_index: 2, max_train_grp_loss:  19.1185, max_train_grp_loss_index: 1, max_val_grp_loss:  19.7218, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:07,728 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  53, train_loss:  14.0347, val_loss:  14.1287, grad_norm: 0.5746, live_grad: 0.0000, reward_err: 0.0078, 0.0776, 0.0124, KL_dist: 0.9668, 0.6743, 1.0679, param: [ 2.64285346  5.00562431  5.45729703 16.90155427], weights: [1.15603612e-04 2.54100489e-03 9.97343392e-01], train_wt_loss:  42.1040, val_wt_loss: 42.3862, train_grp_loss: [ 9.97681586 19.14666832 12.45015233], val_grp_loss: [11.1999397  19.75668256 11.4127008 ], train_hist_grp_loss: [ 4.41793592  7.50808363 13.48061913], cur_train_grp_loss: [0.07675529 0.16066006 0.24412405], max_reward_err:  0.0776, max_reward_err_index: 1, max_kl_dist:  1.0679, max_kl_dist_index: 2, max_train_grp_loss:  19.1467, max_train_grp_loss_index: 1, max_val_grp_loss:  19.7567, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:08,724 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  54, train_loss:  14.0444, val_loss:  14.1404, grad_norm: 0.5770, live_grad: 0.0000, reward_err: 0.0078, 0.0776, 0.0124, KL_dist: 0.9680, 0.6764, 1.0695, param: [ 2.63588972  4.99759985  5.45476844 16.92474168], weights: [9.78085559e-05 2.33860856e-03 9.97563583e-01], train_wt_loss:  42.1332, val_wt_loss: 42.4212, train_grp_loss: [ 9.97557994 19.17263728 12.45000445], val_grp_loss: [11.19822614 19.78887512 11.41718049], train_hist_grp_loss: [ 4.49468066  7.66898    13.72473977], cur_train_grp_loss: [0.07674474 0.16089637 0.24412063], max_reward_err:  0.0776, max_reward_err_index: 1, max_kl_dist:  1.0695, max_kl_dist_index: 2, max_train_grp_loss:  19.1726, max_train_grp_loss_index: 1, max_val_grp_loss:  19.7889, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:09,712 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  55, train_loss:  14.0534, val_loss:  14.1512, grad_norm: 0.5792, live_grad: 0.0000, reward_err: 0.0078, 0.0776, 0.0124, KL_dist: 0.9692, 0.6783, 1.0710, param: [ 2.62947604  4.99019904  5.45243284 16.94611465], weights: [8.27505716e-05 2.15276778e-03 9.97764482e-01], train_wt_loss:  42.1602, val_wt_loss: 42.4536, train_grp_loss: [ 9.97446471 19.1966163  12.44987877], val_grp_loss: [11.1966671  19.8185953  11.42133118], train_hist_grp_loss: [ 4.57141589  7.8300946  13.9688575 ], cur_train_grp_loss: [0.07673523 0.1611146  0.24411773], max_reward_err:  0.0776, max_reward_err_index: 1, max_kl_dist:  1.0710, max_kl_dist_index: 2, max_train_grp_loss:  19.1966, max_train_grp_loss_index: 1, max_val_grp_loss:  19.8186, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:10,694 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  56, train_loss:  14.0617, val_loss:  14.1612, grad_norm: 0.5812, live_grad: 0.0000, reward_err: 0.0078, 0.0776, 0.0124, KL_dist: 0.9702, 0.6800, 1.0723, param: [ 2.62356833  4.98337342  5.45027574 16.96581612], weights: [7.00091636e-05 1.98206459e-03 9.97947926e-01], train_wt_loss:  42.1852, val_wt_loss: 42.4835, train_grp_loss: [ 9.97345705 19.21875584 12.44977194], val_grp_loss: [11.19524731 19.84603087 11.42517554], train_hist_grp_loss: [ 4.64814254  7.9914107  14.21297277], cur_train_grp_loss: [0.07672665 0.1613161  0.24411527], max_reward_err:  0.0776, max_reward_err_index: 1, max_kl_dist:  1.0723, max_kl_dist_index: 2, max_train_grp_loss:  19.2188, max_train_grp_loss_index: 1, max_val_grp_loss:  19.8460, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:11,679 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  57, train_loss:  14.0694, val_loss:  14.1704, grad_norm: 0.5830, live_grad: 0.0000, reward_err: 0.0078, 0.0776, 0.0124, KL_dist: 0.9712, 0.6816, 1.0736, param: [ 2.61812611  4.97707823  5.44828371 16.98397775], weights: [5.92283215e-05 1.82521167e-03 9.98115560e-01], train_wt_loss:  42.2083, val_wt_loss: 42.5112, train_grp_loss: [ 9.97254543 19.2391954  12.44968113], val_grp_loss: [11.19395323 19.87135579 11.42873494], train_hist_grp_loss: [ 4.72486144  8.15291285 14.45708595], cur_train_grp_loss: [0.0767189  0.16150215 0.24411318], max_reward_err:  0.0776, max_reward_err_index: 1, max_kl_dist:  1.0736, max_kl_dist_index: 2, max_train_grp_loss:  19.2392, max_train_grp_loss_index: 1, max_val_grp_loss:  19.8714, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:12,687 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  58, train_loss:  14.0765, val_loss:  14.1789, grad_norm: 0.5847, live_grad: 0.0000, reward_err: 0.0078, 0.0785, 0.0124, KL_dist: 0.9721, 0.6831, 1.0747, param: [ 2.61311217  4.97127218  5.4464443  17.00072074], weights: [5.01066582e-05 1.68103900e-03 9.98268854e-01], train_wt_loss:  42.2296, val_wt_loss: 42.5367, train_grp_loss: [ 9.9717197  19.25806422 12.44960392], val_grp_loss: [11.19277274 19.89473118 11.43202948], train_hist_grp_loss: [ 4.80157333  8.31458676 14.70119734], cur_train_grp_loss: [0.07671189 0.16167391 0.24411139], max_reward_err:  0.0785, max_reward_err_index: 1, max_kl_dist:  1.0747, max_kl_dist_index: 2, max_train_grp_loss:  19.2581, max_train_grp_loss_index: 1, max_val_grp_loss:  19.8947, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:13,670 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  59, train_loss:  14.0831, val_loss:  14.1868, grad_norm: 0.5863, live_grad: 0.0000, reward_err: 0.0078, 0.0788, 0.0124, KL_dist: 0.9729, 0.6845, 1.0758, param: [ 2.60849235  4.96591718  5.44474594 17.0161567 ], weights: [4.23890494e-05 1.54848209e-03 9.98409129e-01], train_wt_loss:  42.2494, val_wt_loss: 42.5603, train_grp_loss: [ 9.97097089 19.27548203 12.44953827], val_grp_loss: [11.19169504 19.9163061  11.43507802], train_hist_grp_loss: [ 4.87827886  8.47641923 14.94530722], cur_train_grp_loss: [0.07670554 0.16183247 0.24410988], max_reward_err:  0.0788, max_reward_err_index: 1, max_kl_dist:  1.0758, max_kl_dist_index: 2, max_train_grp_loss:  19.2755, max_train_grp_loss_index: 1, max_val_grp_loss:  19.9163, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:14,657 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  60, train_loss:  14.0892, val_loss:  14.1940, grad_norm: 0.5878, live_grad: 0.0000, reward_err: 0.0078, 0.0788, 0.0124, KL_dist: 0.9737, 0.6858, 1.0768, param: [ 2.60423528  4.96097808  5.44317793 17.0303884 ], weights: [3.58595485e-05 1.42657153e-03 9.98537569e-01], train_wt_loss:  42.2676, val_wt_loss: 42.5821, train_grp_loss: [ 9.9702911  19.2915596  12.44948245], val_grp_loss: [11.19071046 19.93621847 11.43789822], train_hist_grp_loss: [ 4.95497864  8.63839807 15.18941582], cur_train_grp_loss: [0.07669978 0.16197884 0.24410859], max_reward_err:  0.0788, max_reward_err_index: 1, max_kl_dist:  1.0768, max_kl_dist_index: 2, max_train_grp_loss:  19.2916, max_train_grp_loss_index: 1, max_val_grp_loss:  19.9362, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:15,636 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  61, train_loss:  14.0948, val_loss:  14.2007, grad_norm: 0.5891, live_grad: 0.0000, reward_err: 0.0078, 0.0788, 0.0124, KL_dist: 0.9744, 0.6870, 1.0777, param: [ 2.60031209  4.95642249  5.44173036 17.04351045], weights: [3.03353823e-05 1.31442370e-03 9.98655241e-01], train_wt_loss:  42.2844, val_wt_loss: 42.6022, train_grp_loss: [ 9.96967332 19.30639943 12.44943498], val_grp_loss: [11.18981033 19.9545958  11.44050659], train_hist_grp_loss: [ 5.03167319  8.80051202 15.43352332], cur_train_grp_loss: [0.07669455 0.16211395 0.2441075 ], max_reward_err:  0.0788, max_reward_err_index: 1, max_kl_dist:  1.0777, max_kl_dist_index: 2, max_train_grp_loss:  19.3064, max_train_grp_loss_index: 1, max_val_grp_loss:  19.9546, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:16,629 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  62, train_loss:  14.1000, val_loss:  14.2069, grad_norm: 0.5904, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9751, 0.6881, 1.0785, param: [ 2.59669631  4.95222053  5.44039405 17.05560996], weights: [2.56618639e-05 1.21123246e-03 9.98763106e-01], train_wt_loss:  42.3000, val_wt_loss: 42.6208, train_grp_loss: [ 9.96911133 19.32009632 12.44939461], val_grp_loss: [11.18898689 19.97155598 11.44291856], train_hist_grp_loss: [ 5.10836298  8.96275067 15.67762988], cur_train_grp_loss: [0.07668979 0.16223865 0.24410657], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0785, max_kl_dist_index: 2, max_train_grp_loss:  19.3201, max_train_grp_loss_index: 1, max_val_grp_loss:  19.9716, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:17,615 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  63, train_loss:  14.1048, val_loss:  14.2126, grad_norm: 0.5915, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9757, 0.6891, 1.0793, param: [ 2.59336356  4.94834466  5.43916051 17.06676713], weights: [2.17080841e-05 1.11626180e-03 9.98862030e-01], train_wt_loss:  42.3143, val_wt_loss: 42.6379, train_grp_loss: [ 9.96859963 19.33273789 12.44936028], val_grp_loss: [11.18823314 19.98720791 11.44514851], train_hist_grp_loss: [ 5.18504845  9.12510442 15.92173566], cur_train_grp_loss: [0.07668547 0.16235375 0.24410578], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0793, max_kl_dist_index: 2, max_train_grp_loss:  19.3327, max_train_grp_loss_index: 1, max_val_grp_loss:  19.9872, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:18,628 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  64, train_loss:  14.1092, val_loss:  14.2179, grad_norm: 0.5926, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9762, 0.6900, 1.0800, param: [ 2.59029144  4.94476948  5.4380219  17.07705582], weights: [1.83632619e-05 1.02883919e-03 9.98952798e-01], train_wt_loss:  42.3276, val_wt_loss: 42.6538, train_grp_loss: [ 9.96813329 19.34440515 12.44933107], val_grp_loss: [11.1875428  20.00165225 11.44720983], train_hist_grp_loss: [ 5.26172999  9.2875644  16.16584076], cur_train_grp_loss: [0.07668154 0.16245998 0.2441051 ], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0800, max_kl_dist_index: 2, max_train_grp_loss:  19.3444, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0017, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:19,642 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  65, train_loss:  14.1133, val_loss:  14.2228, grad_norm: 0.5935, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9768, 0.6909, 1.0807, param: [ 2.58745938  4.9414716   5.43697093 17.08654404], weights: [1.55336533e-05 9.48349728e-04 9.99036117e-01], train_wt_loss:  42.3398, val_wt_loss: 42.6684, train_grp_loss: [ 9.96770795 19.35517293 12.44930623], val_grp_loss: [11.18691019 20.01498195 11.44911499], train_hist_grp_loss: [ 5.33840794  9.45012243 16.40994529], cur_train_grp_loss: [0.07667795 0.16255803 0.24410453], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0807, max_kl_dist_index: 2, max_train_grp_loss:  19.3552, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0150, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:20,630 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  66, train_loss:  14.1170, val_loss:  14.2273, grad_norm: 0.5944, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9772, 0.6917, 1.0813, param: [ 2.58484843  4.93842943  5.4360009  17.09529444], weights: [1.31399350e-05 8.74230785e-04 9.99112629e-01], train_wt_loss:  42.3511, val_wt_loss: 42.6819, train_grp_loss: [ 9.96731969 19.3651104  12.44928509], val_grp_loss: [11.1863302  20.02728286 11.45087558], train_hist_grp_loss: [ 5.41508261  9.61277094 16.65404934], cur_train_grp_loss: [0.07667468 0.16264851 0.24410404], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0813, max_kl_dist_index: 2, max_train_grp_loss:  19.3651, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0273, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:21,639 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  67, train_loss:  14.1205, val_loss:  14.2315, grad_norm: 0.5952, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9777, 0.6924, 1.0818, param: [ 2.58244117  4.93562308  5.43510561 17.1033647 ], weights: [1.11149881e-05 8.05967272e-04 9.99182918e-01], train_wt_loss:  42.3616, val_wt_loss: 42.6944, train_grp_loss: [ 9.96696502 19.37428141 12.44926711], val_grp_loss: [11.18579821 20.03863426 11.45250236], train_hist_grp_loss: [ 5.4917543   9.77550296 16.89815297], cur_train_grp_loss: [0.07667169 0.16273202 0.24410363], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0818, max_kl_dist_index: 2, max_train_grp_loss:  19.3743, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0386, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:22,633 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  68, train_loss:  14.1237, val_loss:  14.2353, grad_norm: 0.5960, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9781, 0.6931, 1.0823, param: [ 2.58022157  4.93303422  5.43427931 17.11080796], weights: [9.40202231e-06 7.43087342e-04 9.99247511e-01], train_wt_loss:  42.3712, val_wt_loss: 42.7059, train_grp_loss: [ 9.96664081 19.38274496 12.44925181], val_grp_loss: [11.18531004 20.04910935 11.45400535], train_hist_grp_loss: [ 5.56842326  9.93831205 17.14225624], cur_train_grp_loss: [0.07666896 0.16280909 0.24410328], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0823, max_kl_dist_index: 2, max_train_grp_loss:  19.3827, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0491, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:23,626 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  69, train_loss:  14.1267, val_loss:  14.2388, grad_norm: 0.5967, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9784, 0.6937, 1.0828, param: [ 2.5781749   4.93064594  5.43351671 17.11767317], weights: [7.95298808e-06 6.85158532e-04 9.99306888e-01], train_wt_loss:  42.3801, val_wt_loss: 42.7165, train_grp_loss: [ 9.96634424 19.39055553 12.44923879], val_grp_loss: [11.18486189 20.0587757  11.45539382], train_hist_grp_loss: [ 5.64508973 10.10119226 17.38635922], cur_train_grp_loss: [0.07666647 0.16288021 0.24410298], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0828, max_kl_dist_index: 2, max_train_grp_loss:  19.3906, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0588, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:24,613 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  70, train_loss:  14.1294, val_loss:  14.2421, grad_norm: 0.5974, live_grad: 0.0000, reward_err: 0.0078, 0.0795, 0.0124, KL_dist: 0.9788, 0.6943, 1.0833, param: [ 2.57628757  4.92844266  5.43281291 17.12400545], weights: [6.72723235e-06 6.31784279e-04 9.99361488e-01], train_wt_loss:  42.3883, val_wt_loss: 42.7263, train_grp_loss: [ 9.96607281 19.39776342 12.44922772], val_grp_loss: [11.18445033 20.0676957  11.45667637], train_hist_grp_loss: [ 5.72175392 10.2641381  17.63046194], cur_train_grp_loss: [0.07666419 0.16294584 0.24410272], max_reward_err:  0.0795, max_reward_err_index: 1, max_kl_dist:  1.0833, max_kl_dist_index: 2, max_train_grp_loss:  19.3978, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0677, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:25,599 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  71, train_loss:  14.1320, val_loss:  14.2451, grad_norm: 0.5980, live_grad: 0.0000, reward_err: 0.0078, 0.0799, 0.0124, KL_dist: 0.9791, 0.6948, 1.0837, param: [ 2.57454709  4.92641     5.43216339 17.12984633], weights: [5.69036085e-06 5.82600770e-04 9.99411709e-01], train_wt_loss:  42.3959, val_wt_loss: 42.7354, train_grp_loss: [ 9.96582423 19.40441506 12.44921829], val_grp_loss: [11.18407225 20.07592694 11.45786099], train_hist_grp_loss: [ 5.79841602 10.42714452 17.87456445], cur_train_grp_loss: [0.0766621  0.16300642 0.2441025 ], max_reward_err:  0.0799, max_reward_err_index: 1, max_kl_dist:  1.0837, max_kl_dist_index: 2, max_train_grp_loss:  19.4044, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0759, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:26,595 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  72, train_loss:  14.1343, val_loss:  14.2479, grad_norm: 0.5985, live_grad: 0.0000, reward_err: 0.0078, 0.0805, 0.0124, KL_dist: 0.9794, 0.6953, 1.0840, param: [ 2.57294196  4.92453473  5.43156397 17.1352341 ], weights: [4.81327517e-06 5.37274098e-04 9.99457913e-01], train_wt_loss:  42.4029, val_wt_loss: 42.7437, train_grp_loss: [ 9.96559646 19.41055335 12.44921027], val_grp_loss: [11.1837248  20.08352256 11.45895507], train_hist_grp_loss: [ 5.8750762  10.59020683 18.11866677], cur_train_grp_loss: [0.07666019 0.16306231 0.24410232], max_reward_err:  0.0805, max_reward_err_index: 1, max_kl_dist:  1.0840, max_kl_dist_index: 2, max_train_grp_loss:  19.4106, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0835, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:27,581 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  73, train_loss:  14.1365, val_loss:  14.2505, grad_norm: 0.5990, live_grad: 0.0000, reward_err: 0.0078, 0.0805, 0.0124, KL_dist: 0.9797, 0.6958, 1.0844, param: [ 2.5714616   4.92280463  5.4310108  17.14020406], weights: [4.07135787e-06 4.95497675e-04 9.99500431e-01], train_wt_loss:  42.4094, val_wt_loss: 42.7514, train_grp_loss: [ 9.96538765 19.41621785 12.44920344], val_grp_loss: [11.18340541 20.09053164 11.45996545], train_hist_grp_loss: [ 5.95173464 10.75332072 18.36276893], cur_train_grp_loss: [0.07665843 0.16311389 0.24410216], max_reward_err:  0.0805, max_reward_err_index: 1, max_kl_dist:  1.0844, max_kl_dist_index: 2, max_train_grp_loss:  19.4162, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0905, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:28,591 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  74, train_loss:  14.1385, val_loss:  14.2529, grad_norm: 0.5995, live_grad: 0.0000, reward_err: 0.0078, 0.0805, 0.0124, KL_dist: 0.9799, 0.6962, 1.0847, param: [ 2.57009625  4.92120843  5.43050031 17.14478871], weights: [3.44378283e-06 4.56989896e-04 9.99539566e-01], train_wt_loss:  42.4154, val_wt_loss: 42.7586, train_grp_loss: [ 9.96519614 19.42144511 12.44919763], val_grp_loss: [11.18311173 20.09699943 11.46089849], train_hist_grp_loss: [ 6.02839147 10.91648222 18.60687096], cur_train_grp_loss: [0.07665683 0.16316149 0.24410203], max_reward_err:  0.0805, max_reward_err_index: 1, max_kl_dist:  1.0847, max_kl_dist_index: 2, max_train_grp_loss:  19.4214, max_train_grp_loss_index: 1, max_val_grp_loss:  20.0970, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:29,571 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  75, train_loss:  14.1403, val_loss:  14.2550, grad_norm: 0.5999, live_grad: 0.0000, reward_err: 0.0077, 0.0805, 0.0127, KL_dist: 0.9801, 0.6966, 1.0850, param: [ 2.56883693  4.91973575  5.43002922 17.14901804], weights: [2.91293165e-06 4.21492015e-04 9.99575595e-01], train_wt_loss:  42.4209, val_wt_loss: 42.7651, train_grp_loss: [ 9.96502041 19.42626885 12.44919269], val_grp_loss: [11.18284162 20.10296774 11.46176005], train_hist_grp_loss: [ 6.10504682 11.07968764 18.85097287], cur_train_grp_loss: [0.07665535 0.16320542 0.24410191], max_reward_err:  0.0805, max_reward_err_index: 1, max_kl_dist:  1.0850, max_kl_dist_index: 2, max_train_grp_loss:  19.4263, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1030, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:30,560 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  76, train_loss:  14.1420, val_loss:  14.2571, grad_norm: 0.6003, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9804, 0.6969, 1.0853, param: [ 2.56767535  4.91837701  5.42959448 17.15291966], weights: [2.46389982e-06 3.88766210e-04 9.99608770e-01], train_wt_loss:  42.4260, val_wt_loss: 42.7712, train_grp_loss: [ 9.9648591  19.4307202  12.44918848], val_grp_loss: [11.18259313 20.10847512 11.46255556], train_hist_grp_loss: [ 6.18170082 11.2429336  19.09507469], cur_train_grp_loss: [0.076654   0.16324596 0.24410182], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0853, max_kl_dist_index: 2, max_train_grp_loss:  19.4307, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1085, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:31,542 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  77, train_loss:  14.1436, val_loss:  14.2589, grad_norm: 0.6007, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9806, 0.6972, 1.0855, param: [ 2.56660391  4.91712339  5.4291933  17.15651904], weights: [2.08407889e-06 3.58593829e-04 9.99639322e-01], train_wt_loss:  42.4307, val_wt_loss: 42.7768, train_grp_loss: [ 9.96471097 19.4348279  12.4491849 ], val_grp_loss: [11.18236447 20.11355718 11.46329005], train_hist_grp_loss: [ 6.25835359 11.40621696 19.33917642], cur_train_grp_loss: [0.07665276 0.16328336 0.24410173], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0855, max_kl_dist_index: 2, max_train_grp_loss:  19.4348, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1136, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:32,534 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  78, train_loss:  14.1450, val_loss:  14.2607, grad_norm: 0.6010, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9807, 0.6975, 1.0857, param: [ 2.56561558  4.91596672  5.42882309 17.15983966], weights: [1.76280288e-06 3.30773787e-04 9.99667463e-01], train_wt_loss:  42.4350, val_wt_loss: 42.7820, train_grp_loss: [ 9.9645749  19.43861846 12.44918185], val_grp_loss: [11.18215402 20.11824674 11.46396818], train_hist_grp_loss: [ 6.33500521 11.56953484 19.58327809], cur_train_grp_loss: [0.07665162 0.16331788 0.24410166], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0857, max_kl_dist_index: 2, max_train_grp_loss:  19.4386, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1182, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:33,550 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  79, train_loss:  14.1463, val_loss:  14.2622, grad_norm: 0.6013, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9809, 0.6978, 1.0860, param: [ 2.56470389  4.91489952  5.42848146 17.16290316], weights: [1.49104914e-06 3.05121111e-04 9.99693388e-01], train_wt_loss:  42.4390, val_wt_loss: 42.7867, train_grp_loss: [ 9.96444986 19.44211635 12.44917925], val_grp_loss: [11.1819603  20.12257412 11.46459423], train_hist_grp_loss: [ 6.41165578 11.73288458 19.82737969], cur_train_grp_loss: [0.07665058 0.16334973 0.2441016 ], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0860, max_kl_dist_index: 2, max_train_grp_loss:  19.4421, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1226, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:34,534 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  80, train_loss:  14.1476, val_loss:  14.2637, grad_norm: 0.6016, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9811, 0.6981, 1.0862, param: [ 2.56386286  4.91391484  5.42816621 17.1657295 ], weights: [1.26118524e-06 2.81465603e-04 9.99717273e-01], train_wt_loss:  42.4427, val_wt_loss: 42.7911, train_grp_loss: [ 9.96433493 19.44534418 12.44917705], val_grp_loss: [11.18178193 20.12656729 11.46517219], train_hist_grp_loss: [ 6.4883054  11.89626371 20.07148125], cur_train_grp_loss: [0.07664961 0.16337913 0.24410155], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0862, max_kl_dist_index: 2, max_train_grp_loss:  19.4453, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1266, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:35,550 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  81, train_loss:  14.1487, val_loss:  14.2651, grad_norm: 0.6019, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9812, 0.6983, 1.0863, param: [ 2.56308701  4.91300629  5.42787529 17.16833708], weights: [1.06675484e-06 2.59650633e-04 9.99739283e-01], train_wt_loss:  42.4461, val_wt_loss: 42.7952, train_grp_loss: [ 9.96422925 19.44832277 12.44917516], val_grp_loss: [11.18161768 20.13025207 11.46570574], train_hist_grp_loss: [ 6.56495413 12.05966996 20.31558276], cur_train_grp_loss: [0.07664873 0.16340625 0.24410151], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0863, max_kl_dist_index: 2, max_train_grp_loss:  19.4483, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1303, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:36,532 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  82, train_loss:  14.1497, val_loss:  14.2663, grad_norm: 0.6021, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9813, 0.6985, 1.0865, param: [ 2.56237127  4.91216799  5.42760684 17.17074286], weights: [9.02296508e-07 2.39532021e-04 9.99759566e-01], train_wt_loss:  42.4492, val_wt_loss: 42.7989, train_grp_loss: [ 9.96413205 19.45107137 12.44917356], val_grp_loss: [11.18146641 20.13365226 11.46619826], train_hist_grp_loss: [ 6.64160205 12.22310124 20.55968423], cur_train_grp_loss: [0.07664792 0.16343128 0.24410147], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0865, max_kl_dist_index: 2, max_train_grp_loss:  19.4511, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1337, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:37,517 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  83, train_loss:  14.1507, val_loss:  14.2675, grad_norm: 0.6024, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9815, 0.6988, 1.0867, param: [ 2.56171096  4.91139449  5.42735912 17.17296249], weights: [7.63190456e-07 2.20977026e-04 9.99778260e-01], train_wt_loss:  42.4521, val_wt_loss: 42.8024, train_grp_loss: [ 9.96404264 19.45360774 12.4491722 ], val_grp_loss: [11.18132707 20.13678984 11.46665291], train_hist_grp_loss: [ 6.71824922 12.38655562 20.80378567], cur_train_grp_loss: [0.07664717 0.16345438 0.24410144], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0867, max_kl_dist_index: 2, max_train_grp_loss:  19.4536, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1368, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:38,529 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  84, train_loss:  14.1516, val_loss:  14.2685, grad_norm: 0.6026, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9816, 0.6989, 1.0868, param: [ 2.56110179  4.91068078  5.42713053 17.17501041], weights: [6.45528858e-07 2.03863420e-04 9.99795491e-01], train_wt_loss:  42.4548, val_wt_loss: 42.8056, train_grp_loss: [ 9.96396037 19.45594826 12.44917104], val_grp_loss: [11.18119869 20.13968511 11.46707258], train_hist_grp_loss: [ 6.7948957  12.55003132 21.04788709], cur_train_grp_loss: [0.07664648 0.1634757  0.24410142], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0868, max_kl_dist_index: 2, max_train_grp_loss:  19.4559, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1397, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:39,515 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  85, train_loss:  14.1524, val_loss:  14.2695, grad_norm: 0.6028, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9817, 0.6991, 1.0869, param: [ 2.56053978  4.91002224  5.42691959 17.1768999 ], weights: [5.46006161e-07 1.88078629e-04 9.99811375e-01], train_wt_loss:  42.4573, val_wt_loss: 42.8085, train_grp_loss: [ 9.96388465 19.45810804 12.44917005], val_grp_loss: [11.18108042 20.14235676 11.46745995], train_hist_grp_loss: [ 6.87154155 12.71352668 21.29198848], cur_train_grp_loss: [0.07664585 0.16349536 0.24410139], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0869, max_kl_dist_index: 2, max_train_grp_loss:  19.4581, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1424, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:40,501 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  86, train_loss:  14.1532, val_loss:  14.2704, grad_norm: 0.6029, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9818, 0.6993, 1.0871, param: [ 2.56002128  4.90941459  5.42672494 17.17864326], weights: [4.61826278e-07 1.73518965e-04 9.99826019e-01], train_wt_loss:  42.4596, val_wt_loss: 42.8113, train_grp_loss: [ 9.96381496 19.46010105 12.44916921], val_grp_loss: [11.18097142 20.14482208 11.46781751], train_hist_grp_loss: [ 6.94818681 12.8770402  21.53608986], cur_train_grp_loss: [0.07664527 0.16351351 0.24410137], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0871, max_kl_dist_index: 2, max_train_grp_loss:  19.4601, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1448, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:41,496 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  87, train_loss:  14.1539, val_loss:  14.2713, grad_norm: 0.6031, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9818, 0.6994, 1.0872, param: [ 2.5595429   4.9088539   5.42654532 17.18025179], weights: [3.90624084e-07 1.60088904e-04 9.99839520e-01], train_wt_loss:  42.4617, val_wt_loss: 42.8138, train_grp_loss: [ 9.96375079 19.46194016 12.4491685 ], val_grp_loss: [11.18087098 20.147097   11.46814753], train_hist_grp_loss: [ 7.02483154 13.04057046 21.78019121], cur_train_grp_loss: [0.07664473 0.16353026 0.24410136], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0872, max_kl_dist_index: 2, max_train_grp_loss:  19.4619, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1471, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:42,489 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  88, train_loss:  14.1545, val_loss:  14.2720, grad_norm: 0.6033, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9819, 0.6996, 1.0873, param: [ 2.55910154  4.90833654  5.42637957 17.18173594], weights: [3.30399002e-07 1.47700435e-04 9.99851969e-01], train_wt_loss:  42.4636, val_wt_loss: 42.8161, train_grp_loss: [ 9.9636917  19.46363725 12.44916789], val_grp_loss: [11.18077841 20.14919622 11.46845214], train_hist_grp_loss: [ 7.10147578 13.20411617 22.02429256], cur_train_grp_loss: [0.07664424 0.16354572 0.24410134], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0873, max_kl_dist_index: 2, max_train_grp_loss:  19.4636, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1492, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:43,516 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  89, train_loss:  14.1551, val_loss:  14.2727, grad_norm: 0.6034, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9820, 0.6997, 1.0874, param: [ 2.55869433  4.90785916  5.42622662 17.18310534], weights: [2.79458821e-07 1.36272459e-04 9.99863448e-01], train_wt_loss:  42.4654, val_wt_loss: 42.8182, train_grp_loss: [ 9.96363728 19.46520329 12.44916737], val_grp_loss: [11.18069308 20.15113332 11.46873328], train_hist_grp_loss: [ 7.17811956 13.36767615 22.26839389], cur_train_grp_loss: [0.07664378 0.16355998 0.24410133], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0874, max_kl_dist_index: 2, max_train_grp_loss:  19.4652, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1511, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:44,511 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  90, train_loss:  14.1557, val_loss:  14.2734, grad_norm: 0.6035, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9821, 0.6998, 1.0875, param: [ 2.55831862  4.90741866  5.42608549 17.18436887], weights: [2.36372176e-07 1.25730240e-04 9.99874033e-01], train_wt_loss:  42.4671, val_wt_loss: 42.8202, train_grp_loss: [ 9.96358715 19.4666484  12.44916693], val_grp_loss: [11.18061442 20.15292082 11.46899276], train_hist_grp_loss: [ 7.25476293 13.53124929 22.51249521], cur_train_grp_loss: [0.07664336 0.16357314 0.24410132], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0875, max_kl_dist_index: 2, max_train_grp_loss:  19.4666, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1529, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:45,497 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  91, train_loss:  14.1562, val_loss:  14.2740, grad_norm: 0.6036, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9821, 0.6999, 1.0875, param: [ 2.55797197  4.9070122   5.42595525 17.18553472], weights: [1.99928342e-07 1.16004895e-04 9.99883795e-01], train_wt_loss:  42.4686, val_wt_loss: 42.8220, train_grp_loss: [ 9.96354097 19.46798191 12.44916655], val_grp_loss: [11.1805419  20.15457026 11.46923225], train_hist_grp_loss: [ 7.3314059  13.69483457 22.75659652], cur_train_grp_loss: [0.07664298 0.16358528 0.24410131], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0875, max_kl_dist_index: 2, max_train_grp_loss:  19.4680, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1546, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:46,485 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  92, train_loss:  14.1567, val_loss:  14.2746, grad_norm: 0.6037, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9822, 0.7000, 1.0876, param: [ 2.55765212  4.90663714  5.42583508 17.18661045], weights: [1.69103226e-07 1.07032934e-04 9.99892798e-01], train_wt_loss:  42.4700, val_wt_loss: 42.8237, train_grp_loss: [ 9.96349842 19.46921244 12.44916623], val_grp_loss: [11.18047504 20.15609232 11.46945328], train_hist_grp_loss: [ 7.40804853 13.85843105 23.00069783], cur_train_grp_loss: [0.07664262 0.16359649 0.2441013 ], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0876, max_kl_dist_index: 2, max_train_grp_loss:  19.4692, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1561, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:47,478 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  93, train_loss:  14.1571, val_loss:  14.2751, grad_norm: 0.6038, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9822, 0.7001, 1.0877, param: [ 2.55735702  4.90629106  5.42572418 17.18760303], weights: [1.43030606e-07 9.87558271e-05 9.99901101e-01], train_wt_loss:  42.4713, val_wt_loss: 42.8252, train_grp_loss: [ 9.96345922 19.47034795 12.44916596], val_grp_loss: [11.1804134  20.15749682 11.46965727], train_hist_grp_loss: [ 7.48469082 14.02203788 23.24479912], cur_train_grp_loss: [0.0766423  0.16360683 0.2441013 ], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0877, max_kl_dist_index: 2, max_train_grp_loss:  19.4703, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1575, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:48,468 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  94, train_loss:  14.1575, val_loss:  14.2756, grad_norm: 0.6039, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9823, 0.7002, 1.0877, param: [ 2.55708472  4.90597171  5.42562185 17.1885189 ], weights: [1.20977792e-07 9.11196197e-05 9.99908759e-01], train_wt_loss:  42.4725, val_wt_loss: 42.8267, train_grp_loss: [ 9.96342309 19.47139577 12.44916573], val_grp_loss: [11.18035656 20.15879286 11.46984553], train_hist_grp_loss: [ 7.56133282 14.18565425 23.48890042], cur_train_grp_loss: [0.07664199 0.16361637 0.24410129], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0877, max_kl_dist_index: 2, max_train_grp_loss:  19.4714, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1588, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:49,452 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  95, train_loss:  14.1579, val_loss:  14.2760, grad_norm: 0.6040, live_grad: 0.0000, reward_err: 0.0077, 0.0811, 0.0127, KL_dist: 0.9823, 0.7003, 1.0878, param: [ 2.55683349  4.90567704  5.42552742 17.18936398], weights: [1.02325047e-07 8.40745660e-05 9.99915823e-01], train_wt_loss:  42.4736, val_wt_loss: 42.8280, train_grp_loss: [ 9.96338979 19.47236267 12.44916553], val_grp_loss: [11.18030414 20.1599888  11.47001928], train_hist_grp_loss: [ 7.63797453 14.34927942 23.73300171], cur_train_grp_loss: [0.07664172 0.16362517 0.24410129], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0878, max_kl_dist_index: 2, max_train_grp_loss:  19.4724, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1600, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:50,457 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  96, train_loss:  14.1582, val_loss:  14.2764, grad_norm: 0.6041, live_grad: 0.0000, reward_err: 0.0076, 0.0811, 0.0129, KL_dist: 0.9824, 0.7003, 1.0879, param: [ 2.55660167  4.90540513  5.42544029 17.19014376], weights: [8.65481732e-08 7.75747996e-05 9.99922339e-01], train_wt_loss:  42.4746, val_wt_loss: 42.8292, train_grp_loss: [ 9.96335909 19.4732549  12.44916536], val_grp_loss: [11.18025581 20.16109239 11.47017963], train_hist_grp_loss: [ 7.71461599 14.51291272 23.97710299], cur_train_grp_loss: [0.07664146 0.1636333  0.24410128], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0879, max_kl_dist_index: 2, max_train_grp_loss:  19.4733, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1611, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:51,442 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  97, train_loss:  14.1585, val_loss:  14.2768, grad_norm: 0.6042, live_grad: 0.0000, reward_err: 0.0076, 0.0811, 0.0129, KL_dist: 0.9824, 0.7004, 1.0879, param: [ 2.55638777  4.90515422  5.42535988 17.19086328], weights: [7.32037850e-08 7.15780279e-05 9.99928349e-01], train_wt_loss:  42.4756, val_wt_loss: 42.8303, train_grp_loss: [ 9.9633308  19.47407823 12.44916522], val_grp_loss: [11.18021123 20.16211074 11.47032761], train_hist_grp_loss: [ 7.79125722 14.67655352 24.22120427], cur_train_grp_loss: [0.07664122 0.1636408  0.24410128], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0879, max_kl_dist_index: 2, max_train_grp_loss:  19.4741, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1621, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:52,434 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  98, train_loss:  14.1588, val_loss:  14.2771, grad_norm: 0.6042, live_grad: 0.0000, reward_err: 0.0076, 0.0811, 0.0129, KL_dist: 0.9825, 0.7005, 1.0879, param: [ 2.55619041  4.90492269  5.42528569 17.1915272 ], weights: [6.19168527e-08 6.60452515e-05 9.99933893e-01], train_wt_loss:  42.4764, val_wt_loss: 42.8314, train_grp_loss: [ 9.96330472 19.47483799 12.44916509], val_grp_loss: [11.18017012 20.16305044 11.47046418], train_hist_grp_loss: [ 7.86789822 14.84020124 24.46530555], cur_train_grp_loss: [0.07664101 0.16364772 0.24410128], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0879, max_kl_dist_index: 2, max_train_grp_loss:  19.4748, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1631, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:53,441 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1546] - INFO: Iteration:  99, train_loss:  14.1591, val_loss:  14.2774, grad_norm: 0.6043, live_grad: 0.0000, reward_err: 0.0076, 0.0811, 0.0129, KL_dist: 0.9825, 0.7005, 1.0880, param: [ 2.5560083   4.90470905  5.42521722 17.19213982], weights: [5.23701642e-08 6.09405058e-05 9.99939007e-01], train_wt_loss:  42.4772, val_wt_loss: 42.8323, train_grp_loss: [ 9.96328067 19.47553906 12.44916499], val_grp_loss: [11.1801322  20.16391758 11.47059022], train_hist_grp_loss: [ 7.94453903 15.00385534 24.70940683], cur_train_grp_loss: [0.07664081 0.1636541  0.24410128], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0880, max_kl_dist_index: 2, max_train_grp_loss:  19.4755, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1639, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:54,352 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_robust_dpo_vectorised_gradfix.py[line:1654] - INFO: Iteration:  99, train_loss:  14.1591, val_loss:  14.2774, grad_norm: 0.6043,  live_grad: 0.0000, reward_err: 0.0076, 0.0811, 0.0129, KL_dist: 0.9825, 0.7005, 1.0880, param: [ 2.5560083   4.90470905  5.42521722 17.19213982], weights: [5.23701642e-08 6.09405058e-05 9.99939007e-01], train_wt_loss:  42.4772, val_wt_loss: 42.8323, train_grp_loss: [ 9.96328067 19.47553906 12.44916499], val_grp_loss: [11.1801322  20.16391758 11.47059022], train_hist_grp_loss: [ 7.94453903 15.00385534 24.70940683], cur_train_grp_loss: [0.07664081 0.1636541  0.24410128], max_reward_err:  0.0811, max_reward_err_index: 1, max_kl_dist:  1.0880, max_kl_dist_index: 2, max_train_grp_loss:  19.4755, max_train_grp_loss_index: 1, max_val_grp_loss:  20.1639, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  0.2441, max_cur_train_grp_loss_index: 2, 
2024-10-07 17:20:54,579 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:388] - INFO: Policy parameter learned solely on the preference data rdpo: [ 2.5560083   4.90470905  5.42521722 17.19213982].
2024-10-07 17:20:54,922 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Uniform reward: 3.8511, 3.8511, 3.1970
2024-10-07 17:20:54,923 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Optimal reward: 3.8856, 7.2460, 3.3201
2024-10-07 17:20:54,923 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Policy reward: 3.8561, 6.6586, 3.2773
2024-10-07 17:20:54,924 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:402] - INFO: Reward Error: 0.0076, 0.0811, 0.0129
2024-10-07 17:20:55,600 - /Users/noesis/Code/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:417] - INFO: Optimal reward: 3.8856, 7.2460, 3.3201
Known param reward: [[3.885563232421875, 6.83889453125, 3.291370361328125], [3.493690185546875, 7.24602734375, 3.1155205078125], [3.850873046875, 7.10487548828125, 3.320061279296875]], Known param reward error: [[0.0, 0.05618703783269167, 0.008641683256769945], [0.10085360176489651, 0.0, 0.06160752898142066], [0.008927968346368324, 0.019479895503085456, 0.0]].
