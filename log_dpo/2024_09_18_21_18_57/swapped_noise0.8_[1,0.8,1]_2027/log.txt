2024-09-18 21:27:03,593 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_21_18_57/swapped_noise0.8_[1,0.8,1]_2027
2024-09-18 21:27:03,595 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2027
2024-09-18 21:27:03,596 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 21:27:03,758 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3814, l2 distance: 23.7630, acc: 0.85.
2024-09-18 21:27:03,759 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 21:27:03,760 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [ 8.88080729  4.95567706 12.33493647  6.36297512]
2024-09-18 21:27:03,970 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4864, 3.7728, 3.1363
2024-09-18 21:27:05,381 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.3580, val_loss:  20.0949, grad_norm: 0.8272, reward_err: 0.0626, 0.0774, 0.0381, KL_dist: 0.1948, 0.2748, 0.1371, param: [1.18153194 0.51810201 4.20744823 4.89851458]train_grp_loss: [21.71754131 16.06922338 20.50396697], val_grp_loss: [21.97542423 17.5190625  21.92337178], train_hist_grp_loss: [21.75578534 16.17664593 20.56196328], cur_train_grp_loss: [21.75578534 16.17664593 20.56196328],max_reward_err:  0.0774, max_reward_err_index: 1, max_kl_dist:  0.2748, max_kl_dist_index: 1, max_train_grp_loss:  21.7175, max_train_grp_loss_index: 0, max_val_grp_loss:  21.9754, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.7558, max_cur_train_grp_loss_index: 0, 
2024-09-18 21:27:08,648 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  15.8202, val_loss:  16.4757, grad_norm: 0.4144, reward_err: 0.0782, 0.0295, 0.0451, KL_dist: 0.7947, 0.4417, 0.6347, param: [4.25656927 2.96252628 8.40172733 5.64533391]train_grp_loss: [19.55516021 11.05744618 16.83358509], val_grp_loss: [19.87838305 11.93516413 19.48730053], train_hist_grp_loss: [2044.51056923 1295.4285275  1842.79459467], cur_train_grp_loss: [19.56639778 11.07796916 16.85650965],max_reward_err:  0.0782, max_reward_err_index: 0, max_kl_dist:  0.7947, max_kl_dist_index: 0, max_train_grp_loss:  19.5552, max_train_grp_loss_index: 0, max_val_grp_loss:  19.8784, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.5664, max_cur_train_grp_loss_index: 0, 
2024-09-18 21:27:08,879 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [4.25656927 2.96252628 8.40172733 5.64533391].
2024-09-18 21:27:09,221 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7625, 3.7625, 3.1399
2024-09-18 21:27:09,222 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8051, 3.8051, 3.2906
2024-09-18 21:27:09,223 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5075, 3.6929, 3.1421
2024-09-18 21:27:09,223 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0782, 0.0295, 0.0451
2024-09-18 21:27:09,909 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8051, 3.8051, 3.2906
Known param reward: [[3.805128812789917, 3.434398889541626, 3.249009847640991], [3.434398889541626, 3.805128812789917, 3.1050264835357666], [3.7626492977142334, 3.5557007789611816, 3.2906291484832764]], Known param reward error: [[0.0, 0.09742900739711678, 0.012647824766722318], [0.09742900739711678, 0.0, 0.05640339782228518], [0.011163752179137834, 0.06555048359738837, 0.0]].
