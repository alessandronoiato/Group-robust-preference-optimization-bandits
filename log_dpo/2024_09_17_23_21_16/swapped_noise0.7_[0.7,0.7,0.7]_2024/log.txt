2024-09-17 23:22:18,652 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2024
2024-09-17 23:22:18,654 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2024
2024-09-17 23:22:18,655 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:22:18,814 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5476, l2 distance: 11.1533, acc: 0.75.
2024-09-17 23:22:18,815 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:22:18,816 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.90113754 4.18203339 5.68128049 3.94458909]
2024-09-17 23:22:19,017 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.6835, 3.8594, 3.3262
2024-09-17 23:22:20,300 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.3824, val_loss:  21.7916, grad_norm: 0.5680, reward_err: 0.0031, 0.1104, 0.0156, KL_dist: 0.0527, 0.4007, 0.0920, param: [0.52265919 4.296162   2.01816576 4.3536708 ]train_grp_loss: [22.54799552 21.64528413 22.84853595], val_grp_loss: [22.24677007 21.19638172 21.9007753 ], train_hist_grp_loss: [22.55347625 21.72730171 22.86467947], cur_train_grp_loss: [22.55347625 21.72730171 22.86467947],max_reward_err:  0.1104, max_reward_err_index: 1, max_kl_dist:  0.4007, max_kl_dist_index: 1, max_train_grp_loss:  22.8485, max_train_grp_loss_index: 2, max_val_grp_loss:  22.2468, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.8647, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:22:23,411 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  20.4161, val_loss:  19.5172, grad_norm: 0.3386, reward_err: 0.0375, 0.0422, 0.0110, KL_dist: 0.2963, 0.3372, 0.2309, param: [3.7641733  4.41650192 4.88357348 4.93689606]train_grp_loss: [22.21122803 16.80133343 21.71936383], val_grp_loss: [21.80468967 15.83781731 20.72135271], train_hist_grp_loss: [2235.52024407 1883.22226582 2222.91947067], cur_train_grp_loss: [22.2132108  16.82793917 21.72725726],max_reward_err:  0.0422, max_reward_err_index: 1, max_kl_dist:  0.3372, max_kl_dist_index: 1, max_train_grp_loss:  22.2112, max_train_grp_loss_index: 0, max_val_grp_loss:  21.8047, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2132, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:22:23,624 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [3.7641733  4.41650192 4.88357348 4.93689606].
2024-09-17 23:22:23,946 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8925, 3.8925, 3.2811
2024-09-17 23:22:23,947 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.9322, 3.9322, 3.4172
2024-09-17 23:22:23,947 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.7849, 3.7663, 3.3795
2024-09-17 23:22:23,948 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0375, 0.0422, 0.0110
2024-09-17 23:22:24,617 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.9322, 3.9322, 3.4172
Known param reward: [[3.9322431087493896, 3.5081164836883545, 3.382530450820923], [3.5081164836883545, 3.9322431087493896, 3.203848123550415], [3.891317367553711, 3.641733407974243, 3.4171640872955322]], Known param reward error: [[0.0, 0.10785869879645472, 0.010135198541788402], [0.10785869879645472, 0.0, 0.06242485239096118], [0.010407734227982341, 0.07387887593438752, 0.0]].
