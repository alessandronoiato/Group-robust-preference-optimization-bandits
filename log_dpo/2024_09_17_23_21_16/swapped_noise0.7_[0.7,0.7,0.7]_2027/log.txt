2024-09-17 23:23:14,107 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2027
2024-09-17 23:23:14,109 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2027
2024-09-17 23:23:14,109 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:23:14,273 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4784, l2 distance: 16.4441, acc: 0.78.
2024-09-17 23:23:14,274 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:23:14,275 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [9.38436691 4.90218903 6.44837083 3.36986384]
2024-09-17 23:23:14,481 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5260, 3.8224, 3.1967
2024-09-17 23:23:15,805 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.4724, val_loss:  22.5869, grad_norm: 0.7928, reward_err: 0.0408, 0.0458, 0.0125, KL_dist: 0.0642, 0.0623, 0.0234, param: [1.35054225 1.80040237 1.66551823 1.10716777]train_grp_loss: [23.78438664 19.7440854  23.53756557], val_grp_loss: [24.01585955 20.15087942 23.57450546], train_hist_grp_loss: [23.80915977 19.87937698 23.57481435], cur_train_grp_loss: [23.80915977 19.87937698 23.57481435],max_reward_err:  0.0458, max_reward_err_index: 1, max_kl_dist:  0.0642, max_kl_dist_index: 0, max_train_grp_loss:  23.7844, max_train_grp_loss_index: 0, max_val_grp_loss:  24.0159, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.8092, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:23:18,988 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.8103, val_loss:  19.3566, grad_norm: 0.4499, reward_err: 0.0753, 0.0180, 0.0368, KL_dist: 0.4828, 0.1987, 0.3336, param: [5.50973866 4.04702258 5.17589632 1.92951557]train_grp_loss: [22.27059487 12.28038956 21.04224933], val_grp_loss: [23.0163298  13.60133707 21.41334453], train_hist_grp_loss: [2291.46530805 1531.94014303 2214.88123648], cur_train_grp_loss: [22.27929834 12.31797415 21.05871935],max_reward_err:  0.0753, max_reward_err_index: 0, max_kl_dist:  0.4828, max_kl_dist_index: 0, max_train_grp_loss:  22.2706, max_train_grp_loss_index: 0, max_val_grp_loss:  23.0163, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2793, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:23:19,217 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [5.50973866 4.04702258 5.17589632 1.92951557].
2024-09-17 23:23:19,546 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8110, 3.8110, 3.2056
2024-09-17 23:23:19,547 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8554, 3.8554, 3.3544
2024-09-17 23:23:19,547 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5650, 3.7858, 3.2310
2024-09-17 23:23:19,548 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0753, 0.0180, 0.0368
2024-09-17 23:23:20,234 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8554, 3.8554, 3.3544
Known param reward: [[3.8553552627563477, 3.4600868225097656, 3.3136484622955322], [3.4600868225097656, 3.8553552627563477, 3.155968189239502], [3.8124771118164062, 3.5852954387664795, 3.354396104812622]], Known param reward error: [[0.0, 0.10252451805543565, 0.012147534531961908], [0.10252451805543565, 0.0, 0.059154586808764606], [0.011121712012938103, 0.07004797368447742, 0.0]].
