2024-09-17 23:22:00,053 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2023
2024-09-17 23:22:00,055 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2023
2024-09-17 23:22:00,056 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:22:00,220 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4946, l2 distance: 13.2114, acc: 0.75.
2024-09-17 23:22:00,221 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:22:00,221 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.84205766 5.03735432 6.87980734 4.5148032 ]
2024-09-17 23:22:00,424 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5857, 3.7393, 3.1731
2024-09-17 23:22:01,698 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.5497, val_loss:  21.4362, grad_norm: 0.7951, reward_err: 0.1252, 0.0043, 0.0845, KL_dist: 0.3396, 0.0156, 0.2152, param: [4.4782965  1.16197253 2.3399102  0.31134404]train_grp_loss: [24.65104815 17.01259982 23.36662515], val_grp_loss: [23.96259242 16.9448681  23.16061393], train_hist_grp_loss: [24.67652468 17.13269264 23.40499165], cur_train_grp_loss: [24.67652468 17.13269264 23.40499165],max_reward_err:  0.1252, max_reward_err_index: 0, max_kl_dist:  0.3396, max_kl_dist_index: 0, max_train_grp_loss:  24.6510, max_train_grp_loss_index: 0, max_val_grp_loss:  23.9626, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  24.6765, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:22:04,842 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.4996, val_loss:  18.9799, grad_norm: 0.3573, reward_err: 0.0721, 0.0141, 0.0360, KL_dist: 0.5519, 0.2675, 0.4114, param: [6.55871954 3.97834508 4.93863129 3.4252522 ]train_grp_loss: [23.23465422 11.80667504 21.00959823], val_grp_loss: [22.14691773 13.67730854 20.84207383], train_hist_grp_loss: [2381.11842165 1367.49988593 2201.84092633], cur_train_grp_loss: [23.24166613 11.82367722 21.02356436],max_reward_err:  0.0721, max_reward_err_index: 0, max_kl_dist:  0.5519, max_kl_dist_index: 0, max_train_grp_loss:  23.2347, max_train_grp_loss_index: 0, max_val_grp_loss:  22.1469, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.2417, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:22:05,057 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.55871954 3.97834508 4.93863129 3.4252522 ].
2024-09-17 23:22:05,372 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7839, 3.7839, 3.1364
2024-09-17 23:22:05,372 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8185, 3.8185, 3.2590
2024-09-17 23:22:05,373 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5433, 3.7646, 3.1416
2024-09-17 23:22:05,373 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0721, 0.0141, 0.0360
2024-09-17 23:22:06,045 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8185, 3.8185, 3.2590
Known param reward: [[3.8185489177703857, 3.427854061126709, 3.2254443168640137], [3.427854061126709, 3.8185489177703857, 3.064500570297241], [3.7794888019561768, 3.5508782863616943, 3.258955478668213]], Known param reward error: [[0.0, 0.10231500631705924, 0.010282792147223106], [0.10231500631705924, 0.0, 0.059667862799536175], [0.010229046859249302, 0.07009747345726862, 0.0]].
