2024-09-17 23:21:40,542 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2022
2024-09-17 23:21:40,543 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2022
2024-09-17 23:21:40,544 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:21:40,707 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4993, l2 distance: 13.7379, acc: 0.76.
2024-09-17 23:21:40,708 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:21:40,709 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [6.5702886  3.90633307 7.4250801  3.98010948]
2024-09-17 23:21:40,911 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.6248, 3.8416, 3.2262
2024-09-17 23:21:42,236 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.6983, val_loss:  20.7926, grad_norm: 0.6382, reward_err: 0.0778, 0.0313, 0.0446, KL_dist: 0.2506, 0.0732, 0.1460, param: [4.15418066 2.9467504  3.06848926 0.60089115]train_grp_loss: [23.11974073 16.41389439 22.6380873 ], val_grp_loss: [23.23264794 16.14821854 22.91318875], train_hist_grp_loss: [23.14357016 16.48630488 22.66307687], cur_train_grp_loss: [23.14357016 16.48630488 22.66307687],max_reward_err:  0.0778, max_reward_err_index: 0, max_kl_dist:  0.2506, max_kl_dist_index: 0, max_train_grp_loss:  23.1197, max_train_grp_loss_index: 0, max_val_grp_loss:  23.2326, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.1436, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:21:45,387 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.5438, val_loss:  18.6845, grad_norm: 0.3244, reward_err: 0.0631, 0.0100, 0.0305, KL_dist: 0.6001, 0.2822, 0.4474, param: [6.51091578 4.00028331 5.86319452 3.06364791]train_grp_loss: [21.72279942 12.96549041 21.04433984], val_grp_loss: [22.09927303 12.39200714 21.45295148], train_hist_grp_loss: [2230.56988195 1427.47825973 2173.6602393 ], cur_train_grp_loss: [21.73031117 12.97948757 21.05419124],max_reward_err:  0.0631, max_reward_err_index: 0, max_kl_dist:  0.6001, max_kl_dist_index: 0, max_train_grp_loss:  21.7228, max_train_grp_loss_index: 0, max_val_grp_loss:  22.0993, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.7303, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:21:45,631 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.51091578 4.00028331 5.86319452 3.06364791].
2024-09-17 23:21:45,959 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8444, 3.8444, 3.1998
2024-09-17 23:21:45,960 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8757, 3.8757, 3.3316
2024-09-17 23:21:45,961 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6310, 3.8371, 3.2300
2024-09-17 23:21:45,961 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0631, 0.0100, 0.0305
2024-09-17 23:21:46,634 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8757, 3.8757, 3.3316
Known param reward: [[3.8757123947143555, 3.5474281311035156, 3.28242826461792], [3.5474281311035156, 3.8757123947143555, 3.1710550785064697], [3.833740711212158, 3.6443803310394287, 3.3315517902374268]], Known param reward error: [[0.0, 0.08470294752999462, 0.014744938308765115], [0.08470294752999462, 0.0, 0.048174761143220605], [0.010829411274024791, 0.059687623877977714, 0.0]].
