2024-09-17 23:24:10,191 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2030
2024-09-17 23:24:10,192 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2030
2024-09-17 23:24:10,193 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:24:10,350 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4482, l2 distance: 19.4925, acc: 0.79.
2024-09-17 23:24:10,351 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:24:10,352 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [ 5.97129893  4.34120631 11.87224408  3.23095933]
2024-09-17 23:24:10,558 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.3778, 3.7898, 3.0477
2024-09-17 23:24:11,826 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.8175, val_loss:  20.0997, grad_norm: 0.6228, reward_err: 0.0510, 0.0280, 0.0198, KL_dist: 0.2326, 0.1622, 0.1502, param: [4.29155474 3.15461275 3.45450334 3.57947795]train_grp_loss: [22.7710557  15.80486896 21.36488428], val_grp_loss: [22.53886256 15.5934398  21.96852086], train_hist_grp_loss: [22.77324583 15.88552582 21.3921267 ], cur_train_grp_loss: [22.77324583 15.88552582 21.3921267 ],max_reward_err:  0.0510, max_reward_err_index: 0, max_kl_dist:  0.2326, max_kl_dist_index: 0, max_train_grp_loss:  22.7711, max_train_grp_loss_index: 0, max_val_grp_loss:  22.5389, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.7732, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:24:14,962 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.4374, val_loss:  18.3807, grad_norm: 0.3737, reward_err: 0.0806, 0.0034, 0.0423, KL_dist: 0.8214, 0.3420, 0.6184, param: [6.78590588 3.60235151 7.53421982 3.38070602]train_grp_loss: [22.80543387 10.89834384 19.48881381], val_grp_loss: [22.26180143 11.58864107 20.96909995], train_hist_grp_loss: [2275.68874593 1298.34921824 2033.02712881], cur_train_grp_loss: [22.8036588  10.92654012 19.50164963],max_reward_err:  0.0806, max_reward_err_index: 0, max_kl_dist:  0.8214, max_kl_dist_index: 0, max_train_grp_loss:  22.8054, max_train_grp_loss_index: 0, max_val_grp_loss:  22.2618, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.8037, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:24:15,181 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.78590588 3.60235151 7.53421982 3.38070602].
2024-09-17 23:24:15,508 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7589, 3.7589, 3.1455
2024-09-17 23:24:15,508 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.7968, 3.7968, 3.2861
2024-09-17 23:24:15,509 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.4908, 3.7838, 3.1472
2024-09-17 23:24:15,509 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0806, 0.0034, 0.0423
2024-09-17 23:24:16,180 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.7968, 3.7968, 3.2861
Known param reward: [[3.7967944145202637, 3.4310121536254883, 3.2510554790496826], [3.4310121536254883, 3.7967944145202637, 3.104628562927246], [3.762887954711914, 3.552025556564331, 3.2860965728759766]], Known param reward error: [[0.0, 0.09633975953396283, 0.010663440056974997], [0.09633975953396283, 0.0, 0.05522296923547518], [0.008930285948240838, 0.06446724031721372, 0.0]].
