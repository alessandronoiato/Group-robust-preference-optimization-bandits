2024-09-17 23:22:36,251 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2025
2024-09-17 23:22:36,253 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2025
2024-09-17 23:22:36,253 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:22:36,417 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4874, l2 distance: 14.5203, acc: 0.75.
2024-09-17 23:22:36,418 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:22:36,419 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.86893008 3.81025835 8.78907709 2.02187181]
2024-09-17 23:22:36,626 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4656, 3.8462, 3.0516
2024-09-17 23:22:37,938 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.7649, val_loss:  20.6197, grad_norm: 0.6794, reward_err: 0.0517, 0.0568, 0.0239, KL_dist: 0.1919, 0.1913, 0.1277, param: [4.42030755 1.81171928 2.50723874 4.5387899 ]train_grp_loss: [22.40443702 17.83354429 22.56422812], val_grp_loss: [22.70163177 17.15187116 22.0955431 ], train_hist_grp_loss: [22.41187013 17.93974997 22.57794539], cur_train_grp_loss: [22.41187013 17.93974997 22.57794539],max_reward_err:  0.0568, max_reward_err_index: 1, max_kl_dist:  0.1919, max_kl_dist_index: 0, max_train_grp_loss:  22.5642, max_train_grp_loss_index: 2, max_val_grp_loss:  22.7016, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.5779, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:22:41,079 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.1879, val_loss:  18.5248, grad_norm: 0.3656, reward_err: 0.0778, 0.0043, 0.0415, KL_dist: 0.6767, 0.3004, 0.5086, param: [7.13172754 3.30857859 6.19545178 3.28866075]train_grp_loss: [22.06354857 11.9079045  21.69844416], val_grp_loss: [22.32996419 12.25303827 21.15600039], train_hist_grp_loss: [2218.593043   1433.66908562 2207.2972116 ], cur_train_grp_loss: [22.06446288 11.93853905 21.70373961],max_reward_err:  0.0778, max_reward_err_index: 0, max_kl_dist:  0.6767, max_kl_dist_index: 0, max_train_grp_loss:  22.0635, max_train_grp_loss_index: 0, max_val_grp_loss:  22.3300, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.0645, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:22:41,312 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [7.13172754 3.30857859 6.19545178 3.28866075].
2024-09-17 23:22:41,643 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8189, 3.8189, 3.1471
2024-09-17 23:22:41,644 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8509, 3.8509, 3.2572
2024-09-17 23:22:41,644 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5514, 3.8342, 3.1220
2024-09-17 23:22:41,645 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0778, 0.0043, 0.0415
2024-09-17 23:22:42,338 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8509, 3.8509, 3.2572
Known param reward: [[3.850886821746826, 3.4759232997894287, 3.2200934886932373], [3.4759232997894287, 3.850886821746826, 3.068084716796875], [3.8119678497314453, 3.597700595855713, 3.257154703140259]], Known param reward error: [[0.0, 0.09737069389832334, 0.011378401649541042], [0.09737069389832334, 0.0, 0.058047591709751864], [0.010106495936363709, 0.06574751157611633, 0.0]].
