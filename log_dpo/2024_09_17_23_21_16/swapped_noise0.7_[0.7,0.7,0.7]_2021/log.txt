2024-09-17 23:21:21,513 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2021
2024-09-17 23:21:21,515 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2021
2024-09-17 23:21:21,515 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:21:21,683 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5223, l2 distance: 11.7253, acc: 0.75.
2024-09-17 23:21:21,684 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:21:21,685 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.4903206  2.66270796 6.9133025  4.04088463]
2024-09-17 23:21:21,892 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5535, 3.8556, 3.1690
2024-09-17 23:21:23,184 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.8938, val_loss:  21.7552, grad_norm: 0.6808, reward_err: 0.0413, 0.0861, 0.0318, KL_dist: 0.0889, 0.2146, 0.0882, param: [0.18668365 3.51525408 3.11036256 3.16805277]train_grp_loss: [22.95233065 20.03216182 22.83730235], val_grp_loss: [22.76015425 19.70533884 22.8891147 ], train_hist_grp_loss: [22.96604454 20.14134791 22.84803814], cur_train_grp_loss: [22.96604454 20.14134791 22.84803814],max_reward_err:  0.0861, max_reward_err_index: 1, max_kl_dist:  0.2146, max_kl_dist_index: 1, max_train_grp_loss:  22.9523, max_train_grp_loss_index: 0, max_val_grp_loss:  22.8891, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.9660, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:21:26,378 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  19.3758, val_loss:  19.5931, grad_norm: 0.3593, reward_err: 0.0697, 0.0368, 0.0373, KL_dist: 0.4197, 0.2893, 0.3269, param: [3.36203758 3.13499188 6.32058061 5.05066212]train_grp_loss: [22.12853641 14.15663862 22.24677728], val_grp_loss: [21.77333927 15.84252465 21.33355943], train_hist_grp_loss: [2247.63664104 1653.89517712 2248.46779066], cur_train_grp_loss: [22.13315543 14.18651099 22.24954092],max_reward_err:  0.0697, max_reward_err_index: 0, max_kl_dist:  0.4197, max_kl_dist_index: 0, max_train_grp_loss:  22.2468, max_train_grp_loss_index: 2, max_val_grp_loss:  21.7733, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2495, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:21:26,611 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [3.36203758 3.13499188 6.32058061 5.05066212].
2024-09-17 23:21:26,948 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8605, 3.8605, 3.2147
2024-09-17 23:21:26,948 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8953, 3.8953, 3.3285
2024-09-17 23:21:26,949 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6238, 3.7522, 3.2045
2024-09-17 23:21:26,949 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0697, 0.0368, 0.0373
2024-09-17 23:21:27,632 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8953, 3.8953, 3.3285
Known param reward: [[3.8953495025634766, 3.470022678375244, 3.3036766052246094], [3.470022678375244, 3.8953495025634766, 3.1119751930236816], [3.860280990600586, 3.6015567779541016, 3.328472137451172]], Known param reward error: [[0.0, 0.10918836009665644, 0.007449523746216501], [0.10918836009665644, 0.0, 0.06504394072929691], [0.009002661234842346, 0.07542140298734017, 0.0]].
