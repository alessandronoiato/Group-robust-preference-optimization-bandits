2024-09-17 23:23:51,122 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_21_16/swapped_noise0.7_[0.7,0.7,0.7]_2029
2024-09-17 23:23:51,124 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2029
2024-09-17 23:23:51,125 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:23:51,295 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5036, l2 distance: 12.2568, acc: 0.76.
2024-09-17 23:23:51,296 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:23:51,297 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [4.87068676 3.15664954 7.32045632 4.78837788]
2024-09-17 23:23:51,505 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5801, 3.7810, 3.2295
2024-09-17 23:23:52,803 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.2051, val_loss:  20.0916, grad_norm: 0.4434, reward_err: 0.0600, 0.0401, 0.0273, KL_dist: 0.2925, 0.2046, 0.1982, param: [3.66935591 2.56952399 4.8071062  4.36419318]train_grp_loss: [22.91405033 14.36584465 21.11842044], val_grp_loss: [21.76877978 15.69518104 22.09151045], train_hist_grp_loss: [22.92270746 14.39491747 21.13783677], cur_train_grp_loss: [22.92270746 14.39491747 21.13783677],max_reward_err:  0.0600, max_reward_err_index: 0, max_kl_dist:  0.2925, max_kl_dist_index: 0, max_train_grp_loss:  22.9141, max_train_grp_loss_index: 0, max_val_grp_loss:  22.0915, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.9227, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:23:55,948 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.1592, val_loss:  19.1379, grad_norm: 0.2274, reward_err: 0.0674, 0.0313, 0.0325, KL_dist: 0.6254, 0.4121, 0.4847, param: [5.3394399  3.53985061 7.01095131 5.54298458]train_grp_loss: [22.46154088 12.99787055 19.86043241], val_grp_loss: [20.83728139 14.62909665 21.20914324], train_hist_grp_loss: [2263.90638359 1351.47538844 2041.05764061], cur_train_grp_loss: [22.46346209 13.00335501 19.86837945],max_reward_err:  0.0674, max_reward_err_index: 0, max_kl_dist:  0.6254, max_kl_dist_index: 0, max_train_grp_loss:  22.4615, max_train_grp_loss_index: 0, max_val_grp_loss:  21.2091, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.4635, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:23:56,184 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [5.3394399  3.53985061 7.01095131 5.54298458].
2024-09-17 23:23:56,510 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8223, 3.8223, 3.2131
2024-09-17 23:23:56,511 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8685, 3.8685, 3.3593
2024-09-17 23:23:56,512 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6076, 3.7475, 3.2500
2024-09-17 23:23:56,512 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0674, 0.0313, 0.0325
2024-09-17 23:23:57,185 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8685, 3.8685, 3.3593
Known param reward: [[3.8685142993927, 3.4599270820617676, 3.3229901790618896], [3.4599270820617676, 3.8685142993927, 3.1513752937316895], [3.8328397274017334, 3.5916991233825684, 3.3593173027038574]], Known param reward error: [[0.0, 0.10561863953690821, 0.010813841137521808], [0.10561863953690821, 0.0, 0.06190007975870543], [0.00922177591448148, 0.0715559397191805, 0.0]].
