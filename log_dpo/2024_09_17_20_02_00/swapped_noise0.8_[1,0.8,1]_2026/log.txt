2024-09-17 20:03:37,957 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_20_02_00/swapped_noise0.8_[1,0.8,1]_2026
2024-09-17 20:03:37,958 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2026
2024-09-17 20:03:37,959 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 20:03:38,116 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.3977, l2 distance: 25.9068, acc: 0.83.
2024-09-17 20:03:38,117 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 20:03:38,118 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [11.60729834  4.46739144 11.40637411  7.49257033]
2024-09-17 20:03:38,315 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5574, 3.8264, 3.1706
2024-09-17 20:03:39,575 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.5382, val_loss:  22.1094, grad_norm: 0.9487, reward_err: 0.0547, 0.0758, 0.0305, KL_dist: 0.0683, 0.1048, 0.0353, param: [2.20294827 2.62484919 0.76793443 0.81302547]train_grp_loss: [23.11183466 21.59100851 22.91504102], val_grp_loss: [23.38916201 19.76852798 22.91215493], train_hist_grp_loss: [23.15267726 21.77179231 22.96151518], cur_train_grp_loss: [23.15267726 21.77179231 22.96151518],max_reward_err:  0.0758, max_reward_err_index: 1, max_kl_dist:  0.1048, max_kl_dist_index: 1, max_train_grp_loss:  23.1118, max_train_grp_loss_index: 0, max_val_grp_loss:  23.3892, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.1527, max_cur_train_grp_loss_index: 0, 
2024-09-17 20:03:42,574 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.3705, val_loss:  17.6746, grad_norm: 0.5285, reward_err: 0.0768, 0.0070, 0.0384, KL_dist: 0.5973, 0.2447, 0.4370, param: [6.70182719 2.76527773 5.33940053 3.75346563]train_grp_loss: [20.43356007 11.94884602 19.77429659], val_grp_loss: [21.22167338 11.31802698 19.78178529], train_hist_grp_loss: [2161.06529694 1580.39213159 2117.2086828 ], cur_train_grp_loss: [20.4507484  11.99419515 19.79526054],max_reward_err:  0.0768, max_reward_err_index: 0, max_kl_dist:  0.5973, max_kl_dist_index: 0, max_train_grp_loss:  20.4336, max_train_grp_loss_index: 0, max_val_grp_loss:  21.2217, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.4507, max_cur_train_grp_loss_index: 0, 
2024-09-17 20:03:42,788 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.70182719 2.76527773 5.33940053 3.75346563].
2024-09-17 20:03:43,107 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8147, 3.8147, 3.1704
2024-09-17 20:03:43,107 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8534, 3.8534, 3.2942
2024-09-17 20:03:43,108 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5576, 3.8265, 3.1679
2024-09-17 20:03:43,108 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0768, 0.0070, 0.0384
2024-09-17 20:03:43,767 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8534, 3.8534, 3.2942
Known param reward: [[3.85337495803833, 3.4755589962005615, 3.260967493057251], [3.4755589962005615, 3.85337495803833, 3.111621856689453], [3.811624526977539, 3.59085750579834, 3.2942490577697754]], Known param reward error: [[0.0, 0.0980480659037932, 0.01010292911339746], [0.0980480659037932, 0.0, 0.05543818875794469], [0.010834769913500776, 0.06812663057675347, 0.0]].
