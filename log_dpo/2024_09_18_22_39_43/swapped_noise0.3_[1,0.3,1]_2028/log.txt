2024-09-18 22:49:19,800 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.3_[1,0.3,1]_2028
2024-09-18 22:49:19,802 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2028
2024-09-18 22:49:19,802 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:49:19,965 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4759, l2 distance: 14.7351, acc: 0.82.
2024-09-18 22:49:19,966 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:49:19,967 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [7.47006007 5.63067938 6.50156715 4.14686146]
2024-09-18 22:49:20,173 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.6326, 3.7996, 3.2436
2024-09-18 22:49:21,603 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.5147, val_loss:  20.3389, grad_norm: 0.6882, reward_err: 0.0314, 0.0700, 0.0161, KL_dist: 0.1227, 0.2692, 0.1009, param: [1.44053254 2.67700393 3.65191677 4.81666409]train_grp_loss: [21.35155429 19.19900677 21.21464062], val_grp_loss: [21.23402418 18.44645974 21.22437389], train_hist_grp_loss: [21.37237284 19.28308248 21.24890643], cur_train_grp_loss: [21.37237284 19.28308248 21.24890643],max_reward_err:  0.0700, max_reward_err_index: 1, max_kl_dist:  0.2692, max_kl_dist_index: 1, max_train_grp_loss:  21.3516, max_train_grp_loss_index: 0, max_val_grp_loss:  21.2340, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.3724, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:49:24,816 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  17.9820, val_loss:  17.1347, grad_norm: 0.3458, reward_err: 0.0515, 0.0217, 0.0226, KL_dist: 0.5283, 0.3619, 0.4106, param: [4.91307943 4.72671201 6.48740894 4.46404962]train_grp_loss: [20.08024917 15.08116916 18.94704021], val_grp_loss: [19.65289512 12.28615298 19.15341718], train_hist_grp_loss: [2082.54836811 1681.29168433 2014.29538004], cur_train_grp_loss: [20.08745197 15.09723346 18.96147371],max_reward_err:  0.0515, max_reward_err_index: 0, max_kl_dist:  0.5283, max_kl_dist_index: 0, max_train_grp_loss:  20.0802, max_train_grp_loss_index: 0, max_val_grp_loss:  19.6529, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.0875, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:49:27,940 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.3298, val_loss:  16.0100, grad_norm: 0.1805, reward_err: 0.0654, 0.0137, 0.0317, KL_dist: 0.8225, 0.5069, 0.6517, param: [6.85060967 5.52470023 7.84159408 4.30698641]train_grp_loss: [19.63513653 14.35841785 17.96277174], val_grp_loss: [19.05586304 10.33265386 18.26573983], train_hist_grp_loss: [4045.02918926 3128.15967002 3835.58295741], cur_train_grp_loss: [19.6377574  14.36066688 17.96942283],max_reward_err:  0.0654, max_reward_err_index: 0, max_kl_dist:  0.8225, max_kl_dist_index: 0, max_train_grp_loss:  19.6351, max_train_grp_loss_index: 0, max_val_grp_loss:  19.0559, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.6378, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:49:28,162 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [6.85060967 5.52470023 7.84159408 4.30698641].
2024-09-18 22:49:28,490 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8327, 3.8327, 3.2044
2024-09-18 22:49:28,490 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8689, 3.8689, 3.3386
2024-09-18 22:49:28,491 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.6158, 3.8159, 3.2328
2024-09-18 22:49:28,491 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0654, 0.0137, 0.0317
2024-09-18 22:49:29,165 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8689, 3.8689, 3.3386
Known param reward: [[3.8689029216766357, 3.4624440670013428, 3.304152727127075], [3.4624440670013428, 3.8689029216766357, 3.1259868144989014], [3.8354125022888184, 3.6089484691619873, 3.338573932647705]], Known param reward error: [[0.0, 0.10505790993048467, 0.010310152243156], [0.10505790993048467, 0.0, 0.06367602528430706], [0.008656309053447097, 0.0671907405735562, 0.0]].
