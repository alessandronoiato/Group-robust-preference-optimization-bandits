2024-09-18 22:42:40,755 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.5_[1,0.5,1]_2029
2024-09-18 22:42:40,757 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2029
2024-09-18 22:42:40,757 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:42:40,920 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4622, l2 distance: 15.7048, acc: 0.76.
2024-09-18 22:42:40,921 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:42:40,921 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [8.03172364 3.1446329  7.65537172 4.62681821]
2024-09-18 22:42:41,132 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5601, 3.8681, 3.2290
2024-09-18 22:42:42,577 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.7960, val_loss:  19.7373, grad_norm: 0.6727, reward_err: 0.0335, 0.0583, 0.0085, KL_dist: 0.1720, 0.2450, 0.1115, param: [3.25491061 4.88390597 3.63107444 2.47203797]train_grp_loss: [21.72458371 17.52242759 20.93726601], val_grp_loss: [21.46644513 16.62724088 20.69712925], train_hist_grp_loss: [21.72982618 17.61532803 20.9561931 ], cur_train_grp_loss: [21.72982618 17.61532803 20.9561931 ],max_reward_err:  0.0583, max_reward_err_index: 1, max_kl_dist:  0.2450, max_kl_dist_index: 1, max_train_grp_loss:  21.7246, max_train_grp_loss_index: 0, max_val_grp_loss:  21.4664, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.7298, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:42:45,907 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  17.3274, val_loss:  16.9228, grad_norm: 0.3463, reward_err: 0.0740, 0.0080, 0.0353, KL_dist: 0.7169, 0.3360, 0.5383, param: [6.4113763  3.89183817 6.95605213 3.83851761]train_grp_loss: [21.48280518 12.46495617 19.67027819], val_grp_loss: [20.96945262  9.7106401  19.11169752], train_hist_grp_loss: [2178.56573761 1464.59963262 2043.17981575], cur_train_grp_loss: [21.48349474 12.48928514 19.67844388],max_reward_err:  0.0740, max_reward_err_index: 0, max_kl_dist:  0.7169, max_kl_dist_index: 0, max_train_grp_loss:  21.4828, max_train_grp_loss_index: 0, max_val_grp_loss:  20.9695, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.4835, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:42:49,049 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  16.6646, val_loss:  15.9685, grad_norm: 0.1829, reward_err: 0.0872, 0.0032, 0.0458, KL_dist: 1.0826, 0.5144, 0.8420, param: [8.0966264  3.57525722 8.71563577 4.41089145]train_grp_loss: [21.47584634 11.11898692 19.11066773], val_grp_loss: [20.82719609  7.56197005 18.37722455], train_hist_grp_loss: [4304.27643749 2618.83001456 3959.60455403], cur_train_grp_loss: [21.47557721 11.12575227 19.11445866],max_reward_err:  0.0872, max_reward_err_index: 0, max_kl_dist:  1.0826, max_kl_dist_index: 0, max_train_grp_loss:  21.4758, max_train_grp_loss_index: 0, max_val_grp_loss:  20.8272, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.4756, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:42:49,271 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [8.0966264  3.57525722 8.71563577 4.41089145].
2024-09-18 22:42:49,599 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8386, 3.8386, 3.2274
2024-09-18 22:42:49,600 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8850, 3.8850, 3.3731
2024-09-18 22:42:49,600 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5462, 3.8725, 3.2187
2024-09-18 22:42:49,601 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0872, 0.0032, 0.0458
2024-09-18 22:42:50,279 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8850, 3.8850, 3.3731
Known param reward: [[3.8849852085113525, 3.4774763584136963, 3.335669994354248], [3.4774763584136963, 3.8849852085113525, 3.1690282821655273], [3.8491885662078857, 3.6059226989746094, 3.3731210231781006]], Known param reward error: [[0.0, 0.10489328227167315, 0.011102782428057319], [0.10489328227167315, 0.0, 0.06050560878491111], [0.009214100024124247, 0.07183103527018941, 0.0]].
