2024-09-18 22:44:05,069 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.4_[1,0.4,1]_2023
2024-09-18 22:44:05,071 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2023
2024-09-18 22:44:05,071 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:44:05,235 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4814, l2 distance: 15.1900, acc: 0.79.
2024-09-18 22:44:05,236 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:44:05,236 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [9.60038822 4.4139189  4.42858643 3.54905581]
2024-09-18 22:44:05,445 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4934, 3.8064, 3.1099
2024-09-18 22:44:06,891 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.1461, val_loss:  20.9725, grad_norm: 0.7480, reward_err: 0.0207, 0.1076, 0.0203, KL_dist: 0.0385, 0.3004, 0.0492, param: [0.41407518 1.69874928 2.18441397 4.75162389]train_grp_loss: [22.26014165 22.07849546 22.01334173], val_grp_loss: [22.11117776 18.76905211 21.99990603], train_hist_grp_loss: [22.28888014 22.17125101 22.05185905], cur_train_grp_loss: [22.28888014 22.17125101 22.05185905],max_reward_err:  0.1076, max_reward_err_index: 1, max_kl_dist:  0.3004, max_kl_dist_index: 1, max_train_grp_loss:  22.2601, max_train_grp_loss_index: 0, max_val_grp_loss:  22.1112, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2889, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:44:10,129 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.7819, val_loss:  17.1472, grad_norm: 0.4330, reward_err: 0.0505, 0.0253, 0.0194, KL_dist: 0.3894, 0.2765, 0.2881, param: [4.94260685 3.59864945 5.14184627 4.58544136]train_grp_loss: [20.35440583 16.80280136 19.39165279], val_grp_loss: [20.79445866 10.94984496 19.63429295], train_hist_grp_loss: [2140.43825585 1915.26106814 2076.24812095], cur_train_grp_loss: [20.36656459 16.82926159 19.40893021],max_reward_err:  0.0505, max_reward_err_index: 0, max_kl_dist:  0.3894, max_kl_dist_index: 0, max_train_grp_loss:  20.3544, max_train_grp_loss_index: 0, max_val_grp_loss:  20.7945, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.3666, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:44:13,266 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.6406, val_loss:  15.7687, grad_norm: 0.2578, reward_err: 0.0716, 0.0119, 0.0356, KL_dist: 0.7896, 0.4477, 0.6209, param: [7.76175954 4.52726047 6.63567501 4.42109306]train_grp_loss: [19.52051909 15.35438299 18.18327562], val_grp_loss: [20.33549279  8.34969207 18.56415055], train_hist_grp_loss: [4109.49102957 3492.23507075 3929.69618651], cur_train_grp_loss: [19.52622328 15.36125597 18.19169611],max_reward_err:  0.0716, max_reward_err_index: 0, max_kl_dist:  0.7896, max_kl_dist_index: 0, max_train_grp_loss:  19.5205, max_train_grp_loss_index: 0, max_val_grp_loss:  20.3355, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.5262, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:44:13,488 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.76175954 4.52726047 6.63567501 4.42109306].
2024-09-18 22:44:13,815 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8122, 3.8122, 3.1725
2024-09-18 22:44:13,815 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8527, 3.8527, 3.3024
2024-09-18 22:44:13,816 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5767, 3.8070, 3.1849
2024-09-18 22:44:13,816 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0716, 0.0119, 0.0356
2024-09-18 22:44:14,493 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8527, 3.8527, 3.3024
Known param reward: [[3.852689266204834, 3.475465774536133, 3.27390193939209], [3.475465774536133, 3.852689266204834, 3.1151528358459473], [3.8164188861846924, 3.5882885456085205, 3.3024346828460693]], Known param reward error: [[0.0, 0.09791173531113566, 0.008639911518065122], [0.09791173531113566, 0.0, 0.0567102350193103], [0.00941430193664968, 0.06862757474774667, 0.0]].
