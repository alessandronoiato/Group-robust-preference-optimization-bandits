2024-09-18 22:42:19,811 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.5_[1,0.5,1]_2028
2024-09-18 22:42:19,813 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2028
2024-09-18 22:42:19,813 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:42:19,974 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5129, l2 distance: 11.5183, acc: 0.77.
2024-09-18 22:42:19,974 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:42:19,975 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [4.82968347 2.91674126 7.37770799 3.19335816]
2024-09-18 22:42:20,186 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5266, 3.8458, 3.1653
2024-09-18 22:42:21,590 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.1623, val_loss:  19.0576, grad_norm: 0.5149, reward_err: 0.0617, 0.0437, 0.0332, KL_dist: 0.2401, 0.1762, 0.1582, param: [4.68298593 4.29243877 2.65253424 1.88839086]train_grp_loss: [21.92766466 17.69636749 21.02451266], val_grp_loss: [21.52783542 14.44027136 20.74828344], train_hist_grp_loss: [21.94026069 17.73950184 21.04938936], cur_train_grp_loss: [21.94026069 17.73950184 21.04938936],max_reward_err:  0.0617, max_reward_err_index: 0, max_kl_dist:  0.2401, max_kl_dist_index: 0, max_train_grp_loss:  21.9277, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5278, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.9403, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:42:24,787 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.6656, val_loss:  16.7608, grad_norm: 0.2779, reward_err: 0.0753, 0.0116, 0.0397, KL_dist: 0.6079, 0.2918, 0.4502, param: [6.68249133 4.26375554 5.63101192 3.17367514]train_grp_loss: [21.23212823 15.35004163 19.38526661], val_grp_loss: [20.52284703 10.08085011 18.99448624], train_hist_grp_loss: [2172.76663399 1645.889314   2030.58067705], cur_train_grp_loss: [21.23542364 15.36144004 19.39568325],max_reward_err:  0.0753, max_reward_err_index: 0, max_kl_dist:  0.6079, max_kl_dist_index: 0, max_train_grp_loss:  21.2321, max_train_grp_loss_index: 0, max_val_grp_loss:  20.5228, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.2354, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:42:27,925 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.2146, val_loss:  15.8583, grad_norm: 0.1580, reward_err: 0.0811, 0.0052, 0.0436, KL_dist: 0.8660, 0.4140, 0.6630, param: [7.62821663 4.08257533 7.40374113 3.70707678]train_grp_loss: [21.06298803 14.70771158 18.6705819 ], val_grp_loss: [20.18089518  8.39368055 18.22056072], train_hist_grp_loss: [4264.40117071 3127.72023805 3910.27772607], cur_train_grp_loss: [21.0636808  14.7110593  18.67547495],max_reward_err:  0.0811, max_reward_err_index: 0, max_kl_dist:  0.8660, max_kl_dist_index: 0, max_train_grp_loss:  21.0630, max_train_grp_loss_index: 0, max_val_grp_loss:  20.1809, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.0637, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:42:28,147 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.62821663 4.08257533 7.40374113 3.70707678].
2024-09-18 22:42:28,476 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8289, 3.8289, 3.2018
2024-09-18 22:42:28,477 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8652, 3.8652, 3.3372
2024-09-18 22:42:28,477 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5517, 3.8452, 3.1919
2024-09-18 22:42:28,478 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0811, 0.0052, 0.0436
2024-09-18 22:42:29,150 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8652, 3.8652, 3.3372
Known param reward: [[3.8651840686798096, 3.4605093002319336, 3.3015952110290527], [3.4605093002319336, 3.8651840686798096, 3.1258203983306885], [3.8319525718688965, 3.6041877269744873, 3.337219715118408]], Known param reward error: [[0.0, 0.10469741188447372, 0.010674905199669023], [0.10469741188447372, 0.0, 0.0633459390851702], [0.00859764922457202, 0.06752494501367125, 0.0]].
