2024-09-18 22:47:16,138 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.3_[1,0.3,1]_2022
2024-09-18 22:47:16,140 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2022
2024-09-18 22:47:16,141 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:47:16,302 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5114, l2 distance: 13.3000, acc: 0.75.
2024-09-18 22:47:16,302 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:47:16,303 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [2.3431996  2.79147837 9.27451545 2.39716811]
2024-09-18 22:47:16,508 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4661, 3.8464, 3.0767
2024-09-18 22:47:17,917 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.2039, val_loss:  20.2436, grad_norm: 0.6724, reward_err: 0.0553, 0.0570, 0.0274, KL_dist: 0.1643, 0.1467, 0.0885, param: [2.49354098 0.41187281 3.64996414 3.88546569]train_grp_loss: [22.2097394  20.17243189 21.38126059], val_grp_loss: [22.59344864 16.12956265 22.02014336], train_hist_grp_loss: [22.23254432 20.24227057 21.41696646], cur_train_grp_loss: [22.23254432 20.24227057 21.41696646],max_reward_err:  0.0570, max_reward_err_index: 1, max_kl_dist:  0.1643, max_kl_dist_index: 0, max_train_grp_loss:  22.2097, max_train_grp_loss_index: 0, max_val_grp_loss:  22.5934, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2325, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:21,156 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.7461, val_loss:  17.3932, grad_norm: 0.3490, reward_err: 0.0808, 0.0072, 0.0473, KL_dist: 0.6380, 0.2394, 0.4779, param: [4.81192384 2.50519277 7.37205679 3.55355167]train_grp_loss: [20.71960501 16.78788359 18.91386274], val_grp_loss: [21.42040821 10.45014071 20.32989469], train_hist_grp_loss: [2158.504095   1825.12641988 2021.8220556 ], cur_train_grp_loss: [20.72897068 16.80105646 18.93049986],max_reward_err:  0.0808, max_reward_err_index: 0, max_kl_dist:  0.6380, max_kl_dist_index: 0, max_train_grp_loss:  20.7196, max_train_grp_loss_index: 0, max_val_grp_loss:  21.4204, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.7290, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:24,275 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.0383, val_loss:  16.3780, grad_norm: 0.1991, reward_err: 0.0888, 0.0031, 0.0550, KL_dist: 1.0020, 0.4137, 0.7936, param: [5.82203638 3.26069737 9.65885998 3.5296379 ]train_grp_loss: [20.08147817 16.18873234 17.72427215], val_grp_loss: [21.03335831  8.50012681 19.62421761], train_hist_grp_loss: [4174.45298557 3449.00444854 3829.49000534], cur_train_grp_loss: [20.08581696 16.19064779 17.7328058 ],max_reward_err:  0.0888, max_reward_err_index: 0, max_kl_dist:  1.0020, max_kl_dist_index: 0, max_train_grp_loss:  20.0815, max_train_grp_loss_index: 0, max_val_grp_loss:  21.0334, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.0858, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:24,495 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [5.82203638 3.26069737 9.65885998 3.5296379 ].
2024-09-18 22:47:24,821 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8377, 3.8377, 3.1941
2024-09-18 22:47:24,822 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8690, 3.8690, 3.3258
2024-09-18 22:47:24,822 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5252, 3.8568, 3.1431
2024-09-18 22:47:24,823 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0888, 0.0031, 0.0550
2024-09-18 22:47:25,494 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8690, 3.8690, 3.3258
Known param reward: [[3.8689613342285156, 3.540985584259033, 3.2756898403167725], [3.540985584259033, 3.8689613342285156, 3.1665854454040527], [3.825187921524048, 3.636335849761963, 3.3258354663848877]], Known param reward error: [[0.0, 0.08477100741945821, 0.01507760277829452], [0.08477100741945821, 0.0, 0.04788271175481099], [0.011313995908205773, 0.060126081490793466, 0.0]].
