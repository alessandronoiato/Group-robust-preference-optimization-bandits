2024-09-18 22:45:54,059 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.4_[1,0.4,1]_2028
2024-09-18 22:45:54,062 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2028
2024-09-18 22:45:54,062 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:45:54,224 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5013, l2 distance: 13.1473, acc: 0.80.
2024-09-18 22:45:54,225 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:45:54,226 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [6.95011815 4.29889072 6.20277518 4.38213871]
2024-09-18 22:45:54,431 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5639, 3.7646, 3.1600
2024-09-18 22:45:55,868 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.7014, val_loss:  19.4034, grad_norm: 0.6957, reward_err: 0.0711, 0.0475, 0.0387, KL_dist: 0.3070, 0.2092, 0.1963, param: [4.84322732 4.38199473 3.10809576 0.06232509]train_grp_loss: [22.61384091 18.23174896 21.23569942], val_grp_loss: [22.20431221 14.15445683 21.22049989], train_hist_grp_loss: [22.64666487 18.30421035 21.2706996 ], cur_train_grp_loss: [22.64666487 18.30421035 21.2706996 ],max_reward_err:  0.0711, max_reward_err_index: 0, max_kl_dist:  0.3070, max_kl_dist_index: 0, max_train_grp_loss:  22.6138, max_train_grp_loss_index: 0, max_val_grp_loss:  22.2043, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.6467, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:45:59,093 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.4563, val_loss:  16.9216, grad_norm: 0.3015, reward_err: 0.0565, 0.0248, 0.0256, KL_dist: 0.5753, 0.3953, 0.4467, param: [6.65495253 5.24707659 5.06629761 3.70288137]train_grp_loss: [20.68163529 15.63286466 18.97849322], val_grp_loss: [20.25284756 10.89295005 18.88650626], train_hist_grp_loss: [2170.32297645 1661.85580274 2016.16085937], cur_train_grp_loss: [20.69208208 15.63779239 18.99256998],max_reward_err:  0.0565, max_reward_err_index: 0, max_kl_dist:  0.5753, max_kl_dist_index: 0, max_train_grp_loss:  20.6816, max_train_grp_loss_index: 0, max_val_grp_loss:  20.2528, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.6921, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:46:02,234 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.9676, val_loss:  16.0778, grad_norm: 0.1581, reward_err: 0.0594, 0.0174, 0.0271, KL_dist: 0.8089, 0.5528, 0.6526, param: [7.86102439 5.43604851 6.2931638  4.99429245]train_grp_loss: [20.05499999 15.53956911 18.02054625], val_grp_loss: [19.57747207 10.00769422 17.90064033], train_hist_grp_loss: [4181.61490703 3200.79318261 3842.02421891], cur_train_grp_loss: [20.0585467  15.5388325  18.02703894],max_reward_err:  0.0594, max_reward_err_index: 0, max_kl_dist:  0.8089, max_kl_dist_index: 0, max_train_grp_loss:  20.0550, max_train_grp_loss_index: 0, max_val_grp_loss:  19.5775, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.0585, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:46:02,458 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.86102439 5.43604851 6.2931638  4.99429245].
2024-09-18 22:46:02,786 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7723, 3.7723, 3.1239
2024-09-18 22:46:02,787 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8153, 3.8153, 3.2617
2024-09-18 22:46:02,787 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5885, 3.7490, 3.1733
2024-09-18 22:46:02,788 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0594, 0.0174, 0.0271
2024-09-18 22:46:03,462 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8153, 3.8153, 3.2617
Known param reward: [[3.815298318862915, 3.4180736541748047, 3.2313497066497803], [3.4180736541748047, 3.815298318862915, 3.058016777038574], [3.7792916297912598, 3.547971248626709, 3.2616615295410156]], Known param reward error: [[0.0, 0.10411365809174691, 0.00929336861495278], [0.10411365809174691, 0.0, 0.06243589368731908], [0.009437450511703749, 0.07006714754506493, 0.0]].
