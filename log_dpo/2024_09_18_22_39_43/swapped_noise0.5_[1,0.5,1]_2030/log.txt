2024-09-18 22:43:03,553 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.5_[1,0.5,1]_2030
2024-09-18 22:43:03,555 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2030
2024-09-18 22:43:03,556 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:43:03,717 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5211, l2 distance: 8.2067, acc: 0.78.
2024-09-18 22:43:03,717 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:43:03,718 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [3.37196976 3.35883333 5.6739704  3.17762949]
2024-09-18 22:43:03,927 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5758, 3.8158, 3.2448
2024-09-18 22:43:05,368 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.7074, val_loss:  23.1227, grad_norm: 0.9119, reward_err: 0.0023, 0.1231, 0.0191, KL_dist: 0.0328, 0.2135, 0.0421, param: [0.47119828 3.09522546 0.57285712 0.73850802]train_grp_loss: [23.21061916 22.10339097 23.38341063], val_grp_loss: [23.39502431 22.51338115 23.44326358], train_hist_grp_loss: [23.25874987 22.22448317 23.42916454], cur_train_grp_loss: [23.25874987 22.22448317 23.42916454],max_reward_err:  0.1231, max_reward_err_index: 1, max_kl_dist:  0.2135, max_kl_dist_index: 1, max_train_grp_loss:  23.3834, max_train_grp_loss_index: 2, max_val_grp_loss:  23.4433, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  23.4292, max_cur_train_grp_loss_index: 2, 
2024-09-18 22:43:08,610 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.9400, val_loss:  18.5823, grad_norm: 0.3746, reward_err: 0.0410, 0.0394, 0.0123, KL_dist: 0.2744, 0.2648, 0.1878, param: [3.94075221 4.43881442 4.26031127 3.55254772]train_grp_loss: [20.55041578 17.11901903 20.77469859], val_grp_loss: [20.79265018 14.56564819 20.2164808 ], train_hist_grp_loss: [2184.61154039 1904.23528817 2206.83049583], cur_train_grp_loss: [20.56356855 17.13373262 20.78831182],max_reward_err:  0.0410, max_reward_err_index: 0, max_kl_dist:  0.2744, max_kl_dist_index: 0, max_train_grp_loss:  20.7747, max_train_grp_loss_index: 2, max_val_grp_loss:  20.7927, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.7883, max_cur_train_grp_loss_index: 2, 
2024-09-18 22:43:11,731 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.2595, val_loss:  17.1450, grad_norm: 0.1695, reward_err: 0.0557, 0.0219, 0.0219, KL_dist: 0.5665, 0.3800, 0.4245, param: [5.47186055 4.74628659 6.08189536 4.36124751]train_grp_loss: [19.80568908 16.53094187 19.96279329], val_grp_loss: [20.03718198 12.06198694 19.10349515], train_hist_grp_loss: [4175.29282943 3560.18092073 4216.6357027 ], cur_train_grp_loss: [19.80957617 16.53243507 19.96743021],max_reward_err:  0.0557, max_reward_err_index: 0, max_kl_dist:  0.5665, max_kl_dist_index: 0, max_train_grp_loss:  19.9628, max_train_grp_loss_index: 2, max_val_grp_loss:  20.0372, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.9674, max_cur_train_grp_loss_index: 2, 
2024-09-18 22:43:11,954 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [5.47186055 4.74628659 6.08189536 4.36124751].
2024-09-18 22:43:12,283 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8388, 3.8388, 3.2321
2024-09-18 22:43:12,284 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8833, 3.8833, 3.3901
2024-09-18 22:43:12,284 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.6669, 3.7982, 3.3159
2024-09-18 22:43:12,285 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0557, 0.0219, 0.0219
2024-09-18 22:43:12,960 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8833, 3.8833, 3.3901
Known param reward: [[3.883310317993164, 3.4867608547210693, 3.3494973182678223], [3.4867608547210693, 3.883310317993164, 3.1918535232543945], [3.8442142009735107, 3.598407745361328, 3.390115261077881]], Known param reward error: [[0.0, 0.1021163468277821, 0.011981286676708506], [0.1021163468277821, 0.0, 0.05848230002671041], [0.010067729287176206, 0.07336590416474088, 0.0]].
