2024-09-18 22:40:08,987 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.5_[1,0.5,1]_2022
2024-09-18 22:40:08,989 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2022
2024-09-18 22:40:08,990 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:40:09,149 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4500, l2 distance: 16.7322, acc: 0.80.
2024-09-18 22:40:09,150 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:40:09,150 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [9.385783   4.87832338 6.31300798 4.61071713]
2024-09-18 22:40:09,351 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.6061, 3.8367, 3.2033
2024-09-18 22:40:10,790 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.5942, val_loss:  21.6082, grad_norm: 0.9217, reward_err: 0.1265, 0.0352, 0.0983, KL_dist: 0.3161, 0.0479, 0.2349, param: [4.38193813 1.7821163  0.31221676 0.06294329]train_grp_loss: [23.0683365  19.68490945 22.94507682], val_grp_loss: [24.03544419 17.73929648 23.12667253], train_hist_grp_loss: [23.1239274  19.80880905 22.99826259], cur_train_grp_loss: [23.1239274  19.80880905 22.99826259],max_reward_err:  0.1265, max_reward_err_index: 0, max_kl_dist:  0.3161, max_kl_dist_index: 0, max_train_grp_loss:  23.0683, max_train_grp_loss_index: 0, max_val_grp_loss:  24.0354, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.1239, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:40:14,007 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  17.4618, val_loss:  17.1783, grad_norm: 0.4225, reward_err: 0.0800, 0.0174, 0.0502, KL_dist: 0.6649, 0.2777, 0.5318, param: [7.97730024 4.11119167 3.49797039 3.33428722]train_grp_loss: [19.74117085 14.46659061 19.66881275], val_grp_loss: [21.51059989 10.34883055 19.81409345], train_hist_grp_loss: [2135.44625408 1647.59050678 2127.81161784], cur_train_grp_loss: [19.75979373 14.48308429 19.68794609],max_reward_err:  0.0800, max_reward_err_index: 0, max_kl_dist:  0.6649, max_kl_dist_index: 0, max_train_grp_loss:  19.7412, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5106, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.7598, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:40:17,092 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  16.4911, val_loss:  15.8954, grad_norm: 0.2221, reward_err: 0.0795, 0.0130, 0.0484, KL_dist: 1.0235, 0.5225, 0.8449, param: [10.00914476  4.98314063  5.35532103  4.30858806]train_grp_loss: [18.57741933 13.77848815 18.43231119], val_grp_loss: [20.74963053  8.56183649 18.53647594], train_hist_grp_loss: [4023.59077368 3034.821895   4005.4315083 ], cur_train_grp_loss: [18.58446839 13.78028977 18.44015235],max_reward_err:  0.0795, max_reward_err_index: 0, max_kl_dist:  1.0235, max_kl_dist_index: 0, max_train_grp_loss:  18.5774, max_train_grp_loss_index: 0, max_val_grp_loss:  20.7496, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  18.5845, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:40:17,311 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [10.00914476  4.98314063  5.35532103  4.30858806].
2024-09-18 22:40:17,632 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8487, 3.8487, 3.2033
2024-09-18 22:40:17,633 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8787, 3.8787, 3.3327
2024-09-18 22:40:17,633 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5705, 3.8281, 3.1713
2024-09-18 22:40:17,634 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0795, 0.0130, 0.0484
2024-09-18 22:40:18,303 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8787, 3.8787, 3.3327
Known param reward: [[3.878673791885376, 3.5492217540740967, 3.282655954360962], [3.5492217540740967, 3.878673791885376, 3.170855760574341], [3.8372855186462402, 3.6475391387939453, 3.3327279090881348]], Known param reward error: [[0.0, 0.08493935182188567, 0.015024315243566644], [0.08493935182188567, 0.0, 0.04857046627550332], [0.01067072805290424, 0.05959115550655239, 0.0]].
