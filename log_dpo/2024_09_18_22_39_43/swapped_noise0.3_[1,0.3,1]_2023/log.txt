2024-09-18 22:47:36,132 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.3_[1,0.3,1]_2023
2024-09-18 22:47:36,134 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2023
2024-09-18 22:47:36,134 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:47:36,297 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4792, l2 distance: 14.4035, acc: 0.78.
2024-09-18 22:47:36,298 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:47:36,299 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [7.9065285  4.34862575 6.15420589 4.60594746]
2024-09-18 22:47:36,506 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5756, 3.8042, 3.1827
2024-09-18 22:47:37,956 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.0222, val_loss:  20.9662, grad_norm: 0.8162, reward_err: 0.0209, 0.1069, 0.0204, KL_dist: 0.0388, 0.3000, 0.0494, param: [0.41608209 1.70209405 2.19174691 4.75140123]train_grp_loss: [22.28193162 21.86137924 21.73518858], val_grp_loss: [22.108713   18.75620363 21.99642378], train_hist_grp_loss: [22.30308431 21.98556831 21.78201823], cur_train_grp_loss: [22.30308431 21.98556831 21.78201823],max_reward_err:  0.1069, max_reward_err_index: 1, max_kl_dist:  0.3000, max_kl_dist_index: 1, max_train_grp_loss:  22.2819, max_train_grp_loss_index: 0, max_val_grp_loss:  22.1087, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.3031, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:41,224 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.3717, val_loss:  16.9585, grad_norm: 0.4229, reward_err: 0.0518, 0.0250, 0.0207, KL_dist: 0.4271, 0.2986, 0.3209, param: [4.86311344 3.61168267 5.64010109 4.75160726]train_grp_loss: [21.02426582 15.35716303 18.64392636], val_grp_loss: [20.68363437 10.64407505 19.4843783 ], train_hist_grp_loss: [2176.59233597 1810.24295939 2020.05135241], cur_train_grp_loss: [21.03105746 15.38629601 18.66349658],max_reward_err:  0.0518, max_reward_err_index: 0, max_kl_dist:  0.4271, max_kl_dist_index: 0, max_train_grp_loss:  21.0243, max_train_grp_loss_index: 0, max_val_grp_loss:  20.6836, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.0311, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:44,355 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.3744, val_loss:  15.6209, grad_norm: 0.2264, reward_err: 0.0662, 0.0113, 0.0307, KL_dist: 0.8065, 0.4709, 0.6353, param: [7.33187252 4.30435334 7.37334737 4.82957483]train_grp_loss: [20.62568343 13.84542395 17.32086545], val_grp_loss: [20.21513184  8.16261447 18.42831067], train_hist_grp_loss: [4234.76804892 3238.96034213 3792.33924148], cur_train_grp_loss: [20.62783165 13.85217845 17.32967428],max_reward_err:  0.0662, max_reward_err_index: 0, max_kl_dist:  0.8065, max_kl_dist_index: 0, max_train_grp_loss:  20.6257, max_train_grp_loss_index: 0, max_val_grp_loss:  20.2151, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.6278, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:44,581 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.33187252 4.30435334 7.37334737 4.82957483].
2024-09-18 22:47:44,912 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8122, 3.8122, 3.1725
2024-09-18 22:47:44,913 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8527, 3.8527, 3.3024
2024-09-18 22:47:44,913 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5977, 3.8090, 3.2009
2024-09-18 22:47:44,914 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0662, 0.0113, 0.0307
2024-09-18 22:47:45,596 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8527, 3.8527, 3.3024
Known param reward: [[3.852689266204834, 3.475465774536133, 3.27390193939209], [3.475465774536133, 3.852689266204834, 3.1151528358459473], [3.8164188861846924, 3.5882885456085205, 3.3024346828460693]], Known param reward error: [[0.0, 0.09791173531113566, 0.008639911518065122], [0.09791173531113566, 0.0, 0.0567102350193103], [0.00941430193664968, 0.06862757474774667, 0.0]].
