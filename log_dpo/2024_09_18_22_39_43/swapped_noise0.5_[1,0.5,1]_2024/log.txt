2024-09-18 22:40:53,017 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.5_[1,0.5,1]_2024
2024-09-18 22:40:53,019 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2024
2024-09-18 22:40:53,019 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:40:53,180 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5287, l2 distance: 9.4697, acc: 0.77.
2024-09-18 22:40:53,181 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:40:53,181 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [5.62552404 3.20254866 5.14111836 2.94602878]
2024-09-18 22:40:53,395 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.6390, 3.8946, 3.2918
2024-09-18 22:40:54,863 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.4871, val_loss:  22.3099, grad_norm: 0.7882, reward_err: 0.0296, 0.1158, 0.0254, KL_dist: 0.0390, 0.2894, 0.0423, param: [1.94276012 4.28519265 0.23332462 1.0854425 ]train_grp_loss: [22.57980415 22.38366395 22.55678479], val_grp_loss: [22.49155585 21.8877674  22.5445523 ], train_hist_grp_loss: [22.60380314 22.48570751 22.59701   ], cur_train_grp_loss: [22.60380314 22.48570751 22.59701   ],max_reward_err:  0.1158, max_reward_err_index: 1, max_kl_dist:  0.2894, max_kl_dist_index: 1, max_train_grp_loss:  22.5798, max_train_grp_loss_index: 0, max_val_grp_loss:  22.5446, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.6038, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:40:58,167 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  19.4500, val_loss:  17.8948, grad_norm: 0.3667, reward_err: 0.0560, 0.0314, 0.0243, KL_dist: 0.3313, 0.2274, 0.2393, param: [5.35475725 4.13375817 3.53874205 3.58038928]train_grp_loss: [21.25410449 17.66931378 20.03211482], val_grp_loss: [20.41028524 13.05256097 20.17667801], train_hist_grp_loss: [2200.8950535  1963.71867563 2133.09864742], cur_train_grp_loss: [21.26058826 17.68833264 20.04733786],max_reward_err:  0.0560, max_reward_err_index: 0, max_kl_dist:  0.3313, max_kl_dist_index: 0, max_train_grp_loss:  21.2541, max_train_grp_loss_index: 0, max_val_grp_loss:  20.4103, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.2606, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:41:01,366 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.7036, val_loss:  16.2886, grad_norm: 0.1973, reward_err: 0.0731, 0.0159, 0.0356, KL_dist: 0.6443, 0.3370, 0.4918, param: [7.19150681 4.17404394 5.39476916 4.16503629]train_grp_loss: [20.89376976 16.65985891 19.01575705], val_grp_loss: [19.67763403  9.89357709 19.23893053], train_hist_grp_loss: [4283.709875   3652.49341629 4059.71705387], cur_train_grp_loss: [20.8955866  16.66471787 19.02251017],max_reward_err:  0.0731, max_reward_err_index: 0, max_kl_dist:  0.6443, max_kl_dist_index: 0, max_train_grp_loss:  20.8938, max_train_grp_loss_index: 0, max_val_grp_loss:  19.6776, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.8956, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:41:01,593 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.19150681 4.17404394 5.39476916 4.16503629].
2024-09-18 22:41:01,926 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8883, 3.8883, 3.2742
2024-09-18 22:41:01,927 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.9280, 3.9280, 3.4116
2024-09-18 22:41:01,927 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.6409, 3.8655, 3.2900
2024-09-18 22:41:01,928 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0731, 0.0159, 0.0356
2024-09-18 22:41:02,620 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.9280, 3.9280, 3.4116
Known param reward: [[3.927974224090576, 3.5139124393463135, 3.376384735107422], [3.5139124393463135, 3.927974224090576, 3.203280210494995], [3.887371301651001, 3.6481499671936035, 3.4115631580352783]], Known param reward error: [[0.0, 0.10541356972374948, 0.010311526211965462], [0.10541356972374948, 0.0, 0.061052056752844495], [0.010336860713228275, 0.07123882208309525, 0.0]].
