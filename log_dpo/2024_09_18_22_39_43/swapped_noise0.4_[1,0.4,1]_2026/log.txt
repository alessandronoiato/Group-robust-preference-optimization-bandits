2024-09-18 22:45:08,396 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.4_[1,0.4,1]_2026
2024-09-18 22:45:08,398 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2026
2024-09-18 22:45:08,399 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:45:08,564 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5431, l2 distance: 9.5009, acc: 0.73.
2024-09-18 22:45:08,565 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:45:08,565 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [4.34794473 3.37466145 6.23728494 2.01628827]
2024-09-18 22:45:08,777 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5407, 3.8522, 3.1577
2024-09-18 22:45:10,217 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.5244, val_loss:  18.9042, grad_norm: 0.4249, reward_err: 0.0327, 0.0494, 0.0075, KL_dist: 0.1610, 0.2410, 0.1165, param: [2.9621078  3.97739552 3.75648003 4.07894981]train_grp_loss: [20.53182903 20.2189577  21.24973604], val_grp_loss: [20.88750783 14.90270164 20.33810442], train_hist_grp_loss: [20.54249291 20.25014099 21.25704717], cur_train_grp_loss: [20.54249291 20.25014099 21.25704717],max_reward_err:  0.0494, max_reward_err_index: 1, max_kl_dist:  0.2410, max_kl_dist_index: 1, max_train_grp_loss:  21.2497, max_train_grp_loss_index: 2, max_val_grp_loss:  20.8875, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.2570, max_cur_train_grp_loss_index: 2, 
2024-09-18 22:45:13,485 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  19.4268, val_loss:  17.1952, grad_norm: 0.2488, reward_err: 0.0638, 0.0236, 0.0276, KL_dist: 0.4936, 0.2984, 0.3710, param: [5.02963515 4.61036161 6.17318682 3.60033599]train_grp_loss: [19.81477549 18.40482892 20.78834824], val_grp_loss: [20.44840458 11.0001993  19.2240874 ], train_hist_grp_loss: [2033.32046266 1934.69884943 2119.6548569 ], cur_train_grp_loss: [19.81942755 18.41423847 20.79107248],max_reward_err:  0.0638, max_reward_err_index: 0, max_kl_dist:  0.4936, max_kl_dist_index: 0, max_train_grp_loss:  20.7883, max_train_grp_loss_index: 2, max_val_grp_loss:  20.4484, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.7911, max_cur_train_grp_loss_index: 2, 
2024-09-18 22:45:16,645 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  19.0500, val_loss:  16.4243, grad_norm: 0.1475, reward_err: 0.0756, 0.0124, 0.0368, KL_dist: 0.7434, 0.3925, 0.5742, param: [6.22910742 4.88041294 7.61895356 3.36575528]train_grp_loss: [19.49336646 17.87790259 20.61599837], val_grp_loss: [20.28907025  9.2626545  18.66072512], train_hist_grp_loss: [3977.27044469 3725.53659964 4167.91004666], cur_train_grp_loss: [19.49557207 17.88049291 20.61703948],max_reward_err:  0.0756, max_reward_err_index: 0, max_kl_dist:  0.7434, max_kl_dist_index: 0, max_train_grp_loss:  20.6160, max_train_grp_loss_index: 2, max_val_grp_loss:  20.2891, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.6170, max_cur_train_grp_loss_index: 2, 
2024-09-18 22:45:16,870 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [6.22910742 4.88041294 7.61895356 3.36575528].
2024-09-18 22:45:17,201 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8382, 3.8382, 3.1948
2024-09-18 22:45:17,202 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8741, 3.8741, 3.3119
2024-09-18 22:45:17,202 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5813, 3.8260, 3.1901
2024-09-18 22:45:17,203 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0756, 0.0124, 0.0368
2024-09-18 22:45:17,892 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8741, 3.8741, 3.3119
Known param reward: [[3.8741025924682617, 3.481130361557007, 3.279806137084961], [3.481130361557007, 3.8741025924682617, 3.1205224990844727], [3.8313021659851074, 3.6054656505584717, 3.3118550777435303]], Known param reward error: [[0.0, 0.10143568001406103, 0.009677035953036107], [0.10143568001406103, 0.0, 0.057772026301772375], [0.011047829906818591, 0.06934172121101123, 0.0]].
