2024-09-18 22:46:55,443 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.3_[1,0.3,1]_2021
2024-09-18 22:46:55,445 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2021
2024-09-18 22:46:55,446 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:46:55,609 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5327, l2 distance: 9.2150, acc: 0.78.
2024-09-18 22:46:55,610 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:46:55,610 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [6.10968104 2.29424698 4.49928353 2.24249985]
2024-09-18 22:46:55,818 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4996, 3.8765, 3.1218
2024-09-18 22:46:57,262 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.0944, val_loss:  21.3010, grad_norm: 0.6872, reward_err: 0.0024, 0.0989, 0.0028, KL_dist: 0.0087, 0.1929, 0.0082, param: [1.0604235  3.56412391 1.66222852 2.23560724]train_grp_loss: [22.31636957 21.69774476 22.53097043], val_grp_loss: [22.31094117 19.38410891 22.32860038], train_hist_grp_loss: [22.3363799  21.77821849 22.56218294], cur_train_grp_loss: [22.3363799  21.77821849 22.56218294],max_reward_err:  0.0989, max_reward_err_index: 1, max_kl_dist:  0.1929, max_kl_dist_index: 1, max_train_grp_loss:  22.5310, max_train_grp_loss_index: 2, max_val_grp_loss:  22.3286, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.5622, max_cur_train_grp_loss_index: 2, 
2024-09-18 22:47:00,526 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  19.4475, val_loss:  17.6494, grad_norm: 0.3668, reward_err: 0.0650, 0.0163, 0.0288, KL_dist: 0.3577, 0.1785, 0.2490, param: [4.81523628 3.53481375 4.91357489 3.2352399 ]train_grp_loss: [21.17529475 17.27952246 20.54820201], val_grp_loss: [21.00087402 11.83082566 20.47462042], train_hist_grp_loss: [2186.06739225 1925.68464206 2161.87519303], cur_train_grp_loss: [21.18114399 17.30108289 20.56017118],max_reward_err:  0.0650, max_reward_err_index: 0, max_kl_dist:  0.3577, max_kl_dist_index: 0, max_train_grp_loss:  21.1753, max_train_grp_loss_index: 0, max_val_grp_loss:  21.0009, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.1811, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:03,685 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.6776, val_loss:  16.2680, grad_norm: 0.2024, reward_err: 0.0837, 0.0036, 0.0434, KL_dist: 0.6964, 0.3115, 0.5247, param: [6.9377394  3.47096371 6.60217254 3.48374755]train_grp_loss: [20.83754424 16.08369348 19.7624737 ], val_grp_loss: [20.59790661  8.94425422 19.70952518], train_hist_grp_loss: [4262.68365388 3565.43659475 4152.12516217], cur_train_grp_loss: [20.83936617 16.08965406 19.76753534],max_reward_err:  0.0837, max_reward_err_index: 0, max_kl_dist:  0.6964, max_kl_dist_index: 0, max_train_grp_loss:  20.8375, max_train_grp_loss_index: 0, max_val_grp_loss:  20.5979, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.8394, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:47:03,909 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [6.9377394  3.47096371 6.60217254 3.48374755].
2024-09-18 22:47:04,240 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8449, 3.8449, 3.1975
2024-09-18 22:47:04,240 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8804, 3.8804, 3.3109
2024-09-18 22:47:04,241 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5555, 3.8666, 3.1671
2024-09-18 22:47:04,241 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0837, 0.0036, 0.0434
2024-09-18 22:47:04,949 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8804, 3.8804, 3.3109
Known param reward: [[3.880363702774048, 3.454080104827881, 3.2871222496032715], [3.454080104827881, 3.880363702774048, 3.0933778285980225], [3.845717430114746, 3.5881128311157227, 3.310894012451172]], Known param reward error: [[0.0, 0.10985660896720055, 0.0071798622240708074], [0.10985660896720055, 0.0, 0.0656971147476009], [0.008928614767356305, 0.07531532970721194, 0.0]].
