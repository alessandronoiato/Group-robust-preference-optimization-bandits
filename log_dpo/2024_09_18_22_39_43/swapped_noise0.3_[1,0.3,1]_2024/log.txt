2024-09-18 22:47:56,879 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.3_[1,0.3,1]_2024
2024-09-18 22:47:56,881 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2024
2024-09-18 22:47:56,881 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:47:57,044 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5272, l2 distance: 8.9728, acc: 0.77.
2024-09-18 22:47:57,045 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:47:57,045 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [3.13265196 2.31429868 6.45347979 3.11117788]
2024-09-18 22:47:57,257 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5829, 3.8743, 3.2326
2024-09-18 22:47:58,668 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.5883, val_loss:  22.2184, grad_norm: 0.8133, reward_err: 0.0068, 0.1457, 0.0336, KL_dist: 0.0332, 0.4562, 0.0918, param: [0.40690328 4.91248207 0.35564045 2.78001327]train_grp_loss: [21.90830882 23.42477216 21.81679603], val_grp_loss: [21.95374147 22.37811651 22.31620906], train_hist_grp_loss: [21.92597678 23.54374465 21.84858494], cur_train_grp_loss: [21.92597678 23.54374465 21.84858494],max_reward_err:  0.1457, max_reward_err_index: 1, max_kl_dist:  0.4562, max_kl_dist_index: 1, max_train_grp_loss:  23.4248, max_train_grp_loss_index: 1, max_val_grp_loss:  22.3781, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  23.5437, max_cur_train_grp_loss_index: 1, 
2024-09-18 22:48:01,891 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  19.3381, val_loss:  18.2665, grad_norm: 0.3756, reward_err: 0.0365, 0.0419, 0.0099, KL_dist: 0.2416, 0.2739, 0.1774, param: [3.59745904 3.83444525 4.3757177  4.62258728]train_grp_loss: [20.99675609 17.68609611 19.92306041], val_grp_loss: [20.94483136 13.43243547 20.32569151], train_hist_grp_loss: [2156.66598731 2008.08187248 2092.7529095 ], cur_train_grp_loss: [21.00070033 17.71019153 19.93369731],max_reward_err:  0.0419, max_reward_err_index: 1, max_kl_dist:  0.2739, max_kl_dist_index: 1, max_train_grp_loss:  20.9968, max_train_grp_loss_index: 0, max_val_grp_loss:  20.9448, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.0007, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:48:05,023 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.5750, val_loss:  16.9007, grad_norm: 0.1963, reward_err: 0.0641, 0.0241, 0.0295, KL_dist: 0.5474, 0.3331, 0.4156, param: [5.01638824 3.43378659 6.61414325 4.95697508]train_grp_loss: [20.79629654 16.39806136 19.23946312], val_grp_loss: [20.74434366 10.22267836 19.60992217], train_hist_grp_loss: [4223.13599065 3682.01639032 4026.69030205], cur_train_grp_loss: [20.79718853 16.40436181 19.24386113],max_reward_err:  0.0641, max_reward_err_index: 0, max_kl_dist:  0.5474, max_kl_dist_index: 0, max_train_grp_loss:  20.7963, max_train_grp_loss_index: 0, max_val_grp_loss:  20.7443, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.7972, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:48:05,248 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [5.01638824 3.43378659 6.61414325 4.95697508].
2024-09-18 22:48:05,584 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8844, 3.8844, 3.2699
2024-09-18 22:48:05,585 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.9240, 3.9240, 3.4065
2024-09-18 22:48:05,585 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.6726, 3.8293, 3.3061
2024-09-18 22:48:05,586 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0641, 0.0241, 0.0295
2024-09-18 22:48:06,279 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.9240, 3.9240, 3.4065
Known param reward: [[3.924014091491699, 3.5108532905578613, 3.372161388397217], [3.5108532905578613, 3.924014091491699, 3.199089765548706], [3.8835723400115967, 3.6450908184051514, 3.40653657913208]], Known param reward error: [[0.0, 0.10529034588068371, 0.010090950129653801], [0.10529034588068371, 0.0, 0.060896693390630634], [0.010306219737536355, 0.0710811089316237, 0.0]].
