2024-09-18 22:43:44,304 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.4_[1,0.4,1]_2022
2024-09-18 22:43:44,306 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2022
2024-09-18 22:43:44,306 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:43:44,469 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4879, l2 distance: 12.6795, acc: 0.82.
2024-09-18 22:43:44,469 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:43:44,470 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [6.75502214 3.97653476 5.79379738 4.90833845]
2024-09-18 22:43:44,676 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.6743, 3.8038, 3.2594
2024-09-18 22:43:46,100 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.5100, val_loss:  19.9880, grad_norm: 0.7233, reward_err: 0.0253, 0.0719, 0.0051, KL_dist: 0.0989, 0.2520, 0.0543, param: [2.86273628 1.2285665  2.69577191 4.99498524]train_grp_loss: [21.3316866  19.64074385 20.72783157], val_grp_loss: [22.07897215 16.6427681  21.38237243], train_hist_grp_loss: [21.36643913 19.71287058 20.77065928], cur_train_grp_loss: [21.36643913 19.71287058 20.77065928],max_reward_err:  0.0719, max_reward_err_index: 1, max_kl_dist:  0.2520, max_kl_dist_index: 1, max_train_grp_loss:  21.3317, max_train_grp_loss_index: 0, max_val_grp_loss:  22.0790, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.3664, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:43:49,329 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  17.9788, val_loss:  16.8973, grad_norm: 0.3297, reward_err: 0.0427, 0.0222, 0.0146, KL_dist: 0.5133, 0.3757, 0.3981, param: [5.75815217 3.77725197 5.70240774 5.26429391]train_grp_loss: [19.1793404  16.85637755 17.89200982], val_grp_loss: [20.67970204 11.10548317 19.14606993], train_hist_grp_loss: [2030.13024279 1795.81831704 1933.16684118], cur_train_grp_loss: [19.19200237 16.86277002 17.91021856],max_reward_err:  0.0427, max_reward_err_index: 0, max_kl_dist:  0.5133, max_kl_dist_index: 0, max_train_grp_loss:  19.1793, max_train_grp_loss_index: 0, max_val_grp_loss:  20.6797, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.1920, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:43:52,451 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.3922, val_loss:  15.8496, grad_norm: 0.1711, reward_err: 0.0516, 0.0152, 0.0209, KL_dist: 0.8099, 0.5341, 0.6510, param: [7.35509371 4.48557472 7.27475913 5.55608954]train_grp_loss: [18.35651471 16.74144531 16.63414453], val_grp_loss: [20.2366682   9.34563239 18.23211987], train_hist_grp_loss: [3882.65697424 3453.05970079 3635.17231942], cur_train_grp_loss: [18.36176809 16.7400349  16.64279046],max_reward_err:  0.0516, max_reward_err_index: 0, max_kl_dist:  0.8099, max_kl_dist_index: 0, max_train_grp_loss:  18.3565, max_train_grp_loss_index: 0, max_val_grp_loss:  20.2367, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  18.3618, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:43:52,672 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.35509371 4.48557472 7.27475913 5.55608954].
2024-09-18 22:43:53,004 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8352, 3.8352, 3.1931
2024-09-18 22:43:53,004 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8672, 3.8672, 3.3262
2024-09-18 22:43:53,005 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.6678, 3.8085, 3.2565
2024-09-18 22:43:53,005 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0516, 0.0152, 0.0209
2024-09-18 22:43:53,683 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8672, 3.8672, 3.3262
Known param reward: [[3.867177724838257, 3.538572311401367, 3.2763869762420654], [3.538572311401367, 3.867177724838257, 3.1662933826446533], [3.823817253112793, 3.633922815322876, 3.3261606693267822]], Known param reward error: [[0.0, 0.08497292775718847, 0.01496430811166769], [0.08497292775718847, 0.0, 0.04806360924064627], [0.011212433151692661, 0.060316573509725795, 0.0]].
