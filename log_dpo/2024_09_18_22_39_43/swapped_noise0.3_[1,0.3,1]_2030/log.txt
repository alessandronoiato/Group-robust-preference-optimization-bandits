2024-09-18 22:50:03,180 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.3_[1,0.3,1]_2030
2024-09-18 22:50:03,182 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2030
2024-09-18 22:50:03,182 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:50:03,349 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5443, l2 distance: 7.9183, acc: 0.76.
2024-09-18 22:50:03,350 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:50:03,351 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [4.75082508 2.39887969 4.69994759 1.0585373 ]
2024-09-18 22:50:03,566 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4510, 3.7901, 3.1262
2024-09-18 22:50:05,000 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  23.5968, val_loss:  22.1740, grad_norm: 0.8826, reward_err: 0.0016, 0.1281, 0.0166, KL_dist: 0.0072, 0.2491, 0.0251, param: [0.50699563 2.01022279 0.98749354 3.5010985 ]train_grp_loss: [22.5563491  24.57525259 22.99346514], val_grp_loss: [22.59012147 21.22441206 22.64207511], train_hist_grp_loss: [22.56821713 24.71490448 23.03221402], cur_train_grp_loss: [22.56821713 24.71490448 23.03221402],max_reward_err:  0.1281, max_reward_err_index: 1, max_kl_dist:  0.2491, max_kl_dist_index: 1, max_train_grp_loss:  24.5753, max_train_grp_loss_index: 1, max_val_grp_loss:  22.6421, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  24.7149, max_cur_train_grp_loss_index: 1, 
2024-09-18 22:50:08,299 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  19.7016, val_loss:  18.2856, grad_norm: 0.4070, reward_err: 0.0719, 0.0082, 0.0349, KL_dist: 0.4058, 0.1268, 0.2687, param: [4.70047303 3.20825276 5.01469186 2.09757455]train_grp_loss: [21.85984752 17.82390863 20.50675101], val_grp_loss: [21.71724755 11.94724173 20.6846373 ], train_hist_grp_loss: [2237.28911932 2058.45902278 2180.24899253], cur_train_grp_loss: [21.86369558 17.85022964 20.52204174],max_reward_err:  0.0719, max_reward_err_index: 0, max_kl_dist:  0.4058, max_kl_dist_index: 0, max_train_grp_loss:  21.8598, max_train_grp_loss_index: 0, max_val_grp_loss:  21.7172, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.8637, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:50:11,444 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.8594, val_loss:  16.9961, grad_norm: 0.1936, reward_err: 0.0899, 0.0031, 0.0496, KL_dist: 0.7771, 0.2874, 0.5687, param: [6.7250046  3.6800485  6.89733653 1.73281577]train_grp_loss: [21.61654058 16.59666977 19.48836758], val_grp_loss: [21.41504085  9.06478763 19.85964103], train_hist_grp_loss: [4387.6398558  3745.84826691 4153.6111472 ], cur_train_grp_loss: [21.61805427 16.60105807 19.49503083],max_reward_err:  0.0899, max_reward_err_index: 0, max_kl_dist:  0.7771, max_kl_dist_index: 0, max_train_grp_loss:  21.6165, max_train_grp_loss_index: 0, max_val_grp_loss:  21.4150, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.6181, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:50:11,669 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [6.7250046  3.6800485  6.89733653 1.73281577].
2024-09-18 22:50:12,002 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7588, 3.7588, 3.1504
2024-09-18 22:50:12,002 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.7981, 3.7981, 3.2939
2024-09-18 22:50:12,003 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4565, 3.7865, 3.1304
2024-09-18 22:50:12,003 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0899, 0.0031, 0.0496
2024-09-18 22:50:12,695 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.7981, 3.7981, 3.2939
Known param reward: [[3.798083543777466, 3.4340016841888428, 3.2582314014434814], [3.4340016841888428, 3.798083543777466, 3.1138041019439697], [3.763892412185669, 3.5496251583099365, 3.293947219848633]], Known param reward error: [[0.0, 0.09585936048855775, 0.010842862991226866], [0.09585936048855775, 0.0, 0.054689133092102556], [0.009002206296334216, 0.06541677733092191, 0.0]].
