2024-09-18 22:48:59,105 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.3_[1,0.3,1]_2027
2024-09-18 22:48:59,107 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2027
2024-09-18 22:48:59,108 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:48:59,271 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5030, l2 distance: 12.0541, acc: 0.81.
2024-09-18 22:48:59,271 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:48:59,272 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [6.32296269 3.05508145 6.41888649 4.05651239]
2024-09-18 22:48:59,480 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5164, 3.7309, 3.1230
2024-09-18 22:49:00,895 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.8842, val_loss:  18.7112, grad_norm: 0.5446, reward_err: 0.0584, 0.0363, 0.0299, KL_dist: 0.2604, 0.1646, 0.1781, param: [2.69500212 4.07737057 4.89631371 2.19100324]train_grp_loss: [21.3356906  17.83481986 20.6363154 ], val_grp_loss: [21.53979493 13.57660913 21.10028034], train_hist_grp_loss: [21.35602753 17.87514344 20.66515902], cur_train_grp_loss: [21.35602753 17.87514344 20.66515902],max_reward_err:  0.0584, max_reward_err_index: 0, max_kl_dist:  0.2604, max_kl_dist_index: 0, max_train_grp_loss:  21.3357, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5398, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.3560, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:49:04,167 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.2676, val_loss:  16.3581, grad_norm: 0.2809, reward_err: 0.0658, 0.0119, 0.0347, KL_dist: 0.6517, 0.3435, 0.5032, param: [5.13540545 4.04564307 7.32291386 4.13516346]train_grp_loss: [20.04931487 16.01463067 18.7164101 ], val_grp_loss: [20.32360032  9.28269384 19.58191116], train_hist_grp_loss: [2081.01161125 1683.72655648 1975.81155565], cur_train_grp_loss: [20.05704438 16.02017247 18.72875111],max_reward_err:  0.0658, max_reward_err_index: 0, max_kl_dist:  0.6517, max_kl_dist_index: 0, max_train_grp_loss:  20.0493, max_train_grp_loss_index: 0, max_val_grp_loss:  20.3236, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.0570, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:49:07,296 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.8279, val_loss:  15.5039, grad_norm: 0.1501, reward_err: 0.0693, 0.0099, 0.0369, KL_dist: 0.9102, 0.4959, 0.7215, param: [6.4947051  3.95598247 8.63807812 4.93519009]train_grp_loss: [19.54279335 15.87680093 17.86741546], val_grp_loss: [19.87035454  7.83249737 18.93182697], train_hist_grp_loss: [4037.50282532 3257.46451376 3781.8857256 ], cur_train_grp_loss: [19.54605301 15.87593865 17.87321228],max_reward_err:  0.0693, max_reward_err_index: 0, max_kl_dist:  0.9102, max_kl_dist_index: 0, max_train_grp_loss:  19.5428, max_train_grp_loss_index: 0, max_val_grp_loss:  19.8704, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.5461, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:49:07,519 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [6.4947051  3.95598247 8.63807812 4.93519009].
2024-09-18 22:49:07,848 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7264, 3.7264, 3.0887
2024-09-18 22:49:07,849 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.7644, 3.7644, 3.2276
2024-09-18 22:49:07,849 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5035, 3.7271, 3.1086
2024-09-18 22:49:07,850 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0693, 0.0099, 0.0369
2024-09-18 22:49:08,523 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.7644, 3.7644, 3.2276
Known param reward: [[3.7643983364105225, 3.4167792797088623, 3.185643196105957], [3.4167792797088623, 3.7643983364105225, 3.0518436431884766], [3.725473642349243, 3.5370585918426514, 3.2276272773742676]], Known param reward error: [[0.0, 0.09234385568056709, 0.013007722906117384], [0.09234385568056709, 0.0, 0.05446218509120859], [0.010340216571871954, 0.06039205319186466, 0.0]].
