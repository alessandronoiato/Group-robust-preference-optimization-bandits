2024-09-18 22:40:29,655 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_22_39_43/swapped_noise0.5_[1,0.5,1]_2023
2024-09-18 22:40:29,657 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2023
2024-09-18 22:40:29,658 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 22:40:29,817 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4750, l2 distance: 15.0966, acc: 0.80.
2024-09-18 22:40:29,818 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 22:40:29,818 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [7.10196517 3.46007026 7.96069143 4.63730604]
2024-09-18 22:40:30,022 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5018, 3.7549, 3.0914
2024-09-18 22:40:31,432 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.9364, val_loss:  21.3850, grad_norm: 0.8663, reward_err: 0.1362, 0.0082, 0.0940, KL_dist: 0.2914, 0.0274, 0.1766, param: [2.38965477 0.20478604 3.30171271 0.05992934]train_grp_loss: [24.09326544 18.872074   23.58849571], val_grp_loss: [24.12328871 17.07601481 23.18163789], train_hist_grp_loss: [24.13627649 18.99165741 23.63963332], cur_train_grp_loss: [24.13627649 18.99165741 23.63963332],max_reward_err:  0.1362, max_reward_err_index: 0, max_kl_dist:  0.2914, max_kl_dist_index: 0, max_train_grp_loss:  24.0933, max_train_grp_loss_index: 0, max_val_grp_loss:  24.1233, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  24.1363, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:40:34,659 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.0844, val_loss:  16.6480, grad_norm: 0.4169, reward_err: 0.0856, 0.0039, 0.0469, KL_dist: 0.6361, 0.2263, 0.4645, param: [5.50658999 2.25440813 6.79373147 3.34519777]train_grp_loss: [21.43451984 13.45077328 20.35659009], val_grp_loss: [21.51165388  9.0336432  19.80000302], train_hist_grp_loss: [2279.45862694 1558.59917803 2196.71752371], cur_train_grp_loss: [21.44992194 13.46931785 20.37596025],max_reward_err:  0.0856, max_reward_err_index: 0, max_kl_dist:  0.6361, max_kl_dist_index: 0, max_train_grp_loss:  21.4345, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5117, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.4499, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:40:37,783 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.1458, val_loss:  15.2168, grad_norm: 0.2158, reward_err: 0.0830, 0.0054, 0.0447, KL_dist: 0.9868, 0.4805, 0.7736, param: [7.18733957 3.14729769 8.78528321 4.52413781]train_grp_loss: [20.46118713 12.68833675 19.09581485], val_grp_loss: [20.56814092  7.12388494 18.39807426], train_hist_grp_loss: [4346.22506587 2839.96581941 4141.15901849], cur_train_grp_loss: [20.46711961 12.69012979 19.10382295],max_reward_err:  0.0830, max_reward_err_index: 0, max_kl_dist:  0.9868, max_kl_dist_index: 0, max_train_grp_loss:  20.4612, max_train_grp_loss_index: 0, max_val_grp_loss:  20.5681, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.4671, max_cur_train_grp_loss_index: 0, 
2024-09-18 22:40:38,004 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.18733957 3.14729769 8.78528321 4.52413781].
2024-09-18 22:40:38,334 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7532, 3.7532, 3.0951
2024-09-18 22:40:38,334 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.7840, 3.7840, 3.2107
2024-09-18 22:40:38,335 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4699, 3.7637, 3.0673
2024-09-18 22:40:38,335 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0830, 0.0054, 0.0447
2024-09-18 22:40:39,009 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.7840, 3.7840, 3.2107
Known param reward: [[3.784027576446533, 3.406392812728882, 3.1765923500061035], [3.406392812728882, 3.784027576446533, 3.023632764816284], [3.743748664855957, 3.5277538299560547, 3.210723400115967]], Known param reward error: [[0.0, 0.09979704325312472, 0.010630330257857316], [0.09979704325312472, 0.0, 0.058270555256465], [0.010644455088353476, 0.06772512655183595, 0.0]].
