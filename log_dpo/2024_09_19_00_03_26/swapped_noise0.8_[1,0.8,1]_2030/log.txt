2024-09-19 00:14:10,982 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.8_[1,0.8,1]_2030
2024-09-19 00:14:10,984 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2030
2024-09-19 00:14:10,984 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:14:11,147 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4616, l2 distance: 14.2969, acc: 0.76.
2024-09-19 00:14:11,148 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:14:11,149 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [5.34201849 3.61195782 8.94222199 2.72247463]
2024-09-19 00:14:11,354 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4007, 3.7996, 3.0770
2024-09-19 00:14:12,803 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.4748, val_loss:  21.5817, grad_norm: 0.8872, reward_err: 0.0512, 0.0625, 0.0325, KL_dist: 0.0575, 0.1011, 0.0322, param: [0.64286361 2.37106243 2.30219421 1.85434433]train_grp_loss: [23.02396628 19.84259978 22.88887068], val_grp_loss: [22.75703694 19.18987368 22.68504877], train_hist_grp_loss: [23.04788075 19.97493208 22.92772615], cur_train_grp_loss: [23.04788075 19.97493208 22.92772615],max_reward_err:  0.0625, max_reward_err_index: 1, max_kl_dist:  0.1011, max_kl_dist_index: 1, max_train_grp_loss:  23.0240, max_train_grp_loss_index: 0, max_val_grp_loss:  22.7570, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.0479, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:14:16,011 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  17.4582, val_loss:  17.1228, grad_norm: 0.4266, reward_err: 0.0867, 0.0090, 0.0497, KL_dist: 0.5651, 0.2242, 0.4113, param: [4.64425388 3.72091336 6.6886073  3.02105707]train_grp_loss: [21.99131882 13.00417287 20.64845416], val_grp_loss: [20.52984172 10.52924402 19.97885201], train_hist_grp_loss: [2257.33622909 1586.78578432 2179.24964869], cur_train_grp_loss: [21.99383394 13.03604029 20.66035679],max_reward_err:  0.0867, max_reward_err_index: 0, max_kl_dist:  0.5651, max_kl_dist_index: 0, max_train_grp_loss:  21.9913, max_train_grp_loss_index: 0, max_val_grp_loss:  20.5298, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.9938, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:14:19,161 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  16.4744, val_loss:  15.7349, grad_norm: 0.2202, reward_err: 0.0991, 0.0027, 0.0596, KL_dist: 1.0043, 0.4187, 0.7755, param: [6.59444079 3.98679753 9.03020288 3.26564597]train_grp_loss: [21.9843623  11.20522717 19.92751043], val_grp_loss: [19.88112668  7.87163054 19.04878545], train_hist_grp_loss: [4431.30557242 2768.47164517 4182.02712076], cur_train_grp_loss: [21.9831951  11.21473103 19.93172301],max_reward_err:  0.0991, max_reward_err_index: 0, max_kl_dist:  1.0043, max_kl_dist_index: 0, max_train_grp_loss:  21.9844, max_train_grp_loss_index: 0, max_val_grp_loss:  19.8811, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.9832, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:14:19,381 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [6.59444079 3.98679753 9.03020288 3.26564597].
2024-09-19 00:14:19,714 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7691, 3.7691, 3.1589
2024-09-19 00:14:19,715 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8076, 3.8076, 3.3007
2024-09-19 00:14:19,716 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4302, 3.7975, 3.1038
2024-09-19 00:14:19,716 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0991, 0.0027, 0.0596
2024-09-19 00:14:20,480 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8076, 3.8076, 3.3007
Known param reward: [[3.80763578414917, 3.4319357872009277, 3.266146421432495], [3.4319357872009277, 3.80763578414917, 3.1126708984375], [3.773042917251587, 3.553501844406128, 3.3006885051727295]], Known param reward error: [[0.0, 0.09867015078286794, 0.010465114683224778], [0.09867015078286794, 0.0, 0.056963147670728256], [0.009085130211663064, 0.06674323757565724, 0.0]].
