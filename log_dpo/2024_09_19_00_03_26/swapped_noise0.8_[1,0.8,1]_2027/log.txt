2024-09-19 00:13:00,023 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.8_[1,0.8,1]_2027
2024-09-19 00:13:00,025 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2027
2024-09-19 00:13:00,025 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:13:00,189 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3814, l2 distance: 23.7630, acc: 0.85.
2024-09-19 00:13:00,190 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:13:00,190 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [ 8.88080729  4.95567706 12.33493647  6.36297512]
2024-09-19 00:13:00,403 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5399, 3.8381, 3.2082
2024-09-19 00:13:01,836 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.9277, val_loss:  20.8443, grad_norm: 1.0557, reward_err: 0.1279, 0.0061, 0.0825, KL_dist: 0.3002, 0.0115, 0.1744, param: [3.1714331  0.41074722 3.10801959 0.49736071]train_grp_loss: [23.48810143 17.37963214 22.13389059], val_grp_loss: [23.69160656 15.95170578 22.87244775], train_hist_grp_loss: [23.54920731 17.56455197 22.21228308], cur_train_grp_loss: [23.54920731 17.56455197 22.21228308],max_reward_err:  0.1279, max_reward_err_index: 0, max_kl_dist:  0.3002, max_kl_dist_index: 0, max_train_grp_loss:  23.4881, max_train_grp_loss_index: 0, max_val_grp_loss:  23.6916, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.5492, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:13:05,047 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  15.9000, val_loss:  16.2485, grad_norm: 0.4421, reward_err: 0.0809, 0.0075, 0.0428, KL_dist: 0.7423, 0.3226, 0.5583, param: [5.93466148 3.20479661 7.47636227 4.03705182]train_grp_loss: [20.07451128 10.3816787  17.38469968], val_grp_loss: [20.47883618  9.11295534 19.13118284], train_hist_grp_loss: [2168.19998493 1282.89661011 1959.69411749], cur_train_grp_loss: [20.09170937 10.39922705 17.41227136],max_reward_err:  0.0809, max_reward_err_index: 0, max_kl_dist:  0.7423, max_kl_dist_index: 0, max_train_grp_loss:  20.0745, max_train_grp_loss_index: 0, max_val_grp_loss:  20.4788, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.0917, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:13:08,202 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  14.8647, val_loss:  15.0714, grad_norm: 0.2288, reward_err: 0.0849, 0.0067, 0.0468, KL_dist: 1.1752, 0.5890, 0.9380, param: [ 7.28947937  3.89265328 10.0301225   5.04917938]train_grp_loss: [19.08895429  9.72036619 15.5728181 ], val_grp_loss: [19.59530499  7.72880688 17.87192229], train_hist_grp_loss: [4097.80723701 2266.3495172  3579.51111244], cur_train_grp_loss: [19.09417382  9.72189844 15.58469684],max_reward_err:  0.0849, max_reward_err_index: 0, max_kl_dist:  1.1752, max_kl_dist_index: 0, max_train_grp_loss:  19.0890, max_train_grp_loss_index: 0, max_val_grp_loss:  19.5953, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.0942, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:13:08,423 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [ 7.28947937  3.89265328 10.0301225   5.04917938].
2024-09-19 00:13:08,750 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8205, 3.8205, 3.2153
2024-09-19 00:13:08,751 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8664, 3.8664, 3.3644
2024-09-19 00:13:08,751 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5380, 3.8405, 3.2070
2024-09-19 00:13:08,752 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0849, 0.0067, 0.0468
2024-09-19 00:13:09,424 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8664, 3.8664, 3.3644
Known param reward: [[3.866352081298828, 3.466999053955078, 3.324810028076172], [3.466999053955078, 3.866352081298828, 3.1610913276672363], [3.8258588314056396, 3.5992729663848877, 3.364398956298828]], Known param reward error: [[0.0, 0.10328935879258955, 0.011767013584562513], [0.10328935879258955, 0.0, 0.06042910822182941], [0.010473244298947946, 0.06907780494326327, 0.0]].
