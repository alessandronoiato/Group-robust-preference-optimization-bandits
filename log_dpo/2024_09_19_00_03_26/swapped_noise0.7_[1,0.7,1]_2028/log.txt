2024-09-19 00:17:18,222 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.7_[1,0.7,1]_2028
2024-09-19 00:17:18,224 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2028
2024-09-19 00:17:18,225 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:17:18,387 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4872, l2 distance: 13.0978, acc: 0.77.
2024-09-19 00:17:18,388 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:17:18,389 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [6.46691538 3.95756071 6.43337544 5.0747662 ]
2024-09-19 00:17:18,599 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.6532, 3.8141, 3.2658
2024-09-19 00:17:20,002 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  23.1672, val_loss:  23.1331, grad_norm: 0.9471, reward_err: 0.0085, 0.1698, 0.0290, KL_dist: 0.0307, 0.4146, 0.0490, param: [0.15974519 4.60263225 0.83195028 0.07094603]train_grp_loss: [23.00552402 23.47106284 22.9334222 ], val_grp_loss: [22.94021556 23.18201921 23.29500232], train_hist_grp_loss: [23.04070851 23.64095306 22.98652823], cur_train_grp_loss: [23.04070851 23.64095306 22.98652823],max_reward_err:  0.1698, max_reward_err_index: 1, max_kl_dist:  0.4146, max_kl_dist_index: 1, max_train_grp_loss:  23.4711, max_train_grp_loss_index: 1, max_val_grp_loss:  23.2950, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  23.6410, max_cur_train_grp_loss_index: 1, 
2024-09-19 00:17:23,256 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.7674, val_loss:  18.3259, grad_norm: 0.4336, reward_err: 0.0287, 0.0442, 0.0067, KL_dist: 0.2581, 0.3072, 0.1956, param: [3.77604799 4.82648321 4.37087718 4.0940065 ]train_grp_loss: [21.13727467 15.58448562 19.67252055], val_grp_loss: [20.26351188 14.3122903  19.91560114], train_hist_grp_loss: [2209.5403837  1872.06027122 2127.28444979], cur_train_grp_loss: [21.14559622 15.61508812 19.69144644],max_reward_err:  0.0442, max_reward_err_index: 1, max_kl_dist:  0.3072, max_kl_dist_index: 1, max_train_grp_loss:  21.1373, max_train_grp_loss_index: 0, max_val_grp_loss:  20.2635, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.1456, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:17:26,427 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.7727, val_loss:  16.7713, grad_norm: 0.2196, reward_err: 0.0451, 0.0262, 0.0167, KL_dist: 0.5729, 0.4546, 0.4540, param: [5.71835366 4.72451978 6.32430802 5.46877024]train_grp_loss: [20.73293889 14.02170448 18.45739478], val_grp_loss: [19.40384504 11.70677961 18.57636376], train_hist_grp_loss: [4276.89794276 3320.27975536 4006.31899826], cur_train_grp_loss: [20.73438175 14.0289521  18.46502644],max_reward_err:  0.0451, max_reward_err_index: 0, max_kl_dist:  0.5729, max_kl_dist_index: 0, max_train_grp_loss:  20.7329, max_train_grp_loss_index: 0, max_val_grp_loss:  19.4038, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.7344, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:17:26,651 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [5.71835366 4.72451978 6.32430802 5.46877024].
2024-09-19 00:17:26,986 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8371, 3.8371, 3.2123
2024-09-19 00:17:26,986 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8741, 3.8741, 3.3487
2024-09-19 00:17:26,987 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.6993, 3.7728, 3.2929
2024-09-19 00:17:26,987 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0451, 0.0262, 0.0167
2024-09-19 00:17:27,676 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8741, 3.8741, 3.3487
Known param reward: [[3.8741343021392822, 3.4676833152770996, 3.31387996673584], [3.4676833152770996, 3.8741343021392822, 3.1369452476501465], [3.840017557144165, 3.611396074295044, 3.3486669063568115]], Known param reward error: [[0.0, 0.10491401566480076, 0.010388294982380973], [0.10491401566480076, 0.0, 0.0632256550523886], [0.008806288665903515, 0.06781856470467615, 0.0]].
