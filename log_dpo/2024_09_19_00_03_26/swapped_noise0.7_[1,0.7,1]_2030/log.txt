2024-09-19 00:18:01,777 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.7_[1,0.7,1]_2030
2024-09-19 00:18:01,779 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2030
2024-09-19 00:18:01,779 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:18:01,943 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4425, l2 distance: 16.8138, acc: 0.81.
2024-09-19 00:18:01,944 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:18:01,945 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [7.62750007 3.92484544 9.1161362  3.44860802]
2024-09-19 00:18:02,158 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4592, 3.8076, 3.1317
2024-09-19 00:18:03,583 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  23.4554, val_loss:  22.1256, grad_norm: 1.0771, reward_err: 0.0058, 0.1266, 0.0333, KL_dist: 0.0249, 0.3851, 0.0736, param: [0.83570155 4.66595969 0.24178786 2.62770147]train_grp_loss: [21.94947186 24.86302896 22.60722197], val_grp_loss: [21.98780832 22.02779852 22.37737074], train_hist_grp_loss: [21.9598347  25.080219   22.65384451], cur_train_grp_loss: [21.9598347  25.080219   22.65384451],max_reward_err:  0.1266, max_reward_err_index: 1, max_kl_dist:  0.3851, max_kl_dist_index: 1, max_train_grp_loss:  24.8630, max_train_grp_loss_index: 1, max_val_grp_loss:  22.3774, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  25.0802, max_cur_train_grp_loss_index: 1, 
2024-09-19 00:18:06,875 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  17.5856, val_loss:  17.1428, grad_norm: 0.5100, reward_err: 0.0689, 0.0097, 0.0328, KL_dist: 0.5418, 0.2294, 0.3893, param: [6.05418245 3.45416574 5.42206711 3.38898442]train_grp_loss: [21.50606378 13.9870944  19.5996037 ], val_grp_loss: [21.38818836  9.67569083 19.80215083], train_hist_grp_loss: [2187.81380151 1838.5777957  2111.95678933], cur_train_grp_loss: [21.50731302 14.03376435 19.6183951 ],max_reward_err:  0.0689, max_reward_err_index: 0, max_kl_dist:  0.5418, max_kl_dist_index: 0, max_train_grp_loss:  21.5061, max_train_grp_loss_index: 0, max_val_grp_loss:  21.3882, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.5073, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:18:10,108 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  16.2051, val_loss:  15.7399, grad_norm: 0.2573, reward_err: 0.0865, 0.0011, 0.0469, KL_dist: 1.0688, 0.4563, 0.8210, param: [8.5571872  3.42548021 8.04848713 3.5304871 ]train_grp_loss: [21.46932588 11.57515065 18.31615329], val_grp_loss: [21.26911313  6.46464611 18.7564791 ], train_hist_grp_loss: [4314.1163122  3077.06427123 3981.38769443], cur_train_grp_loss: [21.46926266 11.5859686  18.32489334],max_reward_err:  0.0865, max_reward_err_index: 0, max_kl_dist:  1.0688, max_kl_dist_index: 0, max_train_grp_loss:  21.4693, max_train_grp_loss_index: 0, max_val_grp_loss:  21.2691, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.4693, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:18:10,334 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [8.5571872  3.42548021 8.04848713 3.5304871 ].
2024-09-19 00:18:10,668 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7748, 3.7748, 3.1636
2024-09-19 00:18:10,669 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8136, 3.8136, 3.3066
2024-09-19 00:18:10,669 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4838, 3.8095, 3.1516
2024-09-19 00:18:10,670 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0865, 0.0011, 0.0469
2024-09-19 00:18:11,369 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8136, 3.8136, 3.3066
Known param reward: [[3.813612222671509, 3.4377872943878174, 3.272095203399658], [3.4377872943878174, 3.813612222671509, 3.118396520614624], [3.779019594192505, 3.5611813068389893, 3.3066372871398926]], Known param reward error: [[0.0, 0.09854828082662763, 0.010446287494118195], [0.09854828082662763, 0.0, 0.056928156970034414], [0.009070830084232083, 0.06619207750904647, 0.0]].
