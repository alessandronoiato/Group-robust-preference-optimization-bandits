2024-09-19 00:21:37,286 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.6_[1,0.6,1]_2030
2024-09-19 00:21:37,288 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2030
2024-09-19 00:21:37,289 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:21:37,450 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4961, l2 distance: 10.5890, acc: 0.77.
2024-09-19 00:21:37,450 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:21:37,451 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [5.36923291 3.75606204 5.9084699  3.85520782]
2024-09-19 00:21:37,655 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5727, 3.7607, 3.2144
2024-09-19 00:21:39,064 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.7513, val_loss:  21.1049, grad_norm: 0.7513, reward_err: 0.0039, 0.0833, 0.0030, KL_dist: 0.0276, 0.2755, 0.0305, param: [1.97802683 4.34569563 1.56217434 3.03577784]train_grp_loss: [21.69822928 19.7280751  21.69068766], val_grp_loss: [21.80248948 19.62325133 21.78802398], train_hist_grp_loss: [21.71797815 19.82017155 21.72094828], cur_train_grp_loss: [21.71797815 19.82017155 21.72094828],max_reward_err:  0.0833, max_reward_err_index: 1, max_kl_dist:  0.2755, max_kl_dist_index: 1, max_train_grp_loss:  21.6982, max_train_grp_loss_index: 0, max_val_grp_loss:  21.8025, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.7209, max_cur_train_grp_loss_index: 2, 
2024-09-19 00:21:42,286 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.1091, val_loss:  17.7593, grad_norm: 0.3267, reward_err: 0.0481, 0.0289, 0.0175, KL_dist: 0.4150, 0.3305, 0.3114, param: [5.37506176 4.20766702 4.78988875 4.75111475]train_grp_loss: [20.66471805 15.59869477 19.90577736], val_grp_loss: [20.669111   12.46699853 19.72392477], train_hist_grp_loss: [2128.27587356 1729.25485606 2085.99666609], cur_train_grp_loss: [20.66931614 15.61408767 19.91560838],max_reward_err:  0.0481, max_reward_err_index: 0, max_kl_dist:  0.4150, max_kl_dist_index: 0, max_train_grp_loss:  20.6647, max_train_grp_loss_index: 0, max_val_grp_loss:  20.6691, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.6693, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:21:45,446 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.5592, val_loss:  16.6514, grad_norm: 0.1596, reward_err: 0.0582, 0.0160, 0.0244, KL_dist: 0.7056, 0.4288, 0.5459, param: [6.96919355 4.30517316 6.40039626 5.03878119]train_grp_loss: [20.43084801 14.83508354 19.29422858], val_grp_loss: [20.38645501 10.06176917 18.97026399], train_hist_grp_loss: [4159.78399596 3226.75663723 4021.83109414], cur_train_grp_loss: [20.43183164 14.83838425 19.29792608],max_reward_err:  0.0582, max_reward_err_index: 0, max_kl_dist:  0.7056, max_kl_dist_index: 0, max_train_grp_loss:  20.4308, max_train_grp_loss_index: 0, max_val_grp_loss:  20.3865, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.4318, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:21:45,668 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [6.96919355 4.30517316 6.40039626 5.03878119].
2024-09-19 00:21:45,997 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7751, 3.7751, 3.1646
2024-09-19 00:21:45,998 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8138, 3.8138, 3.3081
2024-09-19 00:21:45,998 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5920, 3.7530, 3.2275
2024-09-19 00:21:45,999 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0582, 0.0160, 0.0244
2024-09-19 00:21:46,668 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8138, 3.8138, 3.3081
Known param reward: [[3.8138465881347656, 3.439753770828247, 3.273068904876709], [3.439753770828247, 3.8138465881347656, 3.121405839920044], [3.7787630558013916, 3.5612988471984863, 3.308058500289917]], Known param reward error: [[0.0, 0.09808806113763369, 0.010577078794145125], [0.09808806113763369, 0.0, 0.05642362743993641], [0.009198988874519018, 0.06621864175711184, 0.0]].
