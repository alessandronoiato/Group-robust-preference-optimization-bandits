2024-09-19 00:03:30,950 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise1.0_[1,1.0,1]_2021
2024-09-19 00:03:30,952 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2021
2024-09-19 00:03:30,953 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:03:31,124 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3566, l2 distance: 29.5613, acc: 0.82.
2024-09-19 00:03:31,125 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:03:31,126 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [12.9300958   5.57206545 13.55993407  6.18568546]
2024-09-19 00:03:31,336 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5203, 3.8700, 3.1356
2024-09-19 00:03:32,811 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.4903, val_loss:  20.9007, grad_norm: 0.9347, reward_err: 0.1119, 0.0010, 0.0680, KL_dist: 0.2138, 0.0053, 0.1147, param: [3.04421893 1.24399833 2.72641581 0.54475873]train_grp_loss: [23.90255387 16.24992226 22.56048797], val_grp_loss: [23.60121745 16.60542966 22.77906565], train_hist_grp_loss: [23.93346769 16.40519046 22.61737849], cur_train_grp_loss: [23.93346769 16.40519046 22.61737849],max_reward_err:  0.1119, max_reward_err_index: 0, max_kl_dist:  0.2138, max_kl_dist_index: 0, max_train_grp_loss:  23.9026, max_train_grp_loss_index: 0, max_val_grp_loss:  23.6012, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.9335, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:03:36,106 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  15.7611, val_loss:  16.2678, grad_norm: 0.4804, reward_err: 0.0988, 0.0004, 0.0563, KL_dist: 0.8198, 0.3250, 0.6164, param: [7.48267503 3.10662547 7.04113233 2.50911832]train_grp_loss: [22.22473102  8.00722496 18.98700748], val_grp_loss: [21.55736838  8.18731349 19.57811468], train_hist_grp_loss: [2312.51085576 1140.94529741 2072.93123162], cur_train_grp_loss: [22.23250949  8.04612039 19.00832541],max_reward_err:  0.0988, max_reward_err_index: 0, max_kl_dist:  0.8198, max_kl_dist_index: 0, max_train_grp_loss:  22.2247, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5574, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2325, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:03:39,265 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  14.4658, val_loss:  14.9473, grad_norm: 0.2594, reward_err: 0.1027, 0.0001, 0.0597, KL_dist: 1.3040, 0.6554, 1.0346, param: [9.91379022 3.61994629 9.40423221 3.34612124]train_grp_loss: [21.82850807  5.85162939 17.59450374], val_grp_loss: [21.00468536  6.04683817 18.34736554], train_hist_grp_loss: [4488.48103409 1805.99788598 3874.52273503], cur_train_grp_loss: [21.83012453  5.86253444 17.6034376 ],max_reward_err:  0.1027, max_reward_err_index: 0, max_kl_dist:  1.3040, max_kl_dist_index: 0, max_train_grp_loss:  21.8285, max_train_grp_loss_index: 0, max_val_grp_loss:  21.0047, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.8301, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:03:39,491 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [9.91379022 3.61994629 9.40423221 3.34612124].
2024-09-19 00:03:39,823 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8390, 3.8390, 3.1881
2024-09-19 00:03:39,823 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8748, 3.8748, 3.3008
2024-09-19 00:03:39,824 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4768, 3.8743, 3.1037
2024-09-19 00:03:39,824 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.1027, 0.0001, 0.0597
2024-09-19 00:03:40,504 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8748, 3.8748, 3.3008
Known param reward: [[3.8748409748077393, 3.4526655673980713, 3.2780399322509766], [3.4526655673980713, 3.8748409748077393, 3.086048126220703], [3.8403704166412354, 3.587584972381592, 3.300816535949707]], Known param reward error: [[0.0, 0.10895296352919757, 0.00690029374570411], [0.10895296352919757, 0.0, 0.06506523685576818], [0.008895992994451664, 0.07413362362319925, 0.0]].
