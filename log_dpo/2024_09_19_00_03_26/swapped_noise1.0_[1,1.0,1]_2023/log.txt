2024-09-19 00:04:13,880 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise1.0_[1,1.0,1]_2023
2024-09-19 00:04:13,882 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2023
2024-09-19 00:04:13,883 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:04:14,043 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3431, l2 distance: 35.5343, acc: 0.83.
2024-09-19 00:04:14,044 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:04:14,045 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [15.61298457  7.73326877 14.20212669  8.81141422]
2024-09-19 00:04:14,245 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5015, 3.7537, 3.0878
2024-09-19 00:04:15,651 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.5928, val_loss:  20.7199, grad_norm: 0.9086, reward_err: 0.0241, 0.0852, 0.0097, KL_dist: 0.0684, 0.2393, 0.0498, param: [1.27484668 4.57720519 3.01917348 2.10789007]train_grp_loss: [21.73213763 18.9490803  21.52376187], val_grp_loss: [21.80018891 18.62894846 21.70408722], train_hist_grp_loss: [21.75479123 19.11742686 21.55717742], cur_train_grp_loss: [21.75479123 19.11742686 21.55717742],max_reward_err:  0.0852, max_reward_err_index: 1, max_kl_dist:  0.2393, max_kl_dist_index: 1, max_train_grp_loss:  21.7321, max_train_grp_loss_index: 0, max_val_grp_loss:  21.8002, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.7548, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:04:18,861 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  16.0962, val_loss:  16.4078, grad_norm: 0.4704, reward_err: 0.0699, 0.0132, 0.0345, KL_dist: 0.6881, 0.3732, 0.5336, param: [5.66373498 4.04773565 7.3807142  4.31383191]train_grp_loss: [20.42377601  9.93029537 19.45239454], val_grp_loss: [20.02169312  9.9777795  19.17124643], train_hist_grp_loss: [2117.39564799 1367.76080853 2054.07779252], cur_train_grp_loss: [20.43057929  9.97329751 19.46446131],max_reward_err:  0.0699, max_reward_err_index: 0, max_kl_dist:  0.6881, max_kl_dist_index: 0, max_train_grp_loss:  20.4238, max_train_grp_loss_index: 0, max_val_grp_loss:  20.0217, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.4306, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:04:22,115 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  14.8417, val_loss:  15.1201, grad_norm: 0.2577, reward_err: 0.0811, 0.0050, 0.0432, KL_dist: 1.1836, 0.6355, 0.9509, param: [8.10166862 3.98707691 9.76295784 4.98699403]train_grp_loss: [20.03192411  7.52573353 18.68255384], val_grp_loss: [19.41374649  7.78079937 18.12178813], train_hist_grp_loss: [4116.39474589 2208.92529504 3936.32489774], cur_train_grp_loss: [20.03399579  7.53811689 18.68732857],max_reward_err:  0.0811, max_reward_err_index: 0, max_kl_dist:  1.1836, max_kl_dist_index: 0, max_train_grp_loss:  20.0319, max_train_grp_loss_index: 0, max_val_grp_loss:  19.4137, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.0340, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:04:22,341 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [8.10166862 3.98707691 9.76295784 4.98699403].
2024-09-19 00:04:22,670 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7505, 3.7505, 3.0898
2024-09-19 00:04:22,670 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.7806, 3.7806, 3.2051
2024-09-19 00:04:22,671 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4739, 3.7617, 3.0666
2024-09-19 00:04:22,671 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0811, 0.0050, 0.0432
2024-09-19 00:04:23,350 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.7806, 3.7806, 3.2051
Known param reward: [[3.7806432247161865, 3.4037325382232666, 3.1705894470214844], [3.4037325382232666, 3.7806432247161865, 3.017861843109131], [3.7406539916992188, 3.5228474140167236, 3.2050557136535645]], Known param reward error: [[0.0, 0.09969485722134351, 0.010753718409715467], [0.09969485722134351, 0.0, 0.05840580859389936], [0.010577362274106087, 0.06818834663215693, 0.0]].
