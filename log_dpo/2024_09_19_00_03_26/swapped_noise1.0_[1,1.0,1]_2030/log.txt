2024-09-19 00:06:44,202 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise1.0_[1,1.0,1]_2030
2024-09-19 00:06:44,204 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2030
2024-09-19 00:06:44,204 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:06:44,366 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3234, l2 distance: 29.0058, acc: 0.84.
2024-09-19 00:06:44,367 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:06:44,367 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [12.81511744  6.94562577 12.93366958  5.46843   ]
2024-09-19 00:06:44,572 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5538, 3.8595, 3.2321
2024-09-19 00:06:45,992 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  18.7899, val_loss:  19.9570, grad_norm: 0.9991, reward_err: 0.0771, 0.0474, 0.0404, KL_dist: 0.2640, 0.1526, 0.1580, param: [4.45612986 0.5301474  2.77755776 4.06308483]train_grp_loss: [22.37417742 15.27222835 21.30041275], val_grp_loss: [22.24327346 15.90258118 21.63382516], train_hist_grp_loss: [22.39938404 15.4456462  21.34417015], cur_train_grp_loss: [22.39938404 15.4456462  21.34417015],max_reward_err:  0.0771, max_reward_err_index: 0, max_kl_dist:  0.2640, max_kl_dist_index: 0, max_train_grp_loss:  22.3742, max_train_grp_loss_index: 0, max_val_grp_loss:  22.2433, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.3994, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:06:49,214 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  14.0990, val_loss:  16.1179, grad_norm: 0.4308, reward_err: 0.0835, 0.0035, 0.0446, KL_dist: 1.0206, 0.4192, 0.7814, param: [8.72499007 3.69787061 6.76118933 3.63380449]train_grp_loss: [21.14117635  7.27195598 18.78407874], val_grp_loss: [20.60666581  8.36552806 19.19407372], train_hist_grp_loss: [2182.36168811 1035.66184946 2002.21438593], cur_train_grp_loss: [21.14580437  7.30254451 18.79726615],max_reward_err:  0.0835, max_reward_err_index: 0, max_kl_dist:  1.0206, max_kl_dist_index: 0, max_train_grp_loss:  21.1412, max_train_grp_loss_index: 0, max_val_grp_loss:  20.6067, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.1458, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:06:52,347 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  13.1688, val_loss:  15.1995, grad_norm: 0.2031, reward_err: 0.0883, 0.0020, 0.0483, KL_dist: 1.4811, 0.6894, 1.1699, param: [10.75893122  4.56027601  8.74042047  3.53042101]train_grp_loss: [20.95737768  5.7354031  18.00602881], val_grp_loss: [20.21175015  6.78441828 18.38399782], train_hist_grp_loss: [4262.92758679 1661.97800779 3816.68163955], cur_train_grp_loss: [20.9576402   5.74230039 18.01034674],max_reward_err:  0.0883, max_reward_err_index: 0, max_kl_dist:  1.4811, max_kl_dist_index: 0, max_train_grp_loss:  20.9574, max_train_grp_loss_index: 0, max_val_grp_loss:  20.2118, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.9576, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:06:52,570 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [10.75893122  4.56027601  8.74042047  3.53042101].
2024-09-19 00:06:52,901 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8314, 3.8314, 3.2209
2024-09-19 00:06:52,902 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8748, 3.8748, 3.3766
2024-09-19 00:06:52,902 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5327, 3.8669, 3.2135
2024-09-19 00:06:52,903 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0883, 0.0020, 0.0483
2024-09-19 00:06:53,578 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8748, 3.8748, 3.3766
Known param reward: [[3.874847650527954, 3.4763882160186768, 3.337296485900879], [3.4763882160186768, 3.874847650527954, 3.175377130508423], [3.8370981216430664, 3.5955355167388916, 3.3765645027160645]], Known param reward error: [[0.0, 0.102832284117025, 0.011629576980862906], [0.102832284117025, 0.0, 0.059583452958120335], [0.009742196929921686, 0.0720833846850742, 0.0]].
