2024-09-19 00:20:56,438 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.6_[1,0.6,1]_2028
2024-09-19 00:20:56,440 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2028
2024-09-19 00:20:56,441 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:20:56,604 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.5162, l2 distance: 11.4215, acc: 0.77.
2024-09-19 00:20:56,604 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:20:56,605 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [4.65646533 3.49803522 7.38419958 2.36698542]
2024-09-19 00:20:56,814 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5114, 3.8516, 3.1541
2024-09-19 00:20:58,260 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.0791, val_loss:  19.0595, grad_norm: 0.4901, reward_err: 0.0617, 0.0437, 0.0332, KL_dist: 0.2401, 0.1767, 0.1582, param: [4.68245336 4.29964425 2.6521291  1.87942283]train_grp_loss: [22.04227329 17.34462978 21.02273933], val_grp_loss: [21.5286477  14.44457245 20.748971  ], train_hist_grp_loss: [22.05281151 17.38302537 21.04859279], cur_train_grp_loss: [22.05281151 17.38302537 21.04859279],max_reward_err:  0.0617, max_reward_err_index: 0, max_kl_dist:  0.2401, max_kl_dist_index: 0, max_train_grp_loss:  22.0423, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5286, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.0528, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:21:01,455 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.7190, val_loss:  16.8481, grad_norm: 0.2656, reward_err: 0.0741, 0.0207, 0.0388, KL_dist: 0.6044, 0.3058, 0.4469, param: [6.64884799 4.76949277 5.59160377 2.56803807]train_grp_loss: [21.48447542 15.24858847 19.2918295 ], val_grp_loss: [20.56807245 10.26564558 19.03465601], train_hist_grp_loss: [2192.11173759 1625.59557678 2025.72904522], cur_train_grp_loss: [21.48688129 15.25882365 19.30306841],max_reward_err:  0.0741, max_reward_err_index: 0, max_kl_dist:  0.6044, max_kl_dist_index: 0, max_train_grp_loss:  21.4845, max_train_grp_loss_index: 0, max_val_grp_loss:  20.5681, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.4869, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:21:04,586 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  18.3056, val_loss:  15.9481, grad_norm: 0.1517, reward_err: 0.0804, 0.0074, 0.0431, KL_dist: 0.8578, 0.4172, 0.6555, param: [7.58686767 4.81025559 7.3311116  2.85929383]train_grp_loss: [21.37682149 14.67082652 18.50710105], val_grp_loss: [20.23493058  8.56528295 18.27128775], train_hist_grp_loss: [4312.11458913 3101.19477352 3892.56004837], cur_train_grp_loss: [21.37708983 14.67383719 18.51259289],max_reward_err:  0.0804, max_reward_err_index: 0, max_kl_dist:  0.8578, max_kl_dist_index: 0, max_train_grp_loss:  21.3768, max_train_grp_loss_index: 0, max_val_grp_loss:  20.2349, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.3771, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:21:04,807 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.58686767 4.81025559 7.3311116  2.85929383].
2024-09-19 00:21:05,134 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8289, 3.8289, 3.2018
2024-09-19 00:21:05,135 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8652, 3.8652, 3.3372
2024-09-19 00:21:05,135 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5544, 3.8366, 3.1934
2024-09-19 00:21:05,136 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0804, 0.0074, 0.0431
2024-09-19 00:21:05,822 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8652, 3.8652, 3.3372
Known param reward: [[3.8651840686798096, 3.4605093002319336, 3.3015952110290527], [3.4605093002319336, 3.8651840686798096, 3.1258203983306885], [3.8319525718688965, 3.6041877269744873, 3.337219715118408]], Known param reward error: [[0.0, 0.10469741188447372, 0.010674905199669023], [0.10469741188447372, 0.0, 0.0633459390851702], [0.00859764922457202, 0.06752494501367125, 0.0]].
