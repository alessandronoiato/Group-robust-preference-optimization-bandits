2024-09-19 00:18:47,383 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.6_[1,0.6,1]_2022
2024-09-19 00:18:47,385 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2022
2024-09-19 00:18:47,385 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:18:47,548 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4258, l2 distance: 17.8984, acc: 0.79.
2024-09-19 00:18:47,549 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:18:47,550 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [8.50055629 4.88587613 8.63017399 4.5857094 ]
2024-09-19 00:18:47,762 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.6173, 3.8368, 3.2206
2024-09-19 00:18:49,173 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  18.8997, val_loss:  18.7956, grad_norm: 0.7288, reward_err: 0.0672, 0.0150, 0.0351, KL_dist: 0.2547, 0.0748, 0.1557, param: [4.45881133 1.71704344 3.2332928  2.99671058]train_grp_loss: [21.92464142 15.07553222 21.41517797], val_grp_loss: [22.06422482 13.23494496 21.52048499], train_hist_grp_loss: [21.95633568 15.15374045 21.45452751], cur_train_grp_loss: [21.95633568 15.15374045 21.45452751],max_reward_err:  0.0672, max_reward_err_index: 0, max_kl_dist:  0.2547, max_kl_dist_index: 0, max_train_grp_loss:  21.9246, max_train_grp_loss_index: 0, max_val_grp_loss:  22.0642, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.9563, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:18:52,422 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  16.2677, val_loss:  15.8262, grad_norm: 0.3385, reward_err: 0.0637, 0.0098, 0.0311, KL_dist: 0.7423, 0.3771, 0.5751, param: [7.57898533 4.08312948 6.2115513  3.92592988]train_grp_loss: [20.10177042 11.54190062 19.12298946], val_grp_loss: [20.31493542  8.48114427 19.25727558], train_hist_grp_loss: [2106.4014857  1297.13976731 2027.77626703], cur_train_grp_loss: [20.11136004 11.55474049 19.13533305],max_reward_err:  0.0637, max_reward_err_index: 0, max_kl_dist:  0.7423, max_kl_dist_index: 0, max_train_grp_loss:  20.1018, max_train_grp_loss_index: 0, max_val_grp_loss:  20.3149, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.1114, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:18:55,582 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  15.6575, val_loss:  14.9825, grad_norm: 0.1721, reward_err: 0.0679, 0.0069, 0.0341, KL_dist: 1.0620, 0.5795, 0.8502, param: [9.14660428 4.86284339 7.86264198 4.11582078]train_grp_loss: [19.52994701 10.94662776 18.37074982], val_grp_loss: [19.80468014  7.31917544 18.42575737], train_hist_grp_loss: [4063.43315502 2402.34134985 3877.80320835], cur_train_grp_loss: [19.5331962 10.9487623 18.375166 ],max_reward_err:  0.0679, max_reward_err_index: 0, max_kl_dist:  1.0620, max_kl_dist_index: 0, max_train_grp_loss:  19.5299, max_train_grp_loss_index: 0, max_val_grp_loss:  19.8047, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.5332, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:18:55,805 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [9.14660428 4.86284339 7.86264198 4.11582078].
2024-09-19 00:18:56,138 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8345, 3.8345, 3.1912
2024-09-19 00:18:56,138 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8662, 3.8662, 3.3225
2024-09-19 00:18:56,139 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.6038, 3.8394, 3.2092
2024-09-19 00:18:56,139 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0679, 0.0069, 0.0341
2024-09-19 00:18:56,820 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8662, 3.8662, 3.3225
Known param reward: [[3.866189479827881, 3.537107467651367, 3.2731680870056152], [3.537107467651367, 3.866189479827881, 3.16263484954834], [3.823214292526245, 3.6324574947357178, 3.322507381439209]], Known param reward error: [[0.0, 0.08511792137801898, 0.014850018004240361], [0.08511792137801898, 0.0, 0.048118036632206734], [0.011115644364007984, 0.06045538800197879, 0.0]].
