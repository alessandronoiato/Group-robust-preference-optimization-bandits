2024-09-19 00:19:30,748 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.6_[1,0.6,1]_2024
2024-09-19 00:19:30,750 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2024
2024-09-19 00:19:30,750 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:19:30,914 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4614, l2 distance: 14.2156, acc: 0.81.
2024-09-19 00:19:30,915 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:19:30,916 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [7.12777926 3.69118221 7.29367386 4.24080502]
2024-09-19 00:19:31,127 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5381, 3.7683, 3.1486
2024-09-19 00:19:32,528 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  18.7978, val_loss:  18.8681, grad_norm: 0.6257, reward_err: 0.0489, 0.0193, 0.0201, KL_dist: 0.2504, 0.1290, 0.1560, param: [4.2245897  3.02841678 3.68701795 3.04070248]train_grp_loss: [21.57158617 15.92060317 20.15818271], val_grp_loss: [21.44810186 13.94016673 21.04378225], train_hist_grp_loss: [21.59479419 15.97416341 20.1954156 ], cur_train_grp_loss: [21.59479419 15.97416341 20.1954156 ],max_reward_err:  0.0489, max_reward_err_index: 0, max_kl_dist:  0.2504, max_kl_dist_index: 0, max_train_grp_loss:  21.5716, max_train_grp_loss_index: 0, max_val_grp_loss:  21.4481, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.5948, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:19:35,747 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  16.8836, val_loss:  16.5344, grad_norm: 0.2902, reward_err: 0.0583, 0.0132, 0.0266, KL_dist: 0.6995, 0.4040, 0.5325, param: [6.78462788 3.53251824 6.47572954 4.93318205]train_grp_loss: [20.26167071 13.60173218 17.79821667], val_grp_loss: [20.02402704 10.11481158 19.24632953], train_hist_grp_loss: [2100.60946827 1458.53101046 1900.72525852], cur_train_grp_loss: [20.26833236 13.60986673 17.81261818],max_reward_err:  0.0583, max_reward_err_index: 0, max_kl_dist:  0.6995, max_kl_dist_index: 0, max_train_grp_loss:  20.2617, max_train_grp_loss_index: 0, max_val_grp_loss:  20.0240, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.2683, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:19:38,872 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  16.4191, val_loss:  15.6629, grad_norm: 0.1551, reward_err: 0.0652, 0.0082, 0.0320, KL_dist: 0.9920, 0.5657, 0.7783, param: [8.11492781 3.75430152 8.04385485 5.35154017]train_grp_loss: [19.87760061 13.21622839 16.8304992 ], val_grp_loss: [19.58982423  8.59745451 18.56552263], train_hist_grp_loss: [4084.05611582 2781.07843073 3608.99501428], cur_train_grp_loss: [19.87966292 13.21770557 16.8369694 ],max_reward_err:  0.0652, max_reward_err_index: 0, max_kl_dist:  0.9920, max_kl_dist_index: 0, max_train_grp_loss:  19.8776, max_train_grp_loss_index: 0, max_val_grp_loss:  19.5898, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.8797, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:19:39,094 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [8.11492781 3.75430152 8.04385485 5.35154017].
2024-09-19 00:19:39,427 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7483, 3.7483, 3.1113
2024-09-19 00:19:39,428 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.7937, 3.7937, 3.2584
2024-09-19 00:19:39,428 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5462, 3.7625, 3.1542
2024-09-19 00:19:39,429 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0652, 0.0082, 0.0320
2024-09-19 00:19:40,118 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.7937, 3.7937, 3.2584
Known param reward: [[3.7937493324279785, 3.4302093982696533, 3.224942445755005], [3.4302093982696533, 3.7937493324279785, 3.071303129196167], [3.7626752853393555, 3.56032395362854, 3.2583768367767334]], Known param reward error: [[0.0, 0.09582602916088334, 0.010261057175572927], [0.09582602916088334, 0.0, 0.057413159051800874], [0.008190854051165217, 0.061528940988319736, 0.0]].
