2024-09-19 00:18:24,560 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.6_[1,0.6,1]_2021
2024-09-19 00:18:24,562 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2021
2024-09-19 00:18:24,563 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:18:24,727 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4881, l2 distance: 12.8925, acc: 0.78.
2024-09-19 00:18:24,728 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:18:24,729 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [7.49008496 2.76987425 6.30387609 1.756958  ]
2024-09-19 00:18:24,939 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.4326, 3.8265, 3.0841
2024-09-19 00:18:26,401 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.6308, val_loss:  21.5170, grad_norm: 0.8749, reward_err: 0.0029, 0.1172, 0.0219, KL_dist: 0.0463, 0.4215, 0.0915, param: [1.58972602 4.78288195 0.61263037 4.04507777]train_grp_loss: [21.3279136  24.35589553 21.57509488], val_grp_loss: [21.47226201 21.41009551 21.67235424], train_hist_grp_loss: [21.33910039 24.51426569 21.60885871], cur_train_grp_loss: [21.33910039 24.51426569 21.60885871],max_reward_err:  0.1172, max_reward_err_index: 1, max_kl_dist:  0.4215, max_kl_dist_index: 1, max_train_grp_loss:  24.3559, max_train_grp_loss_index: 1, max_val_grp_loss:  21.6724, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  24.5143, max_cur_train_grp_loss_index: 1, 
2024-09-19 00:18:29,680 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  18.4679, val_loss:  17.2806, grad_norm: 0.4530, reward_err: 0.0665, 0.0113, 0.0323, KL_dist: 0.4770, 0.2202, 0.3411, param: [5.88164273 3.33132793 5.02628107 3.61251566]train_grp_loss: [20.66123418 15.95122379 19.2761476 ], val_grp_loss: [20.88928041 11.00580365 20.10465219], train_hist_grp_loss: [2115.11931558 1949.27955225 2050.22521483], cur_train_grp_loss: [20.66495781 15.99049634 19.2913975 ],max_reward_err:  0.0665, max_reward_err_index: 0, max_kl_dist:  0.4770, max_kl_dist_index: 0, max_train_grp_loss:  20.6612, max_train_grp_loss_index: 0, max_val_grp_loss:  20.8893, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.6650, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:18:32,839 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.3015, val_loss:  15.8870, grad_norm: 0.2489, reward_err: 0.0931, 0.0009, 0.0537, KL_dist: 0.9499, 0.3903, 0.7205, param: [8.24561461 3.06727385 7.34797414 3.18447706]train_grp_loss: [20.42800765 13.81541879 18.20015348], val_grp_loss: [20.73789304  7.70695995 19.42040908], train_hist_grp_loss: [4147.32072656 3401.04064468 3899.67564919], cur_train_grp_loss: [20.42942694 13.82578976 18.20773119],max_reward_err:  0.0931, max_reward_err_index: 0, max_kl_dist:  0.9499, max_kl_dist_index: 0, max_train_grp_loss:  20.4280, max_train_grp_loss_index: 0, max_val_grp_loss:  20.7379, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.4294, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:18:33,062 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [8.24561461 3.06727385 7.34797414 3.18447706].
2024-09-19 00:18:33,394 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.7957, 3.7957, 3.1631
2024-09-19 00:18:33,395 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8296, 3.8296, 3.2923
2024-09-19 00:18:33,395 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4731, 3.8263, 3.1156
2024-09-19 00:18:33,396 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0931, 0.0009, 0.0537
2024-09-19 00:18:34,079 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8296, 3.8296, 3.2923
Known param reward: [[3.8295645713806152, 3.440261125564575, 3.2583417892456055], [3.440261125564575, 3.8295645713806152, 3.092778205871582], [3.7937707901000977, 3.5698022842407227, 3.2923295497894287]], Known param reward error: [[0.0, 0.10165736562464864, 0.010323316675876822], [0.10165736562464864, 0.0, 0.0606109871141574], [0.009346697415161585, 0.06783076307974209, 0.0]].
