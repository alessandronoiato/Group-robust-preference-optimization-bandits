2024-09-19 00:16:36,426 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_19_00_03_26/swapped_noise0.7_[1,0.7,1]_2026
2024-09-19 00:16:36,428 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2026
2024-09-19 00:16:36,428 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-19 00:16:36,592 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.4813, l2 distance: 14.8904, acc: 0.78.
2024-09-19 00:16:36,593 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-19 00:16:36,594 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [7.67816127 4.45673794 7.13735006 4.03215563]
2024-09-19 00:16:36,798 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5911, 3.8424, 3.2513
2024-09-19 00:16:38,216 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.7022, val_loss:  19.1859, grad_norm: 0.5715, reward_err: 0.0511, 0.0313, 0.0229, KL_dist: 0.2109, 0.1328, 0.1272, param: [4.15958867 3.63299935 3.04866501 2.36009887]train_grp_loss: [21.61721193 16.69338259 21.44145547], val_grp_loss: [21.35269257 14.81741453 21.08435595], train_hist_grp_loss: [21.64030026 16.73913799 21.46943223], cur_train_grp_loss: [21.64030026 16.73913799 21.46943223],max_reward_err:  0.0511, max_reward_err_index: 0, max_kl_dist:  0.2109, max_kl_dist_index: 0, max_train_grp_loss:  21.6172, max_train_grp_loss_index: 0, max_val_grp_loss:  21.3527, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.6403, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:16:41,418 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  100, train_loss:  17.9368, val_loss:  16.6457, grad_norm: 0.2949, reward_err: 0.0652, 0.0136, 0.0320, KL_dist: 0.6240, 0.3408, 0.4694, param: [6.79696545 4.40553763 5.66948665 3.97391767]train_grp_loss: [20.24518378 14.48702046 19.60020931], val_grp_loss: [19.72881603 10.63131294 19.15282857], train_hist_grp_loss: [2102.84781384 1547.89097903 2061.21940325], cur_train_grp_loss: [20.25269426 14.49584298 19.61190039],max_reward_err:  0.0652, max_reward_err_index: 0, max_kl_dist:  0.6240, max_kl_dist_index: 0, max_train_grp_loss:  20.2452, max_train_grp_loss_index: 0, max_val_grp_loss:  19.7288, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.2527, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:16:44,616 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  199, train_loss:  17.4409, val_loss:  15.6180, grad_norm: 0.1626, reward_err: 0.0724, 0.0092, 0.0373, KL_dist: 0.9139, 0.4942, 0.7124, param: [8.27509868 4.66608124 7.16761203 4.50387156]train_grp_loss: [19.78917604 14.06733154 18.80015501], val_grp_loss: [19.11440776  8.94088167 18.32272136], train_hist_grp_loss: [4080.9029357  2955.81547651 3957.43033636], cur_train_grp_loss: [19.79181436 14.06882217 18.80559743],max_reward_err:  0.0724, max_reward_err_index: 0, max_kl_dist:  0.9139, max_kl_dist_index: 0, max_train_grp_loss:  19.7892, max_train_grp_loss_index: 0, max_val_grp_loss:  19.1144, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  19.7918, max_cur_train_grp_loss_index: 0, 
2024-09-19 00:16:44,845 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [8.27509868 4.66608124 7.16761203 4.50387156].
2024-09-19 00:16:45,180 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8296, 3.8296, 3.2253
2024-09-19 00:16:45,180 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8699, 3.8699, 3.3751
2024-09-19 00:16:45,181 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5896, 3.8345, 3.2491
2024-09-19 00:16:45,181 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0724, 0.0092, 0.0373
2024-09-19 00:16:45,866 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8699, 3.8699, 3.3751
Known param reward: [[3.8699047565460205, 3.4682676792144775, 3.3350672721862793], [3.4682676792144775, 3.8699047565460205, 3.164400100708008], [3.831815481185913, 3.603090763092041, 3.375126361846924]], Known param reward error: [[0.0, 0.10378474474137016, 0.011868915520758028], [0.10378474474137016, 0.0, 0.06243507310452317], [0.009842432244793276, 0.06894588116223231, 0.0]].
