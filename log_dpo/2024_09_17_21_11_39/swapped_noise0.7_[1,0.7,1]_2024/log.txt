2024-09-17 21:12:39,683 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_21_11_39/swapped_noise0.7_[1,0.7,1]_2024
2024-09-17 21:12:39,685 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2024
2024-09-17 21:12:39,686 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 21:12:39,842 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.3935, l2 distance: 30.0930, acc: 0.80.
2024-09-17 21:12:39,843 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 21:12:39,844 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [12.32517611  7.89586346 12.78824134  8.17174951]
2024-09-17 21:12:40,042 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5737, 3.7546, 3.1610
2024-09-17 21:12:41,324 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.1993, val_loss:  20.3734, grad_norm: 0.6923, reward_err: 0.0444, 0.0710, 0.0210, KL_dist: 0.1432, 0.2347, 0.0886, param: [2.19457478 1.46980008 3.67725058 4.76968557]train_grp_loss: [22.02350954 17.02764697 21.09365815], val_grp_loss: [21.41685973 17.94408644 21.63615853], train_hist_grp_loss: [22.04966551 17.10941306 21.13395896], cur_train_grp_loss: [22.04966551 17.10941306 21.13395896],max_reward_err:  0.0710, max_reward_err_index: 1, max_kl_dist:  0.2347, max_kl_dist_index: 1, max_train_grp_loss:  22.0235, max_train_grp_loss_index: 0, max_val_grp_loss:  21.6362, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.0497, max_cur_train_grp_loss_index: 0, 
2024-09-17 21:12:44,291 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.3114, val_loss:  17.5396, grad_norm: 0.4094, reward_err: 0.0524, 0.0257, 0.0233, KL_dist: 0.6407, 0.4492, 0.5020, param: [5.39718715 4.00163183 6.96280312 5.47744412]train_grp_loss: [20.32407425 12.70444809 18.24760245], val_grp_loss: [19.26547646 13.59057261 19.56306608], train_hist_grp_loss: [2106.71699548 1442.39083048 1953.65553305], cur_train_grp_loss: [20.33476344 12.72432104 18.2676408 ],max_reward_err:  0.0524, max_reward_err_index: 0, max_kl_dist:  0.6407, max_kl_dist_index: 0, max_train_grp_loss:  20.3241, max_train_grp_loss_index: 0, max_val_grp_loss:  19.5631, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  20.3348, max_cur_train_grp_loss_index: 0, 
2024-09-17 21:12:44,501 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [5.39718715 4.00163183 6.96280312 5.47744412].
2024-09-17 21:12:44,822 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7502, 3.7502, 3.1027
2024-09-17 21:12:44,822 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.7949, 3.7949, 3.2474
2024-09-17 21:12:44,823 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5963, 3.6974, 3.1716
2024-09-17 21:12:44,823 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0524, 0.0257, 0.0233
2024-09-17 21:12:45,473 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.7949, 3.7949, 3.2474
Known param reward: [[3.7949471473693848, 3.43835186958313, 3.2166433334350586], [3.43835186958313, 3.7949471473693848, 3.0654006004333496], [3.762385606765747, 3.5652241706848145, 3.2474141120910645]], Known param reward error: [[0.0, 0.09396580872896815, 0.009475471126838221], [0.09396580872896815, 0.0, 0.056048753061713245], [0.008580235597275445, 0.060533906735384106, 0.0]].
