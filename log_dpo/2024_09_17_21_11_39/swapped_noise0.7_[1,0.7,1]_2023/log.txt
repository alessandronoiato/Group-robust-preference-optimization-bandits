2024-09-17 21:12:20,956 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_21_11_39/swapped_noise0.7_[1,0.7,1]_2023
2024-09-17 21:12:20,958 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2023
2024-09-17 21:12:20,958 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 21:12:21,116 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4124, l2 distance: 22.4024, acc: 0.81.
2024-09-17 21:12:21,117 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 21:12:21,118 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [12.40847497  5.08652957  7.69270734  5.28267459]
2024-09-17 21:12:21,323 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4590, 3.7859, 3.0700
2024-09-17 21:12:22,610 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.7938, val_loss:  20.8829, grad_norm: 0.7840, reward_err: 0.0500, 0.0884, 0.0363, KL_dist: 0.1109, 0.2051, 0.0982, param: [0.23103444 3.97217849 3.44020203 2.26805369]train_grp_loss: [22.32231298 18.06776443 22.24164556], val_grp_loss: [22.11723868 18.42821346 22.06875255], train_hist_grp_loss: [22.36013609 18.17446765 22.27663319], cur_train_grp_loss: [22.36013609 18.17446765 22.27663319],max_reward_err:  0.0884, max_reward_err_index: 1, max_kl_dist:  0.2051, max_kl_dist_index: 1, max_train_grp_loss:  22.3223, max_train_grp_loss_index: 0, max_val_grp_loss:  22.1172, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.3601, max_cur_train_grp_loss_index: 0, 
2024-09-17 21:12:25,679 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.2746, val_loss:  17.8501, grad_norm: 0.4386, reward_err: 0.0610, 0.0221, 0.0278, KL_dist: 0.5284, 0.3537, 0.4086, param: [4.7321774  4.80641591 6.55636833 4.05925247]train_grp_loss: [19.97903725 12.24636562 20.06770925], val_grp_loss: [20.70441    13.27067538 19.55486125], train_hist_grp_loss: [2098.57083496 1461.30482102 2100.25272726], cur_train_grp_loss: [19.99310835 12.27579599 20.0807849 ],max_reward_err:  0.0610, max_reward_err_index: 0, max_kl_dist:  0.5284, max_kl_dist_index: 0, max_train_grp_loss:  20.0677, max_train_grp_loss_index: 2, max_val_grp_loss:  20.7044, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.0808, max_cur_train_grp_loss_index: 2, 
2024-09-17 21:12:25,895 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [4.7321774  4.80641591 6.55636833 4.05925247].
2024-09-17 21:12:26,214 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7702, 3.7702, 3.1198
2024-09-17 21:12:26,214 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8048, 3.8048, 3.2433
2024-09-17 21:12:26,215 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5726, 3.7207, 3.1531
2024-09-17 21:12:26,215 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0610, 0.0221, 0.0278
2024-09-17 21:12:26,884 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8048, 3.8048, 3.2433
Known param reward: [[3.8048343658447266, 3.420158624649048, 3.208865165710449], [3.420158624649048, 3.8048343658447266, 3.0513827800750732], [3.7656967639923096, 3.5444085597991943, 3.2432823181152344]], Known param reward error: [[0.0, 0.10110183629774783, 0.010611827472603731], [0.10110183629774783, 0.0, 0.059168311364173665], [0.01028628268387917, 0.06844602970981473, 0.0]].
