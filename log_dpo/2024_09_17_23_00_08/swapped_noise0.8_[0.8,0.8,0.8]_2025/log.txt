2024-09-17 23:01:29,970 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_00_08/swapped_noise0.8_[0.8,0.8,0.8]_2025
2024-09-17 23:01:29,972 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2025
2024-09-17 23:01:29,973 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:01:30,130 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4236, l2 distance: 24.0392, acc: 0.79.
2024-09-17 23:01:30,130 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:01:30,131 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [10.95146557  6.00894238 10.92407557  5.19890253]
2024-09-17 23:01:30,335 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5852, 3.8496, 3.1956
2024-09-17 23:01:31,595 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.5413, val_loss:  21.0600, grad_norm: 0.8176, reward_err: 0.0224, 0.0766, 0.0134, KL_dist: 0.0630, 0.2572, 0.0597, param: [2.88918386 4.28643184 0.85758601 2.86642491]train_grp_loss: [22.57702406 20.36662092 21.9032235 ], val_grp_loss: [22.32031486 18.87440019 22.06817881], train_hist_grp_loss: [22.58110275 20.52401778 21.92538149], cur_train_grp_loss: [22.58110275 20.52401778 21.92538149],max_reward_err:  0.0766, max_reward_err_index: 1, max_kl_dist:  0.2572, max_kl_dist_index: 1, max_train_grp_loss:  22.5770, max_train_grp_loss_index: 0, max_val_grp_loss:  22.3203, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.5811, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:01:34,699 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.5739, val_loss:  17.7526, grad_norm: 0.4703, reward_err: 0.0709, 0.0133, 0.0367, KL_dist: 0.6723, 0.3265, 0.5093, param: [7.27492043 4.29994165 5.22158417 3.22377791]train_grp_loss: [22.51299045 10.95920237 20.45874736], val_grp_loss: [21.82326516 11.42739312 20.26792947], train_hist_grp_loss: [2250.34874891 1492.49756745 2109.23338925], cur_train_grp_loss: [22.51167585 11.01184583 20.46797611],max_reward_err:  0.0709, max_reward_err_index: 0, max_kl_dist:  0.6723, max_kl_dist_index: 0, max_train_grp_loss:  22.5130, max_train_grp_loss_index: 0, max_val_grp_loss:  21.8233, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.5117, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:01:34,923 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [7.27492043 4.29994165 5.22158417 3.22377791].
2024-09-17 23:01:35,242 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8311, 3.8311, 3.1785
2024-09-17 23:01:35,243 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8728, 3.8728, 3.3212
2024-09-17 23:01:35,243 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5983, 3.8213, 3.1994
2024-09-17 23:01:35,244 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0709, 0.0133, 0.0367
2024-09-17 23:01:35,913 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8728, 3.8728, 3.3212
Known param reward: [[3.872750997543335, 3.4923548698425293, 3.280984401702881], [3.4923548698425293, 3.872750997543335, 3.128255605697632], [3.8364126682281494, 3.625859260559082, 3.321218729019165]], Known param reward error: [[0.0, 0.09822375049212008, 0.012114326275694084], [0.09822375049212008, 0.0, 0.05810009489453885], [0.00938307919570265, 0.06375099693754331, 0.0]].
