2024-09-17 23:01:09,519 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_00_08/swapped_noise0.8_[0.8,0.8,0.8]_2024
2024-09-17 23:01:09,521 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2024
2024-09-17 23:01:09,522 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:01:09,681 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4566, l2 distance: 20.3015, acc: 0.77.
2024-09-17 23:01:09,682 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:01:09,683 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [10.5690194   4.65715223  8.57528304  4.72120913]
2024-09-17 23:01:09,877 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5722, 3.9111, 3.2469
2024-09-17 23:01:11,128 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.1630, val_loss:  22.2035, grad_norm: 0.7647, reward_err: 0.0889, 0.0196, 0.0561, KL_dist: 0.1358, 0.0354, 0.0791, param: [0.83314074 1.51937363 3.05938572 1.34585842]train_grp_loss: [23.45858673 19.09982636 23.49287209], val_grp_loss: [23.63106462 19.51746266 23.32636828], train_hist_grp_loss: [23.48881094 19.21511    23.53042292], cur_train_grp_loss: [23.48881094 19.21511    23.53042292],max_reward_err:  0.0889, max_reward_err_index: 0, max_kl_dist:  0.1358, max_kl_dist_index: 0, max_train_grp_loss:  23.4929, max_train_grp_loss_index: 2, max_val_grp_loss:  23.6311, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.5304, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:01:14,251 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.6651, val_loss:  19.0497, grad_norm: 0.4493, reward_err: 0.0817, 0.0091, 0.0420, KL_dist: 0.5282, 0.1981, 0.3774, param: [4.97927811 3.23589435 6.29479542 3.14334181]train_grp_loss: [21.55862479 12.6239753  20.94956314], val_grp_loss: [22.0060879  13.69553247 21.17661327], train_hist_grp_loss: [2237.73704486 1528.88593108 2208.28464585], cur_train_grp_loss: [21.57001494 12.65790877 20.96661961],max_reward_err:  0.0817, max_reward_err_index: 0, max_kl_dist:  0.5282, max_kl_dist_index: 0, max_train_grp_loss:  21.5586, max_train_grp_loss_index: 0, max_val_grp_loss:  22.0061, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.5700, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:01:14,465 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [4.97927811 3.23589435 6.29479542 3.14334181].
2024-09-17 23:01:14,796 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8875, 3.8875, 3.2769
2024-09-17 23:01:14,797 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.9272, 3.9272, 3.4129
2024-09-17 23:01:14,797 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6062, 3.8913, 3.2696
2024-09-17 23:01:14,798 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0817, 0.0091, 0.0420
2024-09-17 23:01:15,462 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.9272, 3.9272, 3.4129
Known param reward: [[3.9271814823150635, 3.503054618835449, 3.378288984298706], [3.503054618835449, 3.9271814823150635, 3.1996068954467773], [3.8862557411193848, 3.636671543121338, 3.4129228591918945]], Known param reward error: [[0.0, 0.1079977753484396, 0.0101478633775476], [0.1079977753484396, 0.0, 0.06250242755138795], [0.010421148444495387, 0.07397415691175818, 0.0]].
