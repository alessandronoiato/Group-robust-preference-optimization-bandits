2024-09-17 23:03:03,890 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_00_08/swapped_noise0.8_[0.8,0.8,0.8]_2030
2024-09-17 23:03:03,892 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2030
2024-09-17 23:03:03,892 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:03:04,062 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4085, l2 distance: 24.8006, acc: 0.82.
2024-09-17 23:03:04,062 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:03:04,063 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [10.58796089  5.22407085 12.2021509   5.10448009]
2024-09-17 23:03:04,271 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4633, 3.7848, 3.1291
2024-09-17 23:03:05,580 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.7627, val_loss:  20.7994, grad_norm: 0.7431, reward_err: 0.0272, 0.0743, 0.0194, KL_dist: 0.1625, 0.3667, 0.1666, param: [3.8465734  4.97465702 0.8222224  4.06359672]train_grp_loss: [21.88702327 19.44893046 21.13601699], val_grp_loss: [21.76389304 18.91648059 21.53320059], train_hist_grp_loss: [21.89688013 19.57375775 21.15901579], cur_train_grp_loss: [21.89688013 19.57375775 21.15901579],max_reward_err:  0.0743, max_reward_err_index: 1, max_kl_dist:  0.3667, max_kl_dist_index: 1, max_train_grp_loss:  21.8870, max_train_grp_loss_index: 0, max_val_grp_loss:  21.7639, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.8969, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:03:08,805 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.3277, val_loss:  18.2874, grad_norm: 0.4521, reward_err: 0.0661, 0.0160, 0.0322, KL_dist: 0.6789, 0.3662, 0.5246, param: [7.48648085 4.6973629  5.29048726 3.73687239]train_grp_loss: [21.34392937 11.70294194 19.60191948], val_grp_loss: [21.55563537 12.37318915 20.33346207], train_hist_grp_loss: [2156.29620077 1502.20890314 2028.06336518], cur_train_grp_loss: [21.34648976 11.7485633  19.61195711],max_reward_err:  0.0661, max_reward_err_index: 0, max_kl_dist:  0.6789, max_kl_dist_index: 0, max_train_grp_loss:  21.3439, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5556, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.3465, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:03:09,043 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [7.48648085 4.6973629  5.29048726 3.73687239].
2024-09-17 23:03:09,379 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7556, 3.7556, 3.1458
2024-09-17 23:03:09,380 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.7933, 3.7933, 3.2870
2024-09-17 23:03:09,380 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5424, 3.7325, 3.1812
2024-09-17 23:03:09,381 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0661, 0.0160, 0.0322
2024-09-17 23:03:10,073 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.7933, 3.7933, 3.2870
Known param reward: [[3.793292999267578, 3.4222915172576904, 3.2522172927856445], [3.4222915172576904, 3.793292999267578, 3.1010961532592773], [3.759493589401245, 3.5484824180603027, 3.287027359008789]], Known param reward error: [[0.0, 0.0978045940773681, 0.010590135834355084], [0.0978045940773681, 0.0, 0.0565651530827476], [0.008910308239532014, 0.0645377462944582, 0.0]].
