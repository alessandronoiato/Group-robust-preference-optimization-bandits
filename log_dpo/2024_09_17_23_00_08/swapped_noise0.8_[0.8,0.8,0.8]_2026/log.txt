2024-09-17 23:01:49,549 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_00_08/swapped_noise0.8_[0.8,0.8,0.8]_2026
2024-09-17 23:01:49,551 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2026
2024-09-17 23:01:49,552 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:01:49,712 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5020, l2 distance: 14.5300, acc: 0.74.
2024-09-17 23:01:49,713 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:01:49,714 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [8.12325045 3.75093326 6.69577588 3.496602  ]
2024-09-17 23:01:49,926 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5262, 3.8545, 3.1910
2024-09-17 23:01:51,208 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.4359, val_loss:  19.7898, grad_norm: 0.5697, reward_err: 0.0453, 0.0401, 0.0184, KL_dist: 0.2309, 0.1839, 0.1447, param: [3.35679286 4.36323247 4.3141093  1.98672649]train_grp_loss: [23.21289294 16.64107364 21.41147003], val_grp_loss: [22.1404941  15.41539212 21.51809648], train_hist_grp_loss: [23.21764046 16.71467406 21.43032552], cur_train_grp_loss: [23.21764046 16.71467406 21.43032552],max_reward_err:  0.0453, max_reward_err_index: 0, max_kl_dist:  0.2309, max_kl_dist_index: 0, max_train_grp_loss:  23.2129, max_train_grp_loss_index: 0, max_val_grp_loss:  22.1405, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.2176, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:01:54,297 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.6095, val_loss:  18.0442, grad_norm: 0.3112, reward_err: 0.0704, 0.0083, 0.0361, KL_dist: 0.6763, 0.3304, 0.5042, param: [6.48429106 4.19552468 6.66862269 3.47013823]train_grp_loss: [23.04485717 12.57151387 20.14259095], val_grp_loss: [21.44837638 11.92909523 20.33455526], train_hist_grp_loss: [2309.19375191 1423.52096349 2070.69379332], cur_train_grp_loss: [23.04469366 12.59247331 20.151073  ],max_reward_err:  0.0704, max_reward_err_index: 0, max_kl_dist:  0.6763, max_kl_dist_index: 0, max_train_grp_loss:  23.0449, max_train_grp_loss_index: 0, max_val_grp_loss:  21.4484, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.0447, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:01:54,512 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.48429106 4.19552468 6.66862269 3.47013823].
2024-09-17 23:01:54,834 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8279, 3.8279, 3.2154
2024-09-17 23:01:54,834 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8695, 3.8695, 3.3645
2024-09-17 23:01:54,835 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5971, 3.8372, 3.2429
2024-09-17 23:01:54,835 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0704, 0.0083, 0.0361
2024-09-17 23:01:55,501 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8695, 3.8695, 3.3645
Known param reward: [[3.8695008754730225, 3.466553211212158, 3.324392795562744], [3.466553211212158, 3.8695008754730225, 3.1501362323760986], [3.8326010704040527, 3.604762315750122, 3.364454984664917]], Known param reward error: [[0.0, 0.10413427396152389, 0.011907482574376856], [0.10413427396152389, 0.0, 0.063700882688482], [0.009536063243417396, 0.0684167204615344, 0.0]].
