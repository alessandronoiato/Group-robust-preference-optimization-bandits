2024-09-17 23:00:50,549 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_00_08/swapped_noise0.8_[0.8,0.8,0.8]_2023
2024-09-17 23:00:50,551 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2023
2024-09-17 23:00:50,552 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:00:50,709 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5051, l2 distance: 13.0237, acc: 0.73.
2024-09-17 23:00:50,710 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:00:50,711 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.76166032 4.28477317 7.63729755 1.95215171]
2024-09-17 23:00:50,909 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5214, 3.8432, 3.1369
2024-09-17 23:00:52,174 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.6066, val_loss:  19.1769, grad_norm: 0.5938, reward_err: 0.0618, 0.0543, 0.0334, KL_dist: 0.2535, 0.2280, 0.1839, param: [5.00590955 2.09761642 2.45049901 4.88406727]train_grp_loss: [22.77958553 17.55813566 21.73242643], val_grp_loss: [21.62142565 14.54816596 21.36383589], train_hist_grp_loss: [22.78147553 17.6407569  21.74941916], cur_train_grp_loss: [22.78147553 17.6407569  21.74941916],max_reward_err:  0.0618, max_reward_err_index: 0, max_kl_dist:  0.2535, max_kl_dist_index: 0, max_train_grp_loss:  22.7796, max_train_grp_loss_index: 0, max_val_grp_loss:  21.6214, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.7815, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:00:55,319 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.6017, val_loss:  17.2890, grad_norm: 0.3261, reward_err: 0.0794, 0.0098, 0.0436, KL_dist: 0.6577, 0.3205, 0.5028, param: [7.32477128 3.95684809 5.44214961 3.64998156]train_grp_loss: [22.81384269 12.86454562 20.59025565], val_grp_loss: [21.18579623 10.4456998  20.26007907], train_hist_grp_loss: [2276.95913787 1480.21453526 2109.79034182], cur_train_grp_loss: [22.81228814 12.88927338 20.59786953],max_reward_err:  0.0794, max_reward_err_index: 0, max_kl_dist:  0.6577, max_kl_dist_index: 0, max_train_grp_loss:  22.8138, max_train_grp_loss_index: 0, max_val_grp_loss:  21.1858, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.8123, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:00:55,536 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [7.32477128 3.95684809 5.44214961 3.64998156].
2024-09-17 23:00:55,868 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8265, 3.8265, 3.1778
2024-09-17 23:00:55,869 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8671, 3.8671, 3.3081
2024-09-17 23:00:55,869 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5599, 3.8293, 3.1640
2024-09-17 23:00:55,870 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0794, 0.0098, 0.0436
2024-09-17 23:00:56,545 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8671, 3.8671, 3.3081
Known param reward: [[3.867140769958496, 3.49029541015625, 3.2803642749786377], [3.49029541015625, 3.867140769958496, 3.118959903717041], [3.833752155303955, 3.6075756549835205, 3.3081343173980713]], Known param reward error: [[0.0, 0.09744805845438374, 0.00839447245941193], [0.09744805845438374, 0.0, 0.05718462297196584], [0.008633927917472566, 0.06712067918276514, 0.0]].
