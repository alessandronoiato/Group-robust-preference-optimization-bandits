2024-09-17 23:00:31,830 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_00_08/swapped_noise0.8_[0.8,0.8,0.8]_2022
2024-09-17 23:00:31,832 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2022
2024-09-17 23:00:31,833 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:00:31,997 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4589, l2 distance: 19.0000, acc: 0.79.
2024-09-17 23:00:31,997 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:00:31,998 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [9.24563481 4.7262934  8.86667748 4.71278162]
2024-09-17 23:00:32,209 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.6158, 3.8500, 3.2174
2024-09-17 23:00:33,526 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.9383, val_loss:  22.1238, grad_norm: 0.7628, reward_err: 0.0093, 0.0631, 0.0002, KL_dist: 0.0126, 0.1285, 0.0005, param: [1.35337278 2.29505366 1.50184183 2.67586613]train_grp_loss: [22.78440489 20.1501547  22.90431456], val_grp_loss: [22.91948719 20.49929933 22.89318221], train_hist_grp_loss: [22.80652379 20.26623221 22.93897123], cur_train_grp_loss: [22.80652379 20.26623221 22.93897123],max_reward_err:  0.0631, max_reward_err_index: 1, max_kl_dist:  0.1285, max_kl_dist_index: 1, max_train_grp_loss:  22.9043, max_train_grp_loss_index: 2, max_val_grp_loss:  22.9195, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.9390, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:00:36,760 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.4834, val_loss:  18.6708, grad_norm: 0.4423, reward_err: 0.0514, 0.0187, 0.0211, KL_dist: 0.4349, 0.2527, 0.3173, param: [5.28284797 3.01652105 5.31258053 4.38825291]train_grp_loss: [21.4679272  13.47029628 20.60913609], val_grp_loss: [21.16908252 14.29549962 20.40508059], train_hist_grp_loss: [2202.14439587 1624.4527377  2162.24292064], cur_train_grp_loss: [21.47521067 13.50614241 20.62409336],max_reward_err:  0.0514, max_reward_err_index: 0, max_kl_dist:  0.4349, max_kl_dist_index: 0, max_train_grp_loss:  21.4679, max_train_grp_loss_index: 0, max_val_grp_loss:  21.1691, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.4752, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:00:36,981 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [5.28284797 3.01652105 5.31258053 4.38825291].
2024-09-17 23:00:37,305 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8413, 3.8413, 3.1954
2024-09-17 23:00:37,306 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8718, 3.8718, 3.3255
2024-09-17 23:00:37,306 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6728, 3.7993, 3.2553
2024-09-17 23:00:37,307 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0514, 0.0187, 0.0211
2024-09-17 23:00:37,999 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8718, 3.8718, 3.3255
Known param reward: [[3.8717610836029053, 3.543931722640991, 3.2762246131896973], [3.543931722640991, 3.8717610836029053, 3.1653871536254883], [3.828972101211548, 3.640883684158325, 3.3254706859588623]], Known param reward error: [[0.0, 0.08467189836436118, 0.014808752630746911], [0.08467189836436118, 0.0, 0.04813860877177323], [0.01105155547241043, 0.05963110699737052, 0.0]].
