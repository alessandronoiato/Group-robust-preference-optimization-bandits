2024-09-17 21:38:53,775 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_21_36_55/swapped_noise0.6_[1,0.6,1]_2027
2024-09-17 21:38:53,777 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2027
2024-09-17 21:38:53,777 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 21:38:53,936 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4381, l2 distance: 23.5455, acc: 0.80.
2024-09-17 21:38:53,937 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 21:38:53,938 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [10.57644988  5.52515933  9.88716849  7.5816798 ]
2024-09-17 21:38:54,143 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5609, 3.7305, 3.1644
2024-09-17 21:38:55,431 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.7605, val_loss:  21.4647, grad_norm: 0.7720, reward_err: 0.0847, 0.0629, 0.0630, KL_dist: 0.2262, 0.1467, 0.1768, param: [4.37959331 3.68305501 0.33176239 0.92644878]train_grp_loss: [22.44479407 20.19054221 22.44344109], val_grp_loss: [22.45725427 20.23847406 21.64768808], train_hist_grp_loss: [22.47746028 20.31199744 22.47560089], cur_train_grp_loss: [22.47746028 20.31199744 22.47560089],max_reward_err:  0.0847, max_reward_err_index: 0, max_kl_dist:  0.2262, max_kl_dist_index: 0, max_train_grp_loss:  22.4448, max_train_grp_loss_index: 0, max_val_grp_loss:  22.4573, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.4775, max_cur_train_grp_loss_index: 0, 
2024-09-17 21:38:58,446 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.2648, val_loss:  18.2363, grad_norm: 0.4407, reward_err: 0.0777, 0.0129, 0.0447, KL_dist: 0.6264, 0.2940, 0.4825, param: [7.41734376 3.84679162 4.3414037  3.83147402]train_grp_loss: [20.28736008 13.63918777 20.27029354], val_grp_loss: [20.64408547 15.21186264 18.72755779], train_hist_grp_loss: [2123.79515737 1627.08095274 2123.70191433], cur_train_grp_loss: [20.30129666 13.67042122 20.28476236],max_reward_err:  0.0777, max_reward_err_index: 0, max_kl_dist:  0.6264, max_kl_dist_index: 0, max_train_grp_loss:  20.2874, max_train_grp_loss_index: 0, max_val_grp_loss:  20.6441, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.3013, max_cur_train_grp_loss_index: 0, 
2024-09-17 21:38:58,660 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [7.41734376 3.84679162 4.3414037  3.83147402].
2024-09-17 21:38:58,978 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7386, 3.7386, 3.1050
2024-09-17 21:38:58,979 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.7797, 3.7797, 3.2487
2024-09-17 21:38:58,979 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.4861, 3.7310, 3.1035
2024-09-17 21:38:58,980 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0777, 0.0129, 0.0447
2024-09-17 21:38:59,643 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.7797, 3.7797, 3.2487
Known param reward: [[3.779723644256592, 3.4200356006622314, 3.2069242000579834], [3.4200356006622314, 3.779723644256592, 3.065584421157837], [3.741098642349243, 3.5430009365081787, 3.248685598373413]], Known param reward error: [[0.0, 0.0951625244191907, 0.012854859927454733], [0.0951625244191907, 0.0, 0.05636161815943446], [0.01021900158389636, 0.06262963381148795, 0.0]].
