2024-09-17 23:49:47,688 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_47_28/swapped_noise0.5_[0.5,0.5,0.5]_2028
2024-09-17 23:49:47,690 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2028
2024-09-17 23:49:47,690 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:49:47,850 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5722, l2 distance: 8.8869, acc: 0.70.
2024-09-17 23:49:47,851 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:49:47,852 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.53230593 3.96086721 4.37459771 1.88868499]
2024-09-17 23:49:48,056 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5739, 3.7602, 3.1693
2024-09-17 23:49:49,307 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.0309, val_loss:  20.7894, grad_norm: 0.5238, reward_err: 0.0780, 0.0302, 0.0401, KL_dist: 0.3445, 0.1446, 0.2208, param: [4.59791442 0.86938956 4.29459385 3.88894427]train_grp_loss: [23.96009342 19.27030539 22.23847713], val_grp_loss: [23.58224434 15.6583216  22.82235989], train_hist_grp_loss: [23.96495217 19.34298299 22.25035852], cur_train_grp_loss: [23.96495217 19.34298299 22.25035852],max_reward_err:  0.0780, max_reward_err_index: 0, max_kl_dist:  0.3445, max_kl_dist_index: 0, max_train_grp_loss:  23.9601, max_train_grp_loss_index: 0, max_val_grp_loss:  23.5822, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.9650, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:49:52,358 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  20.5658, val_loss:  20.0619, grad_norm: 0.2698, reward_err: 0.0700, 0.0111, 0.0340, KL_dist: 0.5671, 0.2706, 0.4183, param: [6.31402211 3.90049872 5.48016948 3.22237949]train_grp_loss: [23.68641321 15.60802762 21.38823629], val_grp_loss: [23.17949732 14.48021936 22.18820369], train_hist_grp_loss: [2379.7760005  1703.86222481 2177.54467402], cur_train_grp_loss: [23.68772794 15.62430921 21.39433125],max_reward_err:  0.0700, max_reward_err_index: 0, max_kl_dist:  0.5671, max_kl_dist_index: 0, max_train_grp_loss:  23.6864, max_train_grp_loss_index: 0, max_val_grp_loss:  23.1795, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.6877, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:49:52,575 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.31402211 3.90049872 5.48016948 3.22237949].
2024-09-17 23:49:52,897 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8039, 3.8039, 3.1527
2024-09-17 23:49:52,897 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8439, 3.8439, 3.2842
2024-09-17 23:49:52,898 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5749, 3.8013, 3.1724
2024-09-17 23:49:52,898 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0700, 0.0111, 0.0340
2024-09-17 23:49:53,567 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8439, 3.8439, 3.2842
Known param reward: [[3.8439385890960693, 3.447842836380005, 3.2535243034362793], [3.447842836380005, 3.8439385890960693, 3.083505392074585], [3.807281970977783, 3.57368803024292, 3.2841625213623047]], Known param reward error: [[0.0, 0.1030442457742826, 0.009329080922985608], [0.1030442457742826, 0.0, 0.06109841640982038], [0.009536213253320004, 0.07030563901820837, 0.0]].
