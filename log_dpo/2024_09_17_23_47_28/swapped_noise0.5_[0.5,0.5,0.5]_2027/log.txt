2024-09-17 23:49:28,842 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_47_28/swapped_noise0.5_[0.5,0.5,0.5]_2027
2024-09-17 23:49:28,844 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2027
2024-09-17 23:49:28,844 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:49:29,006 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5420, l2 distance: 12.5755, acc: 0.71.
2024-09-17 23:49:29,007 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:49:29,008 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [7.79120092 0.77394543 5.17419383 3.27492704]
2024-09-17 23:49:29,216 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.3567, 3.7492, 2.9984
2024-09-17 23:49:30,488 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  23.2190, val_loss:  21.9145, grad_norm: 0.6842, reward_err: 0.0173, 0.0793, 0.0145, KL_dist: 0.0878, 0.3181, 0.0959, param: [0.78973889 4.14280631 3.01064476 4.20448467]train_grp_loss: [23.37971924 24.16663322 22.23865   ], val_grp_loss: [23.05897889 19.97819536 22.74539912], train_hist_grp_loss: [23.38035148 24.30636546 22.25029526], cur_train_grp_loss: [23.38035148 24.30636546 22.25029526],max_reward_err:  0.0793, max_reward_err_index: 1, max_kl_dist:  0.3181, max_kl_dist_index: 1, max_train_grp_loss:  24.1666, max_train_grp_loss_index: 1, max_val_grp_loss:  23.0590, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  24.3064, max_cur_train_grp_loss_index: 1, 
2024-09-17 23:49:33,581 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  20.4481, val_loss:  19.6704, grad_norm: 0.3969, reward_err: 0.0712, 0.0116, 0.0385, KL_dist: 0.4805, 0.1963, 0.3419, param: [4.64344373 2.52183154 6.01276433 3.67187046]train_grp_loss: [23.42108564 15.96161046 21.39178388], val_grp_loss: [22.92419145 13.86877115 22.33728723], train_hist_grp_loss: [2338.78658286 1940.41358603 2178.0871646 ], cur_train_grp_loss: [23.42013975 16.00726109 21.39805891],max_reward_err:  0.0712, max_reward_err_index: 0, max_kl_dist:  0.4805, max_kl_dist_index: 0, max_train_grp_loss:  23.4211, max_train_grp_loss_index: 0, max_val_grp_loss:  22.9242, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.4201, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:49:33,800 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [4.64344373 2.52183154 6.01276433 3.67187046].
2024-09-17 23:49:34,126 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7279, 3.7279, 3.0898
2024-09-17 23:49:34,126 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.7684, 3.7684, 3.2302
2024-09-17 23:49:34,127 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5002, 3.7248, 3.1059
2024-09-17 23:49:34,127 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0712, 0.0116, 0.0385
2024-09-17 23:49:34,802 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.7684, 3.7684, 3.2302
Known param reward: [[3.7684130668640137, 3.418842077255249, 3.1900854110717773], [3.418842077255249, 3.7684130668640137, 3.053025007247925], [3.7308881282806396, 3.5340662002563477, 3.2301623821258545]], Known param reward error: [[0.0, 0.092763448010138, 0.012407107232702476], [0.092763448010138, 0.0, 0.05483853562846303], [0.00995775620070265, 0.06218714945776474, 0.0]].
