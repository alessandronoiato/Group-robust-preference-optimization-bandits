2024-09-17 16:52:56,009 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_16_51_39/swapped_noise0.9_[0.9,0.9,0.9]_2025
2024-09-17 16:52:56,011 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2025
2024-09-17 16:52:56,011 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 16:52:56,174 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4201, l2 distance: 22.7103, acc: 0.78.
2024-09-17 16:52:56,174 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 16:52:56,175 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [ 8.56561643  4.86877417 12.41753164  4.25959353]
2024-09-17 16:52:56,383 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4903, 3.8609, 3.1151
2024-09-17 16:52:57,660 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.2024, val_loss:  21.3527, grad_norm: 0.8171, reward_err: 0.0389, 0.0577, 0.0155, KL_dist: 0.0682, 0.1023, 0.0233, param: [1.73524001 1.47865526 2.58861304 3.13302438]train_grp_loss: [23.17756925 18.48596597 22.43759135], val_grp_loss: [22.72904838 18.64539752 22.56900417], train_hist_grp_loss: [23.19663976 18.61559621 22.47701354], cur_train_grp_loss: [23.19663976 18.61559621 22.47701354],max_reward_err:  0.0577, max_reward_err_index: 1, max_kl_dist:  0.1023, max_kl_dist_index: 1, max_train_grp_loss:  23.1776, max_train_grp_loss_index: 0, max_val_grp_loss:  22.7290, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.1966, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:53:00,662 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.3376, val_loss:  17.7298, grad_norm: 0.4585, reward_err: 0.0812, 0.0057, 0.0446, KL_dist: 0.6827, 0.2760, 0.5070, param: [5.4394919  3.14127302 7.13540475 3.49798121]train_grp_loss: [22.08638359 11.22750366 19.82559634], val_grp_loss: [21.31464546 11.19028438 20.43558043], train_hist_grp_loss: [2253.62944659 1420.04918928 2097.90748852], cur_train_grp_loss: [22.09201669 11.26460941 19.84263715],max_reward_err:  0.0812, max_reward_err_index: 0, max_kl_dist:  0.6827, max_kl_dist_index: 0, max_train_grp_loss:  22.0864, max_train_grp_loss_index: 0, max_val_grp_loss:  21.3146, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.0920, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:53:00,877 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [5.4394919  3.14127302 7.13540475 3.49798121].
2024-09-17 16:53:01,193 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8283, 3.8283, 3.1728
2024-09-17 16:53:01,194 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8685, 3.8685, 3.3122
2024-09-17 16:53:01,194 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5544, 3.8463, 3.1645
2024-09-17 16:53:01,195 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0812, 0.0057, 0.0446
2024-09-17 16:53:01,855 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8685, 3.8685, 3.3122
Known param reward: [[3.8684823513031006, 3.4799795150756836, 3.271944284439087], [3.4799795150756836, 3.8684823513031006, 3.1135053634643555], [3.832144021987915, 3.6156766414642334, 3.312178611755371]], Known param reward error: [[0.0, 0.10042771323398944, 0.012147390594663916], [0.10042771323398944, 0.0, 0.059982649361328924], [0.009393432880195242, 0.06535010034457296, 0.0]].
