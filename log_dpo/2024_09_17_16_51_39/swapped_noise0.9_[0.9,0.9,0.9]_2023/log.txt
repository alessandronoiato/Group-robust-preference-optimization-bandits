2024-09-17 16:52:18,691 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_16_51_39/swapped_noise0.9_[0.9,0.9,0.9]_2023
2024-09-17 16:52:18,693 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2023
2024-09-17 16:52:18,693 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 16:52:18,858 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4087, l2 distance: 24.8598, acc: 0.79.
2024-09-17 16:52:18,858 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 16:52:18,859 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [12.16811756  7.83016102  8.65317196  6.37640364]
2024-09-17 16:52:19,063 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5418, 3.7349, 3.1310
2024-09-17 16:52:20,327 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.0636, val_loss:  22.0004, grad_norm: 0.8167, reward_err: 0.0268, 0.1004, 0.0115, KL_dist: 0.0377, 0.2797, 0.0133, param: [1.88913256 4.64363151 1.56453773 0.21425486]train_grp_loss: [23.04585775 17.56263717 22.90274318], val_grp_loss: [22.86051824 20.60523036 22.5284383 ], train_hist_grp_loss: [23.07013599 17.69345709 22.94152648], cur_train_grp_loss: [23.07013599 17.69345709 22.94152648],max_reward_err:  0.1004, max_reward_err_index: 1, max_kl_dist:  0.2797, max_kl_dist_index: 1, max_train_grp_loss:  23.0459, max_train_grp_loss_index: 0, max_val_grp_loss:  22.8605, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.0701, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:52:23,341 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.3764, val_loss:  17.9376, grad_norm: 0.4388, reward_err: 0.0527, 0.0446, 0.0218, KL_dist: 0.4762, 0.4188, 0.3664, param: [6.00140333 6.07474602 4.71581037 2.75466025]train_grp_loss: [21.70876329 10.58623253 20.42446621], val_grp_loss: [20.82974854 13.41882479 19.55022091], train_hist_grp_loss: [2224.96560704 1339.77290169 2150.21605429], cur_train_grp_loss: [21.71520963 10.6208458  20.43985609],max_reward_err:  0.0527, max_reward_err_index: 0, max_kl_dist:  0.4762, max_kl_dist_index: 0, max_train_grp_loss:  21.7088, max_train_grp_loss_index: 0, max_val_grp_loss:  20.8297, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.7152, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:52:23,557 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.00140333 6.07474602 4.71581037 2.75466025].
2024-09-17 16:52:23,875 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7702, 3.7702, 3.1198
2024-09-17 16:52:23,876 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8048, 3.8048, 3.2433
2024-09-17 16:52:23,876 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6044, 3.6350, 3.1727
2024-09-17 16:52:23,876 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0527, 0.0446, 0.0218
2024-09-17 16:52:24,542 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8048, 3.8048, 3.2433
Known param reward: [[3.8048343658447266, 3.420158624649048, 3.208865165710449], [3.420158624649048, 3.8048343658447266, 3.0513827800750732], [3.7656967639923096, 3.5444085597991943, 3.2432823181152344]], Known param reward error: [[0.0, 0.10110183629774783, 0.010611827472603731], [0.10110183629774783, 0.0, 0.059168311364173665], [0.01028628268387917, 0.06844602970981473, 0.0]].
