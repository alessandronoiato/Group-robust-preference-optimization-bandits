2024-09-17 16:54:09,265 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_16_51_39/swapped_noise0.9_[0.9,0.9,0.9]_2029
2024-09-17 16:54:09,267 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2029
2024-09-17 16:54:09,268 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 16:54:09,427 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.3426, l2 distance: 33.0020, acc: 0.83.
2024-09-17 16:54:09,427 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 16:54:09,428 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [11.70640074  7.16019475 16.36860299  7.3115033 ]
2024-09-17 16:54:09,625 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5043, 3.8243, 3.1399
2024-09-17 16:54:10,898 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.0864, val_loss:  20.7175, grad_norm: 0.8878, reward_err: 0.0636, 0.0471, 0.0395, KL_dist: 0.1341, 0.1117, 0.0923, param: [0.85901154 2.71677141 3.7377177  2.63352874]train_grp_loss: [22.97643047 16.21404662 21.69358593], val_grp_loss: [22.59488787 16.98320872 22.30441439], train_hist_grp_loss: [23.001973   16.36130054 21.7453226 ], cur_train_grp_loss: [23.001973   16.36130054 21.7453226 ],max_reward_err:  0.0636, max_reward_err_index: 0, max_kl_dist:  0.1341, max_kl_dist_index: 0, max_train_grp_loss:  22.9764, max_train_grp_loss_index: 0, max_val_grp_loss:  22.5949, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.0020, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:54:13,878 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  15.8114, val_loss:  17.1148, grad_norm: 0.4589, reward_err: 0.0798, 0.0115, 0.0432, KL_dist: 0.7764, 0.3654, 0.6075, param: [4.95073843 3.91500121 8.25355833 4.17617278]train_grp_loss: [21.53498493  8.71144496 18.3425333 ], val_grp_loss: [21.3024861   9.79037831 19.71443903], train_hist_grp_loss: [2212.61588295 1163.98608917 1980.845927  ], cur_train_grp_loss: [21.54232708  8.74388767 18.36375515],max_reward_err:  0.0798, max_reward_err_index: 0, max_kl_dist:  0.7764, max_kl_dist_index: 0, max_train_grp_loss:  21.5350, max_train_grp_loss_index: 0, max_val_grp_loss:  21.3025, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.5423, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:54:14,092 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [4.95073843 3.91500121 8.25355833 4.17617278].
2024-09-17 16:54:14,408 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8043, 3.8043, 3.1679
2024-09-17 16:54:14,408 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8449, 3.8449, 3.3022
2024-09-17 16:54:14,409 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5382, 3.8008, 3.1597
2024-09-17 16:54:14,409 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0798, 0.0115, 0.0432
2024-09-17 16:54:15,067 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8449, 3.8449, 3.3022
Known param reward: [[3.844888687133789, 3.455005407333374, 3.274245500564575], [3.455005407333374, 3.844888687133789, 3.1092171669006348], [3.809487819671631, 3.568223237991333, 3.302170515060425]], Known param reward error: [[0.0, 0.10140300839009658, 0.008456563453792035], [0.10140300839009658, 0.0, 0.05843227879353144], [0.009207254186739055, 0.07195668630622935, 0.0]].
