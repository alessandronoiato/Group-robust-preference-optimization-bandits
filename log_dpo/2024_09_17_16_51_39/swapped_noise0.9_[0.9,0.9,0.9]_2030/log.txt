2024-09-17 16:54:29,815 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_16_51_39/swapped_noise0.9_[0.9,0.9,0.9]_2030
2024-09-17 16:54:29,817 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2030
2024-09-17 16:54:29,818 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 16:54:29,976 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4404, l2 distance: 21.8264, acc: 0.80.
2024-09-17 16:54:29,976 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 16:54:29,977 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [ 9.2084507   5.67540874 10.09441932  6.74481639]
2024-09-17 16:54:30,181 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5606, 3.7624, 3.1980
2024-09-17 16:54:31,451 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.6323, val_loss:  20.7993, grad_norm: 0.6740, reward_err: 0.0499, 0.0716, 0.0382, KL_dist: 0.1948, 0.2857, 0.1911, param: [4.20016097 4.48182546 0.21669343 3.3737391 ]train_grp_loss: [22.26610933 18.27119295 21.63105081], val_grp_loss: [21.68518073 19.0394368  21.46205361], train_hist_grp_loss: [22.27293885 18.3667839  21.65819792], cur_train_grp_loss: [22.27293885 18.3667839  21.65819792],max_reward_err:  0.0716, max_reward_err_index: 1, max_kl_dist:  0.2857, max_kl_dist_index: 1, max_train_grp_loss:  22.2661, max_train_grp_loss_index: 0, max_val_grp_loss:  21.6852, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2729, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:54:34,451 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  17.8823, val_loss:  17.7478, grad_norm: 0.3988, reward_err: 0.0638, 0.0246, 0.0333, KL_dist: 0.6027, 0.3716, 0.4845, param: [7.42606305 4.41784486 3.98278389 4.81018215]train_grp_loss: [21.96897794 12.56040169 19.790809  ], val_grp_loss: [20.47339382 12.84354577 19.3120766 ], train_hist_grp_loss: [2207.10617905 1497.07844309 2061.07479345], cur_train_grp_loss: [21.96955019 12.5926033  19.80313531],max_reward_err:  0.0638, max_reward_err_index: 0, max_kl_dist:  0.6027, max_kl_dist_index: 0, max_train_grp_loss:  21.9690, max_train_grp_loss_index: 0, max_val_grp_loss:  20.4734, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.9696, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:54:34,664 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [7.42606305 4.41784486 3.98278389 4.81018215].
2024-09-17 16:54:34,982 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7674, 3.7674, 3.1544
2024-09-17 16:54:34,983 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8061, 3.8061, 3.2958
2024-09-17 16:54:34,983 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5631, 3.7123, 3.1859
2024-09-17 16:54:34,984 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0638, 0.0246, 0.0333
2024-09-17 16:54:35,639 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8061, 3.8061, 3.2958
Known param reward: [[3.8060805797576904, 3.436021089553833, 3.2607407569885254], [3.436021089553833, 3.8060805797576904, 3.1107161045074463], [3.772174119949341, 3.559548854827881, 3.2957820892333984]], Known param reward error: [[0.0, 0.09722849594198997, 0.010632175094143948], [0.09722849594198997, 0.0, 0.056152372855754744], [0.008908497625793363, 0.06477312283953397, 0.0]].
