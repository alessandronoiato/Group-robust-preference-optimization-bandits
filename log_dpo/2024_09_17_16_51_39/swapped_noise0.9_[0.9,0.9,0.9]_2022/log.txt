2024-09-17 16:52:01,054 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_16_51_39/swapped_noise0.9_[0.9,0.9,0.9]_2022
2024-09-17 16:52:01,056 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2022
2024-09-17 16:52:01,056 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 16:52:01,219 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4222, l2 distance: 24.0742, acc: 0.79.
2024-09-17 16:52:01,220 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 16:52:01,221 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [10.4921968   5.43874426 11.64334817  5.06163877]
2024-09-17 16:52:01,421 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.6038, 3.8630, 3.2074
2024-09-17 16:52:02,708 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.1459, val_loss:  18.6564, grad_norm: 0.6082, reward_err: 0.0351, 0.0349, 0.0099, KL_dist: 0.2800, 0.2651, 0.2077, param: [4.38680869 4.48137619 4.23574883 3.84511382]train_grp_loss: [21.18939353 15.69299672 20.62176577], val_grp_loss: [21.04470586 14.52045431 20.45549777], train_hist_grp_loss: [21.19781508 15.780367   20.63576052], cur_train_grp_loss: [21.19781508 15.780367   20.63576052],max_reward_err:  0.0351, max_reward_err_index: 0, max_kl_dist:  0.2800, max_kl_dist_index: 0, max_train_grp_loss:  21.1894, max_train_grp_loss_index: 0, max_val_grp_loss:  21.0447, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.1978, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:52:05,701 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  16.8596, val_loss:  16.5448, grad_norm: 0.3676, reward_err: 0.0636, 0.0075, 0.0306, KL_dist: 0.8618, 0.4645, 0.6766, param: [7.62246341 4.60838917 7.65950461 3.76097768]train_grp_loss: [20.64378049 10.39890849 19.6582214 ], val_grp_loss: [20.52774116 10.08020298 19.10865246], train_hist_grp_loss: [2088.24792289 1264.28597203 2009.03761748], cur_train_grp_loss: [20.64723822 10.42897024 19.66481324],max_reward_err:  0.0636, max_reward_err_index: 0, max_kl_dist:  0.8618, max_kl_dist_index: 0, max_train_grp_loss:  20.6438, max_train_grp_loss_index: 0, max_val_grp_loss:  20.5277, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.6472, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:52:05,913 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [7.62246341 4.60838917 7.65950461 3.76097768].
2024-09-17 16:52:06,238 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8491, 3.8491, 3.2014
2024-09-17 16:52:06,238 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8793, 3.8793, 3.3301
2024-09-17 16:52:06,239 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6327, 3.8504, 3.2282
2024-09-17 16:52:06,239 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0636, 0.0075, 0.0306
2024-09-17 16:52:06,899 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8793, 3.8793, 3.3301
Known param reward: [[3.879298448562622, 3.552269458770752, 3.2809364795684814], [3.552269458770752, 3.879298448562622, 3.1701314449310303], [3.8373265266418457, 3.6485824584960938, 3.3300602436065674]], Known param reward error: [[0.0, 0.08430106477449359, 0.014751614218511329], [0.08430106477449359, 0.0, 0.04802579742591348], [0.010819461940683636, 0.05947363759857518, 0.0]].
