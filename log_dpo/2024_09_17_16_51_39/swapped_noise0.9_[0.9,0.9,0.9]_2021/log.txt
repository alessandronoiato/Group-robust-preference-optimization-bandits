2024-09-17 16:51:43,517 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_16_51_39/swapped_noise0.9_[0.9,0.9,0.9]_2021
2024-09-17 16:51:43,518 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2021
2024-09-17 16:51:43,519 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 16:51:43,684 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.3866, l2 distance: 27.1800, acc: 0.82.
2024-09-17 16:51:43,685 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 16:51:43,686 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [12.80358778  6.31354557 10.43865706  7.80294   ]
2024-09-17 16:51:43,889 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5936, 3.8484, 3.1908
2024-09-17 16:51:45,185 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  19.5638, val_loss:  20.7622, grad_norm: 0.6926, reward_err: 0.0028, 0.0921, 0.0014, KL_dist: 0.0655, 0.3343, 0.0723, param: [1.97858919 4.94104596 2.37648898 3.8172003 ]train_grp_loss: [21.60926886 16.11919234 21.21799051], val_grp_loss: [21.02512633 19.96385167 21.30652475], train_hist_grp_loss: [21.62362972 16.21437659 21.2487043 ], cur_train_grp_loss: [21.62362972 16.21437659 21.2487043 ],max_reward_err:  0.0921, max_reward_err_index: 1, max_kl_dist:  0.3343, max_kl_dist_index: 1, max_train_grp_loss:  21.6093, max_train_grp_loss_index: 0, max_val_grp_loss:  21.3065, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  21.6236, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:51:48,219 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  16.7013, val_loss:  17.5211, grad_norm: 0.4033, reward_err: 0.0521, 0.0285, 0.0197, KL_dist: 0.5059, 0.4091, 0.4040, param: [5.89933821 5.1184834  5.71534766 4.89157911]train_grp_loss: [20.84580319 10.50442385 19.18794543], val_grp_loss: [19.63306674 13.654496   19.30135864], train_hist_grp_loss: [2114.81749287 1286.23073918 2008.37147189], cur_train_grp_loss: [20.84917292 10.5356123  19.20116303],max_reward_err:  0.0521, max_reward_err_index: 0, max_kl_dist:  0.5059, max_kl_dist_index: 0, max_train_grp_loss:  20.8458, max_train_grp_loss_index: 0, max_val_grp_loss:  19.6331, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  20.8492, max_cur_train_grp_loss_index: 0, 
2024-09-17 16:51:48,437 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [5.89933821 5.1184834  5.71534766 4.89157911].
2024-09-17 16:51:48,763 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8512, 3.8512, 3.2031
2024-09-17 16:51:48,764 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8864, 3.8864, 3.3152
2024-09-17 16:51:48,764 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6838, 3.7756, 3.2498
2024-09-17 16:51:48,764 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0521, 0.0285, 0.0197
2024-09-17 16:51:49,438 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8864, 3.8864, 3.3152
Known param reward: [[3.8863613605499268, 3.458714246749878, 3.291393280029297], [3.458714246749878, 3.8863613605499268, 3.0966663360595703], [3.851715087890625, 3.5921897888183594, 3.315164804458618]], Known param reward error: [[0.0, 0.11003791828033614, 0.007170540781969749], [0.11003791828033614, 0.0, 0.06590878019252189], [0.008914835612301181, 0.07569331424444319, 0.0]].
