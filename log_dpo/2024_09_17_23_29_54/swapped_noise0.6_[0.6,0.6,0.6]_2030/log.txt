2024-09-17 23:32:45,326 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_29_54/swapped_noise0.6_[0.6,0.6,0.6]_2030
2024-09-17 23:32:45,328 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2030
2024-09-17 23:32:45,328 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:32:45,486 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5425, l2 distance: 11.3341, acc: 0.74.
2024-09-17 23:32:45,487 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:32:45,487 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [3.12341811 1.68199744 7.99266711 2.47477641]
2024-09-17 23:32:45,689 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4449, 3.9070, 3.1365
2024-09-17 23:32:46,990 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.1894, val_loss:  21.2824, grad_norm: 0.5153, reward_err: 0.1023, 0.0391, 0.0699, KL_dist: 0.2986, 0.0805, 0.2078, param: [1.05777635 1.06476231 4.94048603 3.02692185]train_grp_loss: [22.62579811 19.00743004 22.17458553], val_grp_loss: [23.23742178 17.23926816 23.29459727], train_hist_grp_loss: [22.64010207 19.0525566  22.19254682], cur_train_grp_loss: [22.64010207 19.0525566  22.19254682],max_reward_err:  0.1023, max_reward_err_index: 0, max_kl_dist:  0.2986, max_kl_dist_index: 0, max_train_grp_loss:  22.6258, max_train_grp_loss_index: 0, max_val_grp_loss:  23.2946, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.6401, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:32:50,126 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  19.6135, val_loss:  19.5364, grad_norm: 0.2988, reward_err: 0.1096, 0.0168, 0.0719, KL_dist: 0.7519, 0.2067, 0.5704, param: [3.24145733 1.61114123 8.04508088 3.75825595]train_grp_loss: [21.72983205 16.47121477 20.9917908 ], val_grp_loss: [22.61644963 13.26908205 22.60171534], train_hist_grp_loss: [2211.6218705  1751.17735421 2151.32371475], cur_train_grp_loss: [21.73526105 16.48422534 20.99948953],max_reward_err:  0.1096, max_reward_err_index: 0, max_kl_dist:  0.7519, max_kl_dist_index: 0, max_train_grp_loss:  21.7298, max_train_grp_loss_index: 0, max_val_grp_loss:  22.6164, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.7353, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:32:50,357 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [3.24145733 1.61114123 8.04508088 3.75825595].
2024-09-17 23:32:50,687 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8811, 3.8811, 3.2645
2024-09-17 23:32:50,687 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.9244, 3.9244, 3.4203
2024-09-17 23:32:50,688 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.4943, 3.8585, 3.1742
2024-09-17 23:32:50,688 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.1096, 0.0168, 0.0719
2024-09-17 23:32:51,378 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.9244, 3.9244, 3.4203
Known param reward: [[3.9244344234466553, 3.531189441680908, 3.3788671493530273], [3.531189441680908, 3.9244344234466553, 3.2225024700164795], [3.8856606483459473, 3.648857831954956, 3.420322895050049]], Known param reward error: [[0.0, 0.10020424329587284, 0.01212041873503141], [0.10020424329587284, 0.0, 0.05783676895531077], [0.00988009249665452, 0.07022071507814179, 0.0]].
