2024-09-17 23:30:54,688 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_29_54/swapped_noise0.6_[0.6,0.6,0.6]_2024
2024-09-17 23:30:54,690 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2024
2024-09-17 23:30:54,691 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:30:54,851 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5521, l2 distance: 9.4837, acc: 0.72.
2024-09-17 23:30:54,852 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:30:54,852 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [6.06856284 3.25328814 4.61692761 2.64067382]
2024-09-17 23:30:55,057 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5115, 3.7700, 3.1062
2024-09-17 23:30:56,342 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.0743, val_loss:  20.5579, grad_norm: 0.4690, reward_err: 0.0699, 0.0293, 0.0373, KL_dist: 0.3380, 0.1447, 0.2141, param: [3.97384679 0.66535514 4.67495945 3.75923634]train_grp_loss: [23.13943857 16.42939717 22.99055318], val_grp_loss: [23.40332082 15.42215213 22.67637658], train_hist_grp_loss: [23.1492575  16.47633001 23.00315225], cur_train_grp_loss: [23.1492575  16.47633001 23.00315225],max_reward_err:  0.0699, max_reward_err_index: 0, max_kl_dist:  0.3380, max_kl_dist_index: 0, max_train_grp_loss:  23.1394, max_train_grp_loss_index: 0, max_val_grp_loss:  23.4033, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.1493, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:30:59,484 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  19.8292, val_loss:  19.2974, grad_norm: 0.2561, reward_err: 0.0657, 0.0061, 0.0328, KL_dist: 0.6208, 0.2788, 0.4553, param: [6.10643226 2.88591427 6.25272953 3.88912692]train_grp_loss: [22.53057311 13.9623523  22.15641794], val_grp_loss: [22.87487945 13.10460443 21.70356004], train_hist_grp_loss: [2279.10336953 1494.16906395 2252.42818502], cur_train_grp_loss: [22.53412016 13.97371618 22.1618184 ],max_reward_err:  0.0657, max_reward_err_index: 0, max_kl_dist:  0.6208, max_kl_dist_index: 0, max_train_grp_loss:  22.5306, max_train_grp_loss_index: 0, max_val_grp_loss:  22.8749, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.5341, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:30:59,711 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.10643226 2.88591427 6.25272953 3.88912692].
2024-09-17 23:31:00,043 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7492, 3.7492, 3.0980
2024-09-17 23:31:00,044 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.7932, 3.7932, 3.2404
2024-09-17 23:31:00,045 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5440, 3.7700, 3.1341
2024-09-17 23:31:00,046 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0657, 0.0061, 0.0328
2024-09-17 23:31:00,727 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.7932, 3.7932, 3.2404
Known param reward: [[3.7932193279266357, 3.4367012977600098, 3.2102184295654297], [3.4367012977600098, 3.7932193279266357, 3.057360887527466], [3.7612311840057373, 3.562816858291626, 3.240443229675293]], Known param reward error: [[0.0, 0.0939882456945873, 0.009327366032236257], [0.0939882456945873, 0.0, 0.056499166679174574], [0.008432980314476852, 0.06074061363621364, 0.0]].
