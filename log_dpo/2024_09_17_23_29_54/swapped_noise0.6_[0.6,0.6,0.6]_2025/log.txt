2024-09-17 23:31:12,291 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_29_54/swapped_noise0.6_[0.6,0.6,0.6]_2025
2024-09-17 23:31:12,293 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2025
2024-09-17 23:31:12,294 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:31:12,453 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5578, l2 distance: 7.9530, acc: 0.71.
2024-09-17 23:31:12,454 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:31:12,454 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.04168534 2.56659103 4.61195466 2.02486437]
2024-09-17 23:31:12,655 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5616, 3.8591, 3.1808
2024-09-17 23:31:13,938 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.4266, val_loss:  21.5317, grad_norm: 0.5214, reward_err: 0.0436, 0.0310, 0.0163, KL_dist: 0.1109, 0.0782, 0.0496, param: [3.11735053 2.42133401 2.31762138 2.56751746]train_grp_loss: [23.28280674 18.86203895 22.60089153], val_grp_loss: [23.18865478 18.15867616 23.2540637 ], train_hist_grp_loss: [23.2918733  18.91116286 22.61988415], cur_train_grp_loss: [23.2918733  18.91116286 22.61988415],max_reward_err:  0.0436, max_reward_err_index: 0, max_kl_dist:  0.1109, max_kl_dist_index: 0, max_train_grp_loss:  23.2828, max_train_grp_loss_index: 0, max_val_grp_loss:  23.2541, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  23.2919, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:31:17,022 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  19.9439, val_loss:  19.8420, grad_norm: 0.2730, reward_err: 0.0717, 0.0100, 0.0362, KL_dist: 0.4835, 0.1938, 0.3407, param: [5.80417789 3.11596527 4.78267379 3.21966357]train_grp_loss: [22.80602198 16.30609774 21.39224328], val_grp_loss: [22.62796495 14.51976743 22.40686904], train_hist_grp_loss: [2299.39811172 1731.91852684 2191.70337292], cur_train_grp_loss: [22.80809844 16.31792957 21.39972038],max_reward_err:  0.0717, max_reward_err_index: 0, max_kl_dist:  0.4835, max_kl_dist_index: 0, max_train_grp_loss:  22.8060, max_train_grp_loss_index: 0, max_val_grp_loss:  22.6280, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.8081, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:31:17,239 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [5.80417789 3.11596527 4.78267379 3.21966357].
2024-09-17 23:31:17,565 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8342, 3.8342, 3.1825
2024-09-17 23:31:17,566 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8751, 3.8751, 3.3250
2024-09-17 23:31:17,566 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5972, 3.8362, 3.2047
2024-09-17 23:31:17,567 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0717, 0.0100, 0.0362
2024-09-17 23:31:18,238 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8751, 3.8751, 3.3250
Known param reward: [[3.8750712871551514, 3.489452362060547, 3.283229351043701], [3.489452362060547, 3.8750712871551514, 3.1289710998535156], [3.836576223373413, 3.6257314682006836, 3.3250086307525635]], Known param reward error: [[0.0, 0.09951273061035817, 0.012565164289334858], [0.09951273061035817, 0.0, 0.05895850287001446], [0.009934027255018341, 0.06434457600327614, 0.0]].
