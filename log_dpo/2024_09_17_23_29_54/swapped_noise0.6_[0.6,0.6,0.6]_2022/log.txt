2024-09-17 23:30:17,351 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_29_54/swapped_noise0.6_[0.6,0.6,0.6]_2022
2024-09-17 23:30:17,352 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2022
2024-09-17 23:30:17,353 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:30:17,511 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.4972, l2 distance: 15.7924, acc: 0.74.
2024-09-17 23:30:17,511 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:30:17,513 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [9.34426871 2.83966746 6.08749348 4.46436597]
2024-09-17 23:30:17,720 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5016, 3.8536, 3.1454
2024-09-17 23:30:18,994 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  23.7936, val_loss:  23.4900, grad_norm: 0.7970, reward_err: 0.0048, 0.1312, 0.0286, KL_dist: 0.0192, 0.3875, 0.0547, param: [0.16462363 4.80328303 0.77820626 1.32672423]train_grp_loss: [23.57047625 24.47738553 23.32864607], val_grp_loss: [23.3039163  23.74349726 23.42335085], train_hist_grp_loss: [23.58472193 24.62310349 23.35715246], cur_train_grp_loss: [23.58472193 24.62310349 23.35715246],max_reward_err:  0.1312, max_reward_err_index: 1, max_kl_dist:  0.3875, max_kl_dist_index: 1, max_train_grp_loss:  24.4774, max_train_grp_loss_index: 1, max_val_grp_loss:  23.7435, max_val_grp_loss_index: 1, max_cur_train_grp_loss:  24.6231, max_cur_train_grp_loss_index: 1, 
2024-09-17 23:30:22,040 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  19.8940, val_loss:  20.1068, grad_norm: 0.4814, reward_err: 0.0506, 0.0246, 0.0190, KL_dist: 0.3174, 0.1964, 0.2164, param: [4.51689848 3.74376908 4.44640137 3.4020673 ]train_grp_loss: [22.70099575 15.71118374 21.36649196], val_grp_loss: [22.24062266 16.24635303 21.80013174], train_hist_grp_loss: [2306.96846175 1942.8905235  2224.59792443], cur_train_grp_loss: [22.705889   15.7616259  21.37987214],max_reward_err:  0.0506, max_reward_err_index: 0, max_kl_dist:  0.3174, max_kl_dist_index: 0, max_train_grp_loss:  22.7010, max_train_grp_loss_index: 0, max_val_grp_loss:  22.2406, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.7059, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:30:22,252 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [4.51689848 3.74376908 4.44640137 3.4020673 ].
2024-09-17 23:30:22,569 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8304, 3.8304, 3.1988
2024-09-17 23:30:22,570 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8653, 3.8653, 3.3311
2024-09-17 23:30:22,571 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6697, 3.7704, 3.2678
2024-09-17 23:30:22,571 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0506, 0.0246, 0.0190
2024-09-17 23:30:23,243 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8653, 3.8653, 3.3311
Known param reward: [[3.8653156757354736, 3.477531671524048, 3.30049467086792], [3.477531671524048, 3.8653156757354736, 3.137904405593872], [3.8325858116149902, 3.5891125202178955, 3.331118106842041]], Known param reward error: [[0.0, 0.10032401923748184, 0.009193140258588025], [0.10032401923748184, 0.0, 0.05800265708121018], [0.008467578554048038, 0.0714568171628113, 0.0]].
