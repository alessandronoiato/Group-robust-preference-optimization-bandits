2024-09-17 23:32:05,272 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_29_54/swapped_noise0.6_[0.6,0.6,0.6]_2028
2024-09-17 23:32:05,274 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2028
2024-09-17 23:32:05,275 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:32:05,433 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5679, l2 distance: 10.7043, acc: 0.69.
2024-09-17 23:32:05,433 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:32:05,434 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [6.99370909 3.70566097 4.07167132 1.02776318]
2024-09-17 23:32:05,643 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4824, 3.8302, 3.1395
2024-09-17 23:32:06,930 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  23.8058, val_loss:  23.0778, grad_norm: 0.6567, reward_err: 0.0864, 0.0774, 0.0654, KL_dist: 0.1196, 0.1093, 0.0870, param: [2.73013241 0.80418605 0.05499547 2.62733749]train_grp_loss: [24.16177737 23.76824542 23.37748868], val_grp_loss: [24.01051561 21.23072438 23.86367191], train_hist_grp_loss: [24.16313271 23.89393601 23.39315139], cur_train_grp_loss: [24.16313271 23.89393601 23.39315139],max_reward_err:  0.0864, max_reward_err_index: 0, max_kl_dist:  0.1196, max_kl_dist_index: 0, max_train_grp_loss:  24.1618, max_train_grp_loss_index: 0, max_val_grp_loss:  24.0105, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  24.1631, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:32:10,016 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  21.2466, val_loss:  20.7128, grad_norm: 0.3824, reward_err: 0.1063, 0.0191, 0.0703, KL_dist: 0.4454, 0.0939, 0.3142, param: [6.09633203 3.02428909 2.68913177 1.38818337]train_grp_loss: [24.14382311 16.45491972 22.19696781], val_grp_loss: [23.18507966 15.78240931 22.8290464 ], train_hist_grp_loss: [2413.82416295 1951.10884317 2274.54570758], cur_train_grp_loss: [24.14330191 16.49512023 22.20596633],max_reward_err:  0.1063, max_reward_err_index: 0, max_kl_dist:  0.4454, max_kl_dist_index: 0, max_train_grp_loss:  24.1438, max_train_grp_loss_index: 0, max_val_grp_loss:  23.1851, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  24.1433, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:32:10,237 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.09633203 3.02428909 2.68913177 1.38818337].
2024-09-17 23:32:10,565 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8644, 3.8644, 3.2369
2024-09-17 23:32:10,566 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8990, 3.8990, 3.3708
2024-09-17 23:32:10,567 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.4844, 3.8246, 3.1337
2024-09-17 23:32:10,567 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.1063, 0.0191, 0.0703
2024-09-17 23:32:11,248 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8990, 3.8990, 3.3708
Known param reward: [[3.8990397453308105, 3.4851560592651367, 3.334066867828369], [3.4851560592651367, 3.8990397453308105, 3.152937412261963], [3.8658859729766846, 3.638388156890869, 3.3707845211029053]], Known param reward error: [[0.0, 0.10615015826943262, 0.010892910254174978], [0.10615015826943262, 0.0, 0.06462801388730235], [0.008503060886678156, 0.06685020042488093, 0.0]].
