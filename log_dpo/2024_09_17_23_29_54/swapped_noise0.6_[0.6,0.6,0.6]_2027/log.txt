2024-09-17 23:31:47,739 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_29_54/swapped_noise0.6_[0.6,0.6,0.6]_2027
2024-09-17 23:31:47,741 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2027
2024-09-17 23:31:47,741 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:31:47,904 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5312, l2 distance: 12.5910, acc: 0.72.
2024-09-17 23:31:47,904 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:31:47,905 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [6.59229492 3.67854836 6.67403947 3.4276155 ]
2024-09-17 23:31:48,108 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5080, 3.7471, 3.1184
2024-09-17 23:31:49,416 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  21.2557, val_loss:  20.8099, grad_norm: 0.5045, reward_err: 0.0523, 0.0316, 0.0274, KL_dist: 0.1829, 0.1272, 0.1208, param: [2.0206182  3.29345191 4.27102896 2.76824358]train_grp_loss: [23.01670563 18.02832634 22.30862844], val_grp_loss: [22.87009937 16.60305686 22.80199277], train_hist_grp_loss: [23.02104868 18.08651192 22.32643446], cur_train_grp_loss: [23.02104868 18.08651192 22.32643446],max_reward_err:  0.0523, max_reward_err_index: 0, max_kl_dist:  0.1829, max_kl_dist_index: 0, max_train_grp_loss:  23.0167, max_train_grp_loss_index: 0, max_val_grp_loss:  22.8701, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.0210, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:31:52,559 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  19.6450, val_loss:  19.2226, grad_norm: 0.3139, reward_err: 0.0681, 0.0112, 0.0364, KL_dist: 0.5774, 0.2763, 0.4328, param: [4.88539796 3.59933783 6.78592513 3.79542162]train_grp_loss: [22.82433354 14.40549813 21.03675615], val_grp_loss: [22.21669951 13.3654067  21.87535189], train_hist_grp_loss: [2289.05560965 1596.14524049 2161.52588846], cur_train_grp_loss: [22.82466866 14.42704648 21.0458265 ],max_reward_err:  0.0681, max_reward_err_index: 0, max_kl_dist:  0.5774, max_kl_dist_index: 0, max_train_grp_loss:  22.8243, max_train_grp_loss_index: 0, max_val_grp_loss:  22.2167, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.8247, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:31:52,786 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [4.88539796 3.59933783 6.78592513 3.79542162].
2024-09-17 23:31:53,108 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7290, 3.7290, 3.0905
2024-09-17 23:31:53,109 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.7694, 3.7694, 3.2324
2024-09-17 23:31:53,109 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5128, 3.7272, 3.1148
2024-09-17 23:31:53,109 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0681, 0.0112, 0.0364
2024-09-17 23:31:53,780 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.7694, 3.7694, 3.2324
Known param reward: [[3.769418716430664, 3.4166855812072754, 3.190812110900879], [3.4166855812072754, 3.769418716430664, 3.0528886318206787], [3.7305526733398438, 3.537524461746216, 3.2323977947235107]], Known param reward error: [[0.0, 0.09357759425500983, 0.012865274159794106], [0.09357759425500983, 0.0, 0.055534366220599], [0.010310885050102189, 0.06151989792846188, 0.0]].
