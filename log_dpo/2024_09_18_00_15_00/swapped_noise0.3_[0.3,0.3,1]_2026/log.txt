2024-09-18 00:16:36,196 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_18_00_15_00/swapped_noise0.3_[0.3,0.3,1]_2026
2024-09-18 00:16:36,198 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2026
2024-09-18 00:16:36,199 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-18 00:16:36,367 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5758, l2 distance: 8.1084, acc: 0.74.
2024-09-18 00:16:36,368 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 00:16:36,369 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [3.90195514 2.43903435 5.63229133 2.65737389]
2024-09-18 00:16:36,576 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5354, 3.8444, 3.1588
2024-09-18 00:16:37,847 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.0419, val_loss:  21.9085, grad_norm: 0.4440, reward_err: 0.0242, 0.0752, 0.0032, KL_dist: 0.0642, 0.1841, 0.0308, param: [2.66721373 2.17024286 2.16748424 4.08848147]train_grp_loss: [23.24435569 21.59964915 21.17400565], val_grp_loss: [23.93321146 20.1407498  21.61166501], train_hist_grp_loss: [23.25326856 21.62967111 21.1945466 ], cur_train_grp_loss: [23.25326856 21.62967111 21.1945466 ],max_reward_err:  0.0752, max_reward_err_index: 1, max_kl_dist:  0.1841, max_kl_dist_index: 1, max_train_grp_loss:  23.2444, max_train_grp_loss_index: 0, max_val_grp_loss:  23.9332, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.2533, max_cur_train_grp_loss_index: 0, 
2024-09-18 00:16:40,947 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  20.8134, val_loss:  20.6663, grad_norm: 0.2709, reward_err: 0.0602, 0.0288, 0.0241, KL_dist: 0.3587, 0.2157, 0.2501, param: [4.73774477 2.91876829 4.82687348 4.24908293]train_grp_loss: [22.64647387 19.94023145 19.70412438], val_grp_loss: [23.8874004  17.80329406 20.24767174], train_hist_grp_loss: [2291.16512575 2061.07667676 2037.29652089], cur_train_grp_loss: [22.65041917 19.94811129 19.71460947],max_reward_err:  0.0602, max_reward_err_index: 0, max_kl_dist:  0.3587, max_kl_dist_index: 0, max_train_grp_loss:  22.6465, max_train_grp_loss_index: 0, max_val_grp_loss:  23.8874, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.6504, max_cur_train_grp_loss_index: 0, 
2024-09-18 00:16:41,168 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [4.73774477 2.91876829 4.82687348 4.24908293].
2024-09-18 00:16:41,502 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8317, 3.8317, 3.1908
2024-09-18 00:16:41,503 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8697, 3.8697, 3.3127
2024-09-18 00:16:41,503 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6367, 3.7582, 3.2330
2024-09-18 00:16:41,504 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0602, 0.0288, 0.0241
2024-09-18 00:16:42,188 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8697, 3.8697, 3.3127
Known param reward: [[3.8697431087493896, 3.4774062633514404, 3.2797791957855225], [3.4774062633514404, 3.8697431087493896, 3.122826337814331], [3.8267757892608643, 3.597029209136963, 3.3126978874206543]], Known param reward error: [[0.0, 0.10138575982237315, 0.009937124589638663], [0.10138575982237315, 0.0, 0.05731628903659602], [0.01110340358029901, 0.07047338594539458, 0.0]].
