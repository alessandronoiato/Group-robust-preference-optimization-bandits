2024-09-18 19:52:13,623 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_19_49_22/swapped_noise1.0_[1,1.0,1]_2030
2024-09-18 19:52:13,625 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2030
2024-09-18 19:52:13,626 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 19:52:13,786 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3234, l2 distance: 29.0058, acc: 0.84.
2024-09-18 19:52:13,787 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 19:52:13,788 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [12.81511744  6.94562577 12.93366958  5.46843   ]
2024-09-18 19:52:13,998 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5538, 3.8595, 3.2321
2024-09-18 19:52:15,406 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  18.7899, val_loss:  19.9570, grad_norm: 0.9991, reward_err: 0.0771, 0.0474, 0.0404, KL_dist: 0.2640, 0.1526, 0.1580, param: [4.45612986 0.5301474  2.77755776 4.06308483]train_grp_loss: [22.37417742 15.27222835 21.30041275], val_grp_loss: [22.24327346 15.90258118 21.63382516], train_hist_grp_loss: [22.39938404 15.4456462  21.34417015], cur_train_grp_loss: [22.39938404 15.4456462  21.34417015],max_reward_err:  0.0771, max_reward_err_index: 0, max_kl_dist:  0.2640, max_kl_dist_index: 0, max_train_grp_loss:  22.3742, max_train_grp_loss_index: 0, max_val_grp_loss:  22.2433, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.3994, max_cur_train_grp_loss_index: 0, 
2024-09-18 19:52:18,609 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  14.1174, val_loss:  16.1347, grad_norm: 0.4343, reward_err: 0.0835, 0.0036, 0.0446, KL_dist: 1.0144, 0.4157, 0.7761, param: [8.69597652 3.68212128 6.73357983 3.63578646]train_grp_loss: [21.14580437  7.30254451 18.79726615], val_grp_loss: [20.61421133  8.39552112 19.20735282], train_hist_grp_loss: [2161.21588374 1028.35930495 1983.41711979], cur_train_grp_loss: [21.15052101  7.33363087 18.81060751],max_reward_err:  0.0835, max_reward_err_index: 0, max_kl_dist:  1.0144, max_kl_dist_index: 0, max_train_grp_loss:  21.1458, max_train_grp_loss_index: 0, max_val_grp_loss:  20.6142, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  21.1505, max_cur_train_grp_loss_index: 0, 
2024-09-18 19:52:18,849 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [8.69597652 3.68212128 6.73357983 3.63578646].
2024-09-18 19:52:19,185 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8314, 3.8314, 3.2209
2024-09-18 19:52:19,186 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8748, 3.8748, 3.3766
2024-09-18 19:52:19,186 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.5514, 3.8608, 3.2261
2024-09-18 19:52:19,187 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0835, 0.0036, 0.0446
2024-09-18 19:52:19,856 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8748, 3.8748, 3.3766
Known param reward: [[3.874847650527954, 3.4763882160186768, 3.337296485900879], [3.4763882160186768, 3.874847650527954, 3.175377130508423], [3.8370981216430664, 3.5955355167388916, 3.3765645027160645]], Known param reward error: [[0.0, 0.102832284117025, 0.011629576980862906], [0.102832284117025, 0.0, 0.059583452958120335], [0.009742196929921686, 0.0720833846850742, 0.0]].
