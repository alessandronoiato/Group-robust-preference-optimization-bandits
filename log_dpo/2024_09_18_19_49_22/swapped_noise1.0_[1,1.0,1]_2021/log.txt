2024-09-18 19:49:26,855 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: Logging to log_dpo/2024_09_18_19_49_22/swapped_noise1.0_[1,1.0,1]_2021
2024-09-18 19:49:26,857 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Seed: 2021
2024-09-18 19:49:26,857 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:201] - INFO: (IB) Data: 300
2024-09-18 19:49:27,020 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: MLE reward loss: 0.3566, l2 distance: 29.5613, acc: 0.82.
2024-09-18 19:49:27,021 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-18 19:49:27,022 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:299] - INFO: MLE reward parameter: [12.9300958   5.57206545 13.55993407  6.18568546]
2024-09-18 19:49:27,220 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:314] - INFO: Learned oracle reward: 3.5203, 3.8700, 3.1356
2024-09-18 19:49:28,634 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.4903, val_loss:  20.9007, grad_norm: 0.9347, reward_err: 0.1119, 0.0010, 0.0680, KL_dist: 0.2138, 0.0053, 0.1147, param: [3.04421893 1.24399833 2.72641581 0.54475873]train_grp_loss: [23.90255387 16.24992226 22.56048797], val_grp_loss: [23.60121745 16.60542966 22.77906565], train_hist_grp_loss: [23.93346769 16.40519046 22.61737849], cur_train_grp_loss: [23.93346769 16.40519046 22.61737849],max_reward_err:  0.1119, max_reward_err_index: 0, max_kl_dist:  0.2138, max_kl_dist_index: 0, max_train_grp_loss:  23.9026, max_train_grp_loss_index: 0, max_val_grp_loss:  23.6012, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.9335, max_cur_train_grp_loss_index: 0, 
2024-09-18 19:49:31,836 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  15.7842, val_loss:  16.2909, grad_norm: 0.4834, reward_err: 0.0988, 0.0004, 0.0563, KL_dist: 0.8138, 0.3210, 0.6112, param: [7.45005752 3.09725547 7.00945385 2.49678346]train_grp_loss: [22.23250949  8.04612039 19.00832541], val_grp_loss: [21.56746475  8.2266861  19.59707631], train_hist_grp_loss: [2290.27834628 1132.89917703 2053.92290621], cur_train_grp_loss: [22.24040077  8.08553181 19.02984212],max_reward_err:  0.0988, max_reward_err_index: 0, max_kl_dist:  0.8138, max_kl_dist_index: 0, max_train_grp_loss:  22.2325, max_train_grp_loss_index: 0, max_val_grp_loss:  21.5675, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  22.2404, max_cur_train_grp_loss_index: 0, 
2024-09-18 19:49:32,077 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:387] - INFO: Policy parameter learned solely on the preference data dpo: [7.45005752 3.09725547 7.00945385 2.49678346].
2024-09-18 19:49:32,417 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Uniform reward: 3.8390, 3.8390, 3.1881
2024-09-18 19:49:32,417 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Optimal reward: 3.8748, 3.8748, 3.3008
2024-09-18 19:49:32,418 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Policy reward: 3.4918, 3.8732, 3.1148
2024-09-18 19:49:32,418 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:401] - INFO: Reward Error: 0.0988, 0.0004, 0.0563
2024-09-18 19:49:33,074 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:416] - INFO: Optimal reward: 3.8748, 3.8748, 3.3008
Known param reward: [[3.8748409748077393, 3.4526655673980713, 3.2780399322509766], [3.4526655673980713, 3.8748409748077393, 3.086048126220703], [3.8403704166412354, 3.587584972381592, 3.300816535949707]], Known param reward error: [[0.0, 0.10895296352919757, 0.00690029374570411], [0.10895296352919757, 0.0, 0.06506523685576818], [0.008895992994451664, 0.07413362362319925, 0.0]].
