2024-09-17 23:57:04,674 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_54_32/swapped_noise0.4_[0.4,0.4,0.4]_2029
2024-09-17 23:57:04,676 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2029
2024-09-17 23:57:04,677 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:57:04,840 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5210, l2 distance: 10.4945, acc: 0.73.
2024-09-17 23:57:04,841 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:57:04,842 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [2.93068584 2.27402775 6.80133542 4.9818987 ]
2024-09-17 23:57:05,046 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.5978, 3.7356, 3.2418
2024-09-17 23:57:06,323 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  20.0590, val_loss:  22.1516, grad_norm: 0.5136, reward_err: 0.0428, 0.0606, 0.0183, KL_dist: 0.1402, 0.2043, 0.0923, param: [2.25180027 2.80705346 3.77045401 4.21460449]train_grp_loss: [22.89922243 14.97421198 23.10438451], val_grp_loss: [23.01392987 19.69834054 23.47719921], train_hist_grp_loss: [22.91030713 15.02627691 23.11599797], cur_train_grp_loss: [22.91030713 15.02627691 23.11599797],max_reward_err:  0.0606, max_reward_err_index: 1, max_kl_dist:  0.2043, max_kl_dist_index: 1, max_train_grp_loss:  23.1044, max_train_grp_loss_index: 2, max_val_grp_loss:  23.4772, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  23.1160, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:57:09,447 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  18.6937, val_loss:  21.4041, grad_norm: 0.2536, reward_err: 0.0617, 0.0484, 0.0302, KL_dist: 0.4704, 0.4088, 0.3644, param: [3.97056889 2.76213255 6.32503751 6.08493604]train_grp_loss: [22.2192527  12.46641522 22.37704723], val_grp_loss: [22.38853807 18.47853193 23.02936466], train_hist_grp_loss: [2251.02027944 1342.19049261 2269.08466212], cur_train_grp_loss: [22.22328827 12.47659648 22.38146167],max_reward_err:  0.0617, max_reward_err_index: 0, max_kl_dist:  0.4704, max_kl_dist_index: 0, max_train_grp_loss:  22.3770, max_train_grp_loss_index: 2, max_val_grp_loss:  23.0294, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  22.3815, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:57:09,667 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [3.97056889 2.76213255 6.32503751 6.08493604].
2024-09-17 23:57:10,002 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8505, 3.8505, 3.2427
2024-09-17 23:57:10,002 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8970, 3.8970, 3.3894
2024-09-17 23:57:10,003 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.6566, 3.7082, 3.2869
2024-09-17 23:57:10,003 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0617, 0.0484, 0.0302
2024-09-17 23:57:10,681 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8970, 3.8970, 3.3894
Known param reward: [[3.8969926834106445, 3.485361099243164, 3.352837324142456], [3.485361099243164, 3.8969926834106445, 3.181079149246216], [3.8613202571868896, 3.6162831783294678, 3.3893625736236572]], Known param reward error: [[0.0, 0.10562801052200613, 0.010776436184621895], [0.10562801052200613, 0.0, 0.061452093086270226], [0.009153834539030853, 0.07203234080375538, 0.0]].
