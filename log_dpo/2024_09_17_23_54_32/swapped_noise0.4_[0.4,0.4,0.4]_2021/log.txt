2024-09-17 23:54:36,612 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_54_32/swapped_noise0.4_[0.4,0.4,0.4]_2021
2024-09-17 23:54:36,614 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2021
2024-09-17 23:54:36,614 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:54:36,777 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.5896, l2 distance: 7.2846, acc: 0.70.
2024-09-17 23:54:36,778 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:54:36,778 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [4.36544362 1.37954621 4.59478501 2.54472126]
2024-09-17 23:54:36,983 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4589, 3.7861, 3.0943
2024-09-17 23:54:38,240 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  23.3930, val_loss:  23.2554, grad_norm: 0.5796, reward_err: 0.1283, 0.0123, 0.0993, KL_dist: 0.2350, 0.0388, 0.1719, param: [0.07420509 1.16393232 3.51732709 0.71511336]train_grp_loss: [24.25862886 21.77804476 24.26688122], val_grp_loss: [24.04654581 21.50015376 24.35304662], train_hist_grp_loss: [24.27252571 21.84681804 24.28198546], cur_train_grp_loss: [24.27252571 21.84681804 24.28198546],max_reward_err:  0.1283, max_reward_err_index: 0, max_kl_dist:  0.2350, max_kl_dist_index: 0, max_train_grp_loss:  24.2669, max_train_grp_loss_index: 2, max_val_grp_loss:  24.3530, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  24.2820, max_cur_train_grp_loss_index: 2, 
2024-09-17 23:54:41,345 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  21.4705, val_loss:  21.4674, grad_norm: 0.3239, reward_err: 0.0981, 0.0163, 0.0621, KL_dist: 0.4349, 0.0922, 0.3051, param: [2.87706165 1.35964933 5.92028344 2.91457702]train_grp_loss: [23.37419177 18.02936316 23.26983819], val_grp_loss: [23.01790242 18.20914945 23.42794204], train_hist_grp_loss: [2375.74390971 1955.18373217 2370.93612658], cur_train_grp_loss: [23.37959377 18.04826946 23.27630153],max_reward_err:  0.0981, max_reward_err_index: 0, max_kl_dist:  0.4349, max_kl_dist_index: 0, max_train_grp_loss:  23.3742, max_train_grp_loss_index: 0, max_val_grp_loss:  23.4279, max_val_grp_loss_index: 2, max_cur_train_grp_loss:  23.3796, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:54:41,566 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [2.87706165 1.35964933 5.92028344 2.91457702].
2024-09-17 23:54:41,894 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.7701, 3.7701, 3.1348
2024-09-17 23:54:41,895 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8031, 3.8031, 3.2612
2024-09-17 23:54:41,895 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.4300, 3.7412, 3.0585
2024-09-17 23:54:41,896 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0981, 0.0163, 0.0621
2024-09-17 23:54:42,571 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8031, 3.8031, 3.2612
Known param reward: [[3.8031005859375, 3.407827615737915, 3.227226972579956], [3.407827615737915, 3.8031005859375, 3.0572001934051514], [3.767336845397949, 3.540113687515259, 3.2611656188964844]], Known param reward error: [[0.0, 0.1039343980701332, 0.010406906696143971], [0.1039343980701332, 0.0, 0.0625437188192702], [0.009403837666506178, 0.0691506554926978, 0.0]].
