2024-09-17 23:56:45,681 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:198] - INFO: Logging to log_dpo/2024_09_17_23_54_32/swapped_noise0.4_[0.4,0.4,0.4]_2028
2024-09-17 23:56:45,683 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:199] - INFO: (IB) Seed: 2028
2024-09-17 23:56:45,684 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:200] - INFO: (IB) Data: 300
2024-09-17 23:56:45,847 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:296] - INFO: MLE reward loss: 0.6090, l2 distance: 6.8385, acc: 0.69.
2024-09-17 23:56:45,848 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:297] - INFO: True reward parameter: [[1.  3.  1.  3. ]
 [3.  1.  3.  1. ]
 [1.5 2.5 1.5 2.5]]
2024-09-17 23:56:45,849 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:298] - INFO: MLE reward parameter: [5.18949372 1.72963998 2.8716218  1.75848308]
2024-09-17 23:56:46,053 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:313] - INFO: Learned oracle reward: 3.4853, 3.8323, 3.0983
2024-09-17 23:56:47,322 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:947] - INFO: Iteration:  0, train_loss:  22.9251, val_loss:  22.0583, grad_norm: 0.4226, reward_err: 0.0462, 0.0825, 0.0339, KL_dist: 0.2361, 0.3282, 0.2327, param: [4.67683588 3.66105155 0.14430879 4.86260083]train_grp_loss: [23.70405344 22.29863489 22.52588894], val_grp_loss: [23.25852196 19.76925186 23.15970638], train_hist_grp_loss: [23.70440365 22.35395004 22.52943729], cur_train_grp_loss: [23.70440365 22.35395004 22.52943729],max_reward_err:  0.0825, max_reward_err_index: 1, max_kl_dist:  0.3282, max_kl_dist_index: 1, max_train_grp_loss:  23.7041, max_train_grp_loss_index: 0, max_val_grp_loss:  23.2585, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.7044, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:56:50,450 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/algos/linear_bandit/group_dpo_vectorised.py[line:1040] - INFO: Iteration:  99, train_loss:  21.8516, val_loss:  20.8216, grad_norm: 0.2487, reward_err: 0.0738, 0.0218, 0.0455, KL_dist: 0.4766, 0.2072, 0.3800, param: [6.82639126 3.3173969  2.28156365 3.8197382 ]train_grp_loss: [23.74413737 18.94122932 22.25503966], val_grp_loss: [23.42029725 16.35748625 22.68650504], train_hist_grp_loss: [2371.46794702 2036.96923033 2238.18352289], cur_train_grp_loss: [23.74332745 18.9607149  22.25716576],max_reward_err:  0.0738, max_reward_err_index: 0, max_kl_dist:  0.4766, max_kl_dist_index: 0, max_train_grp_loss:  23.7441, max_train_grp_loss_index: 0, max_val_grp_loss:  23.4203, max_val_grp_loss_index: 0, max_cur_train_grp_loss:  23.7433, max_cur_train_grp_loss_index: 0, 
2024-09-17 23:56:50,672 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:386] - INFO: Policy parameter learned solely on the preference data dpo: [6.82639126 3.3173969  2.28156365 3.8197382 ].
2024-09-17 23:56:51,006 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:397] - INFO: Uniform reward: 3.8016, 3.8016, 3.1504
2024-09-17 23:56:51,007 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:398] - INFO: Optimal reward: 3.8416, 3.8416, 3.2815
2024-09-17 23:56:51,007 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:399] - INFO: Policy reward: 3.5581, 3.7579, 3.1323
2024-09-17 23:56:51,008 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:400] - INFO: Reward Error: 0.0738, 0.0218, 0.0455
2024-09-17 23:56:51,690 - /Users/noesis/Projects/Group-robust-preference-optimization-bandits/experiments/run_glb_noisy.py[line:415] - INFO: Optimal reward: 3.8416, 3.8416, 3.2815
Known param reward: [[3.841636896133423, 3.4451205730438232, 3.251262903213501], [3.4451205730438232, 3.841636896133423, 3.080437421798706], [3.805366277694702, 3.5709657669067383, 3.281508684158325]], Known param reward error: [[0.0, 0.10321546096370798, 0.00921703516764636], [0.10321546096370798, 0.0, 0.0612740302441686], [0.009441448897793229, 0.0704572390740814, 0.0]].
